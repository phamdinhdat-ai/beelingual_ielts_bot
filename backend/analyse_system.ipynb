{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import Iterator, List\n",
    "# CrewAI imports\n",
    "\n",
    "# LLM\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "# from agent.utils.load_documents import covert_document\n",
    "from agent.tools.retrieve_tool import RetrieveTool, IngestTool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "# from docling.document_converter import DocumentConverter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = [\"data/2501.07329v2.pdf\",\n",
    "             \"data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DocumentPDFLoader(BaseLoader):\n",
    "    \n",
    "#     def __init__(self, filepath: List[str]) -> None: \n",
    "#         self._filepath = filepath if isinstance(filepath, list) else [filepath]\n",
    "#         self._coverter = DocumentConverter()\n",
    "    \n",
    "#     def lazy_load (self)->Iterator[Document]:\n",
    "#         for file in self._filepath:\n",
    "#             dl = self._coverter.convert(file).document\n",
    "#             text = dl.export_to_markdown()\n",
    "#             yield Document(page_content=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_loader = DocumentPDFLoader(file_path)\n",
    "\n",
    "class DocumentPDFLoader(BaseLoader):\n",
    "    def __init__(self, filepath: List[str]) -> None: \n",
    "        self._filepath = filepath if isinstance(filepath, list) else [filepath]\n",
    "        self._loaders = [PyPDFLoader(file) for file in self._filepath]\n",
    "    \n",
    "    def lazy_load (self)->Iterator[Document]:\n",
    "        for loader in self._loaders:\n",
    "            for doc in loader.load():\n",
    "                yield doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = DocumentPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = chunker.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '',\n",
       " 'title': 'ielts-listening-practice-test-pdf-1',\n",
       " 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       " 'total_pages': 7,\n",
       " 'page': 3,\n",
       " 'page_label': '4'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[40].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc.page_content for doc in text_chunks]\n",
    "metadata = [doc.metadata for doc in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load environment variables if needed (e.g., API keys for other tools)\n",
    "TEST_COLLECTION_NAME = \"agent_test_docs\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\" # Ensure this model is available locally\n",
    "PERSIST_DIR = \"./_agent_test_chroma_db\"\n",
    "LLM_MODEL = \"deepseek-r1:1.5b\" # Or your preferred Ollama model\n",
    "# llm = ChatOllama(model='deepseek-r1:1.5b', temperature=0.2, max_tokens=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joint Automatic Speech Recognition And Structure\\nLearning For Better Speech Understanding\\nJiliang Hu1, Zuchao Li 2,*, Mengjia Shen 3, Haojun Ai 1, Sheng Li 4, Jun Zhang 3\\n1Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education,\\nSchool of Cyber Science and Engineering, Wuhan University, Wuhan, China,\\n2School of Computer Science, Wuhan University, Wuhan, China,\\n3Wuhan Second Ship Design and Research Institute, Wuhan, China,\\n4National Institute of Information and Communications Technology, Japan.\\nAbstract—Spoken language understanding (SLU) is a structure\\nprediction task in the field of speech. Recently, many works\\non SLU that treat it as a sequence-to-sequence task have\\nachieved great success. However, This method is not suitable\\nfor simultaneous speech recognition and understanding. In this\\npaper, we propose a joint speech recognition and structure\\nlearning framework (JSRSL), an end-to-end SLU model based'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Initializes LLM, Tools, and cleans up old DB.\"\"\"\n",
    "    logger.info(\"--- Setting up test environment ---\")\n",
    "\n",
    "    try:\n",
    "        # Initialize LLM\n",
    "        logger.info(f\"Loading LLM: {LLM_MODEL}\")\n",
    "        llm = ChatOllama(model=LLM_MODEL, temperature=0.1)\n",
    "        # Simple check if LLM is accessible (optional, Ollama might not have a direct check)\n",
    "        # llm.invoke(\"Hi\")\n",
    "        logger.info(\"LLM loaded successfully.\")\n",
    "\n",
    "        # Initialize Tools\n",
    "        logger.info(\"Initializing RAG Tools...\")\n",
    "        retriever_tool = RetrieveTool(\n",
    "            embedding_model_name=EMBEDDING_MODEL,\n",
    "            persist_dir=PERSIST_DIR\n",
    "        )\n",
    "        ingest_tool = IngestTool(retriever_tool=retriever_tool)\n",
    "        logger.info(\"RAG Tools initialized successfully.\")\n",
    "\n",
    "        return llm, retriever_tool, ingest_tool\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set up environment: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226089/4034160067.py:8: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=LLM_MODEL, temperature=0.1)\n",
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/db/impl/sqlite.py:111: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\n",
      "2025-05-04 23:58:54,443 - 128333791036480 - sqlite.py-sqlite:111 - WARNING: ⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    }
   ],
   "source": [
    "llm, retriever_tool, ingest_tool = setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, ingest_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 23:59:06,407 - 128333791036480 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Defining Knowledge Base Manager Agent ---\")\n",
    "kb_manager_agent = Agent(\n",
    "    role='Knowledge Base Manager',\n",
    "    goal=f\"Efficiently manage and retrieve information from the company's knowledge base stored in ChromaDB. Use the provided tools to ingest new documents into specific collections and retrieve relevant information based on queries.\",\n",
    "    backstory=(\n",
    "        \"You are an expert AI assistant responsible for maintaining the accuracy and accessibility \"\n",
    "        \"of the company's document knowledge base. You meticulously ingest new information using the \"\n",
    "        \"'ChromaDB Document Ingest Tool' and expertly query the database using the \"\n",
    "        \"'ChromaDB Retriever Tool' to answer questions. Always specify the correct collection name.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    verbose=True, # Set to True to see LLM reasoning and tool calls\n",
    "    allow_delegation=False,\n",
    "    \n",
    "    # memory=True # Optional: Enable memory for conversation context if needed\n",
    ")\n",
    "logger.info(f\"Agent '{kb_manager_agent.role}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_repr = repr(docs)\n",
    "metas_repr = repr(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ingest = Task(\n",
    "        description=(\n",
    "            f\"Ingest the following set of documents into the '{TEST_COLLECTION_NAME}' collection \"\n",
    "            f\"using the 'ChromaDB Document Ingest Tool'. Ensure you pass both the document texts \"\n",
    "            f\"and their corresponding metadata.\\n\\n\"\n",
    "            f\"Documents to ingest: {docs_repr}\\n\"\n",
    "            f\"Associated Metadatas: {metas_repr}\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            f\"Confirmation that {len(docs)} documents were successfully ingested \"\n",
    "            f\"into the '{TEST_COLLECTION_NAME}' collection.\"\n",
    "        ),\n",
    "        agent=kb_manager_agent,\n",
    "        tools=[ingest_tool] # Optional: Limit tools for this specific task\n",
    "    )\n",
    "logger.info(\"Ingestion task defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Joint Automatic Speech Recognition and Machine Translation (JASR-MT) task?\"\n",
    "task_retrieve = Task(\n",
    "    description=(\n",
    "        f\"Search the '{TEST_COLLECTION_NAME}' collection using the 'ChromaDB Retriever Tool' to find information relevant to the following query: '{query}'. \"\n",
    "        f\"Retrieve the top 3 most relevant documents using MMR for diversity. \" # Explicitly guide MMR usage\n",
    "        f\"Present the content of the retrieved documents clearly.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A summary or list of the content from the top 3 relevant documents found in the \"\n",
    "        f\"'{TEST_COLLECTION_NAME}' collection related to '{query}', retrieved using MMR.\"\n",
    "    ),\n",
    "    agent=kb_manager_agent,\n",
    "    context=[task_ingest], # Make this task depend on the ingestion task\n",
    "    tools=[retriever_tool] # Optional: Limit tools for this specific task\n",
    ")\n",
    "logger.info(\"Retrieval task defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 23:59:15,119 - 128333791036480 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reflection_agent = Agent(\n",
    "    role='Reflection Agent',\n",
    "    goal=\"Reflect on the tasks and provide insights.\",\n",
    "    backstory=(\n",
    "        \"You are an AI assistant designed to reflect on the tasks performed by the Knowledge Base Manager. \"\n",
    "        \"Your role is to analyze the ingestion and retrieval processes, ensuring they align with the company's goals.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[],\n",
    "    verbose=True, # Set to True to see LLM reasoning and tool calls\n",
    "    allow_delegation=False,\n",
    ")\n",
    "logger.info(\"Reflection agent created.\")\n",
    "reflection_task = Task(\n",
    "    description=(\n",
    "        \"Reflect on the tasks performed by the Knowledge Base Manager. \"\n",
    "        \"Analyze the ingestion and retrieval processes, ensuring they align with the company's goals.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Insights and analysis of the ingestion and retrieval processes, \"\n",
    "        \"ensuring they align with the company's goals.\"\n",
    "    ),\n",
    "    agent=reflection_agent,\n",
    "    context=[task_ingest, task_retrieve], # Make this task depend on both ingestion and retrieval tasks\n",
    "    tools=[] # No tools needed for reflection\n",
    ")\n",
    "logger.info(\"Reflection task defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 23:59:33,036 - 128333791036480 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Create and Run Crew\n",
    "logger.info(\"--- Creating and Running the Crew ---\")\n",
    "company_knowledge_crew = Crew(\n",
    "    agents=[kb_manager_agent],\n",
    "    tasks=[task_ingest, task_retrieve],\n",
    "    process=Process.sequential, # Ensure tasks run in order: ingest then retrieve\n",
    "    verbose=True # Use verbose=2 to see detailed LLM thoughts and tool calls\n",
    ")\n",
    "\n",
    "# result = company_knowledge_crew.kickoff()\n",
    "\n",
    "# logger.info(\"--- Crew Execution Finished ---\")\n",
    "# print(\"\\n\\n===== Final Crew Result =====\")\n",
    "# print(result)\n",
    "# print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Result: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from crewai import Agent, Task, Crew, Process\n",
    "# from crewai_tools import BaseTool, SerperDevTool\n",
    "# from langchain_community.chat_models.ollama import ChatOllama\n",
    "# from agent.config.load_config import agents_config\n",
    "# from agent.tools.nl2sql_tool import NL2SQLTool, ValidateSQLQueryTool\n",
    "# from dotenv import load_dotenv\n",
    "# from typing import List, Optional, Dict, Any\n",
    "# import chromadb\n",
    "# from os.path import dirname, join, abspath\n",
    "\n",
    "# # Import the retrieval tools\n",
    "# from agent.tools.retrieve_tool import RetrieveTool, IngestTool\n",
    "\n",
    "# # Load environment variables\n",
    "# env_path = join(dirname(dirname(abspath(__file__))), '.env')\n",
    "# load_dotenv(env_path)\n",
    "\n",
    "# class MultiAgentSystem:\n",
    "#     \"\"\"\n",
    "#     A multi-agent system using CrewAI framework.\n",
    "#     Integrates company, customer service, HR, and recommender agents.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, model_name: str = \"deepseek-r1:1.5b\"):\n",
    "#         \"\"\"\n",
    "#         Initialize the multi-agent system with specified LLM model.\n",
    "        \n",
    "#         Args:\n",
    "#             model_name: Name of the LLM model to use across agents\n",
    "#         \"\"\"\n",
    "#         self.model_name = model_name\n",
    "#         self.llm = self._load_llm(model_name)\n",
    "        \n",
    "#         # Initialize tools\n",
    "#         self.tools = self._setup_tools()\n",
    "        \n",
    "#         # Initialize agents\n",
    "#         self.agents = self._setup_agents()\n",
    "        \n",
    "#         # Create crew\n",
    "#         self.crew = self._setup_crew()\n",
    "    \n",
    "#     def _load_llm(self, model_name: str):\n",
    "#         \"\"\"Load the language model.\"\"\"\n",
    "#         return ChatOllama(model=model_name, temperature=0.2, max_tokens=2000)\n",
    "    \n",
    "#     def _setup_tools(self) -> Dict[str, List[BaseTool]]:\n",
    "#         \"\"\"Set up tools for each agent.\"\"\"\n",
    "#         # Common tools\n",
    "#         serper_api_key = os.environ.get('SERPER_API_KEY')\n",
    "#         search_tool = SerperDevTool(api_key=serper_api_key) if serper_api_key else None\n",
    "#         nl2sql_tool = NL2SQLTool()\n",
    "#         validate_sql_tool = ValidateSQLQueryTool()\n",
    "        \n",
    "#         # Retrieval tools\n",
    "#         retriever_tool = RetrieveTool(embedding_model_name=\"all-MiniLM-L6-v2\")\n",
    "#         ingest_tool = IngestTool(retriever_tool=retriever_tool)\n",
    "        \n",
    "#         # Define tool sets for each agent\n",
    "#         return {\n",
    "#             \"company_agent\": [tool for tool in [search_tool, nl2sql_tool, validate_sql_tool, retriever_tool, ingest_tool] if tool],\n",
    "#             \"customer_service_agent\": [tool for tool in [search_tool, nl2sql_tool, validate_sql_tool, retriever_tool] if tool],\n",
    "#             \"hr_agent\": [tool for tool in [search_tool, nl2sql_tool, retriever_tool] if tool],\n",
    "#             \"naive_agent\": [tool for tool in [search_tool, retriever_tool] if tool],\n",
    "#             \"recommender_agent\": [tool for tool in [search_tool, retriever_tool] if tool]\n",
    "#         }\n",
    "    \n",
    "#     def _setup_agents(self) -> Dict[str, Agent]:\n",
    "#         \"\"\"Set up all agents with their tools and configurations.\"\"\"\n",
    "#         agents = {}\n",
    "        \n",
    "#         # Create each agent using their config\n",
    "#         for agent_type in [\"company_agent\", \"customer_service_agent\", \"hr_agent\", \"naive_agent\", \"recommender_agent\"]:\n",
    "#             config = agents_config.get(agent_type, {})\n",
    "#             agents[agent_type] = Agent(\n",
    "#                 role=config.get('role', f\"{agent_type.replace('_', ' ').title()}\"),\n",
    "#                 goal=config.get('goal', \"Help the company achieve its objectives\"),\n",
    "#                 backstory=config.get('backstory', \"An experienced professional in the field\"),\n",
    "#                 verbose=config.get('verbose', True),\n",
    "#                 allow_delegation=config.get('allow_delegation', False),\n",
    "#                 tools=self.tools.get(agent_type, []),\n",
    "#                 llm=self.llm\n",
    "#             )\n",
    "        \n",
    "#         return agents\n",
    "    \n",
    "#     def _setup_crew(self) -> Crew:\n",
    "#         \"\"\"Set up the crew with all agents and their tasks.\"\"\"\n",
    "#         # Define tasks for each agent\n",
    "#         company_task = Task(\n",
    "#             description=\"Analyze company data and make strategic decisions\",\n",
    "#             agent=self.agents[\"company_agent\"],\n",
    "#             expected_output=\"Strategic analysis and recommendations for the company\"\n",
    "#         )\n",
    "        \n",
    "#         customer_task = Task(\n",
    "#             description=\"Address customer inquiries and improve satisfaction\",\n",
    "#             agent=self.agents[\"customer_service_agent\"],\n",
    "#             expected_output=\"Customer service report and satisfaction improvement plan\"\n",
    "#         )\n",
    "        \n",
    "#         hr_task = Task(\n",
    "#             description=\"Manage employee relations and recruitment\",\n",
    "#             agent=self.agents[\"hr_agent\"],\n",
    "#             expected_output=\"HR management report and recruitment strategy\"\n",
    "#         )\n",
    "        \n",
    "#         recommender_task = Task(\n",
    "#             description=\"Provide personalized recommendations to customers\",\n",
    "#             agent=self.agents[\"recommender_agent\"],\n",
    "#             expected_output=\"Customer recommendation system plan and implementation strategy\"\n",
    "#         )\n",
    "        \n",
    "#         # Create crew with all agents and tasks\n",
    "#         return Crew(\n",
    "#             agents=list(self.agents.values()),\n",
    "#             tasks=[company_task, customer_task, hr_task, recommender_task],\n",
    "#             verbose=2,\n",
    "#             process=Process.sequential  # Can be changed to Process.hierarchical if needed\n",
    "#         )\n",
    "    \n",
    "#     def run(self, query: str) -> str:\n",
    "#         \"\"\"\n",
    "#         Run the multi-agent system with a specific query.\n",
    "        \n",
    "#         Args:\n",
    "#             query: User query to process across agents\n",
    "            \n",
    "#         Returns:\n",
    "#             str: Results from the crew's execution\n",
    "#         \"\"\"\n",
    "#         # You could customize the tasks or crew configuration based on the query\n",
    "#         result = self.crew.kickoff(inputs={\"query\": query})\n",
    "#         return result\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize the multi-agent system\n",
    "#     system = MultiAgentSystem(model_name=\"deepseek-r1:1.5b\")\n",
    "    \n",
    "#     # Example document ingestion for testing the retrieval tool\n",
    "#     documents = [\n",
    "#         \"Our company specializes in AI-powered solutions for businesses.\",\n",
    "#         \"Customer satisfaction is our top priority, with 24/7 support available.\",\n",
    "#         \"The HR department handles recruitment, onboarding, and employee relations.\",\n",
    "#         \"Our recommendation engine uses machine learning to suggest products.\",\n",
    "#         \"Company policies include flexible working hours and remote options.\"\n",
    "#     ]\n",
    "    \n",
    "#     metadatas = [\n",
    "#         {\"category\": \"company_info\", \"department\": \"general\"},\n",
    "#         {\"category\": \"customer_service\", \"department\": \"support\"},\n",
    "#         {\"category\": \"hr\", \"department\": \"human_resources\"},\n",
    "#         {\"category\": \"technology\", \"department\": \"engineering\"},\n",
    "#         {\"category\": \"policy\", \"department\": \"human_resources\"}\n",
    "#     ]\n",
    "    \n",
    "#     # Access company agent and ingest sample documents\n",
    "#     retriever_tool = next((t for t in system.tools[\"company_agent\"] if isinstance(t, RetrieveTool)), None)\n",
    "#     if retriever_tool:\n",
    "#         retriever_tool.ingest_documents(\n",
    "#             documents=documents,\n",
    "#             collection_name=\"company_docs\",\n",
    "#             metadatas=metadatas\n",
    "#         )\n",
    "    \n",
    "#     # Run system with a test query\n",
    "#     result = system.run(\"What are our company's HR policies and how can we improve employee satisfaction?\")\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPER_API_KEY\"] = \"a6ce4b5fc4ef3f9754144d519ecf9e418bc1c7bc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3549: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/tmp/ipykernel_226089/896718699.py:52: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"hf.co/CompendiumLabs/bge-base-en-v1.5-gguf:latest\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "import operator\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.tools import Tool\n",
    "# from crewai_tools import DuckDuckGoSearchRun # Re-use crewai tool directly\n",
    "from crewai_tools import SerperDevTool\n",
    "# For Customer Retriever (Example using FAISS)\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.tools.vectorstore.tool import VectorStoreQATool\n",
    "\n",
    "# from langgraph.graph import StateGraph, END\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver # For state persistence/debugging if needed\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Initialize LLM ---\n",
    "# Using a more capable model might be beneficial for classification and reflection\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.2, num_predict=200)\n",
    "\n",
    "# --- Initialize Tools ---\n",
    "search_tool = SerperDevTool(n_results=2)\n",
    "\n",
    "# --- Mock Customer Database & Retriever Tool ---\n",
    "# In a real scenario, connect to your actual database/vector store\n",
    "customer_data = {\n",
    "    \"cust123\": [\n",
    "        Document(page_content=\"Customer John Doe. Premium plan member since 2022. Last interaction: support ticket #5678 resolved.\", metadata={\"customer_id\": \"cust123\", \"source\": \"crm\"}),\n",
    "        Document(page_content=\"John Doe purchased Product X in Jan 2023 and Product Y in Nov 2023.\", metadata={\"customer_id\": \"cust123\", \"source\": \"orders\"}),\n",
    "    ],\n",
    "    \"cust456\": [\n",
    "        Document(page_content=\"Customer Jane Smith. Basic plan member since 2023. No open support tickets.\", metadata={\"customer_id\": \"cust456\", \"source\": \"crm\"}),\n",
    "    ]\n",
    "}\n",
    "# Flatten documents for indexing\n",
    "all_docs = [doc for docs in customer_data.values() for doc in docs]\n",
    "\n",
    "if all_docs:\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = OllamaEmbeddings(model=\"hf.co/CompendiumLabs/bge-base-en-v1.5-gguf:latest\")\n",
    "    vector_store = Chroma.from_documents(documents=all_docs, embedding=embeddings, persist_directory=PERSIST_DIR, collection_name=TEST_COLLECTION_NAME)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 docs\n",
    "\n",
    "    customer_retriever_tool = VectorStoreQATool(\n",
    "        name=\"customer_info_retriever\",\n",
    "        description=\"Searches and returns information about a specific customer from the customer database based on their ID.\",\n",
    "        vectorstore=vector_store,\n",
    "        llm=llm # The tool itself can use an LLM for QA over retrieved docs\n",
    "    )\n",
    "    # Note: This tool needs the query to implicitly or explicitly contain the customer ID for filtering,\n",
    "    # or the retriever needs more sophisticated filtering setup.\n",
    "    # For simplicity here, we'll assume the query passed to the tool includes context like \"for customer cust123\".\n",
    "else:\n",
    "    # Create a dummy tool if no customer data exists\n",
    "    def dummy_retriever_func(query: str):\n",
    "        return \"No customer data available for retrieval.\"\n",
    "    customer_retriever_tool = Tool(\n",
    "        name=\"customer_info_retriever\",\n",
    "        description=\"Searches and returns information about a specific customer. Currently, no data is loaded.\",\n",
    "        func=dummy_retriever_func\n",
    "    )\n",
    "\n",
    "print(\"Tools Initialized.\")\n",
    "# print(f\"Customer Retriever Tool Ready: {customer_retriever_tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.vectorstore.tool import VectorStoreQATool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 00:00:06,199 - 128333791036480 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Agent\ntools\n  Value error, Invalid tool type: <class 'langchain_community.tools.vectorstore.tool.VectorStoreQATool'>. Tool must be an instance of BaseTool or an object with 'name', 'func', and 'description' attributes. [type=value_error, input_value=[SerperDevTool(name='Sear...=200, temperature=0.2))], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      4\u001b[39m company_agent_def = CrewAgent(\n\u001b[32m      5\u001b[39m     role=\u001b[33m'\u001b[39m\u001b[33mCompany Information Specialist\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     goal=\u001b[33m'\u001b[39m\u001b[33mProvide accurate information about the company (e.g., Google) including location, products, services, history, and mission, based on search results and internal knowledge.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Agent 2: Customer Agent (Definition)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m customer_agent_def = \u001b[43mCrewAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCustomer Support Agent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAnswer customer-specific questions by retrieving their information from the customer database using their customer ID. Handle queries about plans, purchase history, support tickets etc.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackstory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYou are a helpful customer support agent with access to the customer database. You must use the customer ID provided to retrieve relevant information using your specialized tool.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomer_retriever_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Has retriever and search\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Agent 3: Naive Agent (Definition)\u001b[39;00m\n\u001b[32m     24\u001b[39m naive_agent_def = CrewAgent(\n\u001b[32m     25\u001b[39m     role=\u001b[33m'\u001b[39m\u001b[33mGeneral Knowledge Assistant\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     goal=\u001b[33m'\u001b[39m\u001b[33mAnswer general knowledge questions or queries that do not fall under specific company or customer information categories. Use search tools to find relevant information online.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Agent\ntools\n  Value error, Invalid tool type: <class 'langchain_community.tools.vectorstore.tool.VectorStoreQATool'>. Tool must be an instance of BaseTool or an object with 'name', 'func', and 'description' attributes. [type=value_error, input_value=[SerperDevTool(name='Sear...=200, temperature=0.2))], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from crewai import Agent as CrewAgent # Alias to avoid confusion with node names\n",
    "\n",
    "# Agent 1: Company Agent (Definition)\n",
    "company_agent_def = CrewAgent(\n",
    "    role='Company Information Specialist',\n",
    "    goal='Provide accurate information about the company (e.g., Google) including location, products, services, history, and mission, based on search results and internal knowledge.',\n",
    "    backstory='You are an AI assistant dedicated to representing the company accurately and helpfully to external queries. You use search tools to find the latest public information.',\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Agent 2: Customer Agent (Definition)\n",
    "customer_agent_def = CrewAgent(\n",
    "    role='Customer Support Agent',\n",
    "    goal='Answer customer-specific questions by retrieving their information from the customer database using their customer ID. Handle queries about plans, purchase history, support tickets etc.',\n",
    "    backstory='You are a helpful customer support agent with access to the customer database. You must use the customer ID provided to retrieve relevant information using your specialized tool.',\n",
    "    tools=[search_tool, customer_retriever_tool], # Has retriever and search\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Agent 3: Naive Agent (Definition)\n",
    "naive_agent_def = CrewAgent(\n",
    "    role='General Knowledge Assistant',\n",
    "    goal='Answer general knowledge questions or queries that do not fall under specific company or customer information categories. Use search tools to find relevant information online.',\n",
    "    backstory='You are a general-purpose AI assistant capable of answering a wide range of topics by searching the internet.',\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"CrewAI Agent Definitions Ready (for conceptual reference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Crew, Process\n",
    "from typing import List, Dict, Any, Optional, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph State Defined.\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    original_query: str\n",
    "    customer_id: Optional[str] # Allow optional customer ID\n",
    "    rewritten_query: str\n",
    "    classified_agent: Literal[\"Company\", \"Customer\", \"Naive\", \"Unknown\"] # Agent types\n",
    "    agent_response: str\n",
    "    reflection: str\n",
    "    is_final: bool\n",
    "    error: Optional[str]\n",
    "    retry_count: int\n",
    "\n",
    "print(\"LangGraph State Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph Nodes Defined.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "# --- Node 1: Rewriter ---\n",
    "class RewriterOutput(BaseModel):\n",
    "    \"\"\"Structured output for the Rewriter node.\"\"\"\n",
    "    rewritten_query: str = Field(description=\"The user's query, corrected for spelling and clarity.\")\n",
    "    agent_classification: Literal[\"Company\", \"Customer\", \"Naive\", \"Unknown\"] = Field(description=\"The best agent type to handle the rewritten query.\")\n",
    "    extracted_customer_id: Optional[str] = Field(description=\"Customer ID extracted from the query, if relevant and present (e.g., 'cust123'). Null otherwise.\")\n",
    "\n",
    "def rewrite_query_node(state: AgentState):\n",
    "    \"\"\"Rewrites the user query and classifies which agent should handle it.\"\"\"\n",
    "    print(\"--- Node: Rewriter ---\")\n",
    "    query = state['original_query']\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert query processor. Your tasks are:\n",
    "1.  Correct any spelling mistakes in the user query.\n",
    "2.  Rephrase the query for maximum clarity if necessary.\n",
    "3.  Classify the query's intent to determine the best agent to handle it:\n",
    "    - 'Company': For questions about a specific company's details (location, products, services, mission etc.). Assume the company is 'Google' if not specified.\n",
    "    - 'Customer': For questions related to a specific customer's account, history, plan, support tickets etc. These queries MUST contain or imply a customer ID.\n",
    "    - 'Naive': For general knowledge questions, queries unrelated to the company or a specific customer.\n",
    "    - 'Unknown': If the query is ambiguous or cannot be clearly classified.\n",
    "4. Extract any customer ID mentioned (like cust123, user45, id: 7890). Return null if no ID is found.\n",
    "Provide your output in the specified JSON format.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Process the following user query: {query}\")\n",
    "    ])\n",
    "\n",
    "    # Use OpenAI Functions for structured output\n",
    "    rewriter_chain = prompt | llm.bind_functions(functions=[RewriterOutput], function_call=\"RewriterOutput\") | JsonOutputFunctionsParser()\n",
    "\n",
    "    try:\n",
    "        response: RewriterOutput = rewriter_chain.invoke({\"query\": query})\n",
    "        print(f\"Rewriter Output: {response}\")\n",
    "        return {\n",
    "            \"rewritten_query\": response['rewritten_query'],\n",
    "            \"classified_agent\": response['agent_classification'],\n",
    "            \"customer_id\": response.get('extracted_customer_id') or state.get('customer_id'), # Prioritize extracted, fallback to state\n",
    "             \"error\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Rewriter node: {e}\")\n",
    "        return {\"error\": f\"Failed to rewrite/classify query: {e}\"}\n",
    "\n",
    "# --- Nodes 2, 3, 4: Specialized Agent Execution ---\n",
    "# These nodes will simulate the execution of the conceptually defined CrewAI agents.\n",
    "\n",
    "def execute_agent_node(state: AgentState, agent_def: CrewAgent, agent_name: str):\n",
    "    \"\"\"Generic function to execute the logic of a specific agent.\"\"\"\n",
    "    print(f\"--- Node: Execute {agent_name} Agent ---\")\n",
    "    if state.get(\"error\"): return {} # Don't run if prior error\n",
    "\n",
    "    query = state['rewritten_query']\n",
    "    customer_id = state.get('customer_id') # Relevant for Customer agent\n",
    "\n",
    "    # Construct a prompt using the agent's definition\n",
    "    prompt_messages = [\n",
    "        SystemMessage(content=f\"Role: {agent_def.role}\\nGoal: {agent_def.goal}\\nBackstory: {agent_def.backstory}\"),\n",
    "        HumanMessage(content=f\"User Query: {query}\")\n",
    "    ]\n",
    "    if agent_name == \"Customer\" and customer_id:\n",
    "         prompt_messages.append(HumanMessage(content=f\"Context: Apply this query to customer ID: {customer_id}.\"))\n",
    "    elif agent_name == \"Customer\" and not customer_id:\n",
    "         return {\"agent_response\": \"Cannot answer customer question without a Customer ID.\", \"error\": \"Missing Customer ID\"}\n",
    "\n",
    "\n",
    "    # Simplified tool handling for LangGraph node\n",
    "    # A more robust implementation would use LangChain's agent executors or tool calling\n",
    "    available_tools = {tool.name: tool for tool in agent_def.tools}\n",
    "    tool_response = \"\"\n",
    "\n",
    "    # Basic tool check (can be expanded)\n",
    "    # This is a placeholder - real tool use requires more complex agent logic (like ReAct or OpenAI Functions Agent)\n",
    "    if \"duckduckgo_search\" in available_tools and agent_name != \"Customer\": # Example: Use search for non-customer\n",
    "        try:\n",
    "            tool_response = f\"\\nSearch Results: {search_tool.run(query)}\"\n",
    "            prompt_messages.append(AIMessage(content=f\"Tool Used: duckduckgo_search\\nResult: {tool_response[:500]}...\")) # Add tool result snippet\n",
    "        except Exception as e:\n",
    "            print(f\"Error using search tool: {e}\")\n",
    "            tool_response = \"\\nSearch tool failed.\"\n",
    "\n",
    "    if \"customer_info_retriever\" in available_tools and agent_name == \"Customer\":\n",
    "        try:\n",
    "            # Pass query potentially enriched with customer ID context\n",
    "            retriever_query = f\"Info for customer {customer_id}: {query}\"\n",
    "            tool_response = f\"\\nCustomer DB Info: {customer_retriever_tool.run(retriever_query)}\"\n",
    "            prompt_messages.append(AIMessage(content=f\"Tool Used: customer_info_retriever\\nResult: {tool_response[:500]}...\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error using customer retriever tool: {e}\")\n",
    "            tool_response = \"\\nCustomer retrieval tool failed.\"\n",
    "\n",
    "    # Final LLM call to generate response based on persona and potential tool output\n",
    "    final_prompt = ChatPromptTemplate.from_messages(prompt_messages)\n",
    "    chain = final_prompt | llm\n",
    "    try:\n",
    "        response = chain.invoke({}) # Query is already in messages\n",
    "        print(f\"{agent_name} Agent Response Snippet: {response.content[:200]}...\")\n",
    "        return {\"agent_response\": response.content, \"error\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {agent_name} agent LLM call: {e}\")\n",
    "        return {\"error\": f\"{agent_name} agent failed during generation: {e}\"}\n",
    "\n",
    "\n",
    "# Specific node functions calling the generic executor\n",
    "def company_agent_node(state: AgentState):\n",
    "    return execute_agent_node(state, company_agent_def, \"Company\")\n",
    "\n",
    "def customer_agent_node(state: AgentState):\n",
    "    return execute_agent_node(state, customer_agent_def, \"Customer\")\n",
    "\n",
    "def naive_agent_node(state: AgentState):\n",
    "    return execute_agent_node(state, naive_agent_def, \"Naive\")\n",
    "\n",
    "# --- Node 5: Reflection ---\n",
    "class ReflectionOutput(BaseModel):\n",
    "    \"\"\"Structured output for the Reflection node.\"\"\"\n",
    "    feedback: str = Field(description=\"Constructive feedback on the response's relevance and correctness relative to the original query.\")\n",
    "    is_final_answer: bool = Field(description=\"True if the answer is satisfactory and directly addresses the original query, False otherwise.\")\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Reflects on the generated answer, checking relevance and correctness.\"\"\"\n",
    "    print(\"--- Node: Reflection ---\")\n",
    "    if state.get(\"error\"): return {\"is_final\": True} # If error occurred, end the loop\n",
    "\n",
    "    original_query = state['original_query']\n",
    "    agent_response = state['agent_response']\n",
    "    # rewritten_query = state['rewritten_query'] # Could also be used for context\n",
    "\n",
    "    if not agent_response:\n",
    "         print(\"No agent response to reflect on.\")\n",
    "         return {\"reflection\": \"No response generated.\", \"is_final\": True, \"error\": \"Reflection failed: No agent response found.\"}\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\"You are a meticulous quality assurance reviewer. Your task is to evaluate an AI agent's response based on the user's original query.\n",
    "                    Assess the following:\n",
    "                    1.  **Relevance:** Does the response directly address the user's original question?\n",
    "                    2.  **Correctness:** Is the information likely correct (based on general knowledge or provided context)? You don't need to verify external facts exhaustively, but check for obvious flaws or contradictions.\n",
    "                    3.  **Completeness:** Does the response sufficiently answer the question?\n",
    "\n",
    "                    Provide constructive feedback and determine if the answer is final (good enough) or needs revision/retry. Use the specified JSON format.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Original Query: {original_query}\\n\\nAgent Response:\\n{agent_response}\\n\\nPlease evaluate and provide feedback.\")\n",
    "    ])\n",
    "\n",
    "    reflection_chain = prompt | llm.bind_functions(functions=[ReflectionOutput], function_call=\"ReflectionOutput\") | JsonOutputFunctionsParser()\n",
    "\n",
    "    try:\n",
    "        response: ReflectionOutput = reflection_chain.invoke({\n",
    "            \"original_query\": original_query,\n",
    "            \"agent_response\": agent_response\n",
    "        })\n",
    "        print(f\"Reflection Output: {response}\")\n",
    "        return {\n",
    "            \"reflection\": response['feedback'],\n",
    "            \"is_final\": response['is_final_answer'],\n",
    "            \"retry_count\": state.get('retry_count', 0) + 1, # Increment retry count\n",
    "            \"error\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Reflection node: {e}\")\n",
    "        # Decide how to handle reflection error - maybe treat as non-final?\n",
    "        return {\"error\": f\"Reflection failed: {e}\", \"is_final\": False, \"retry_count\": state.get('retry_count', 0) + 1}\n",
    "\n",
    "print(\"LangGraph Nodes Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CONFIG_KEYS' from 'langchain_core.runnables.config' (/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langchain_core/runnables/config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/graph/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m END, START, Graph\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MessageGraph, MessagesState, add_messages\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/graph/graph.py:33\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchannels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mephemeral_value\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EphemeralValue\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     EMPTY_SEQ,\n\u001b[32m     26\u001b[39m     END,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     Send,\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbranch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Branch\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Channel, Pregel\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PregelProtocol\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/graph/branch.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m END, START\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InvalidUpdateError\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwrite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelWrite\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Send\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     RunnableCallable,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/pregel/__init__.py:86\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     80\u001b[39m     ErrorCode,\n\u001b[32m     81\u001b[39m     GraphRecursionError,\n\u001b[32m     82\u001b[39m     InvalidUpdateError,\n\u001b[32m     83\u001b[39m     create_error_message,\n\u001b[32m     84\u001b[39m )\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanaged\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ManagedValueSpec\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     87\u001b[39m     PregelTaskWrites,\n\u001b[32m     88\u001b[39m     apply_writes,\n\u001b[32m     89\u001b[39m     local_read,\n\u001b[32m     90\u001b[39m     local_write,\n\u001b[32m     91\u001b[39m     prepare_next_tasks,\n\u001b[32m     92\u001b[39m )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_checkpoint, empty_checkpoint\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tasks_w_writes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/pregel/algo.py:68\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmptyChannelError, InvalidUpdateError\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanaged\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ManagedValueMapping\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcall\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_runnable_for_task\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_channel, read_channels\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlog\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/pregel/call.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONF, CONFIG_KEY_CALL, RETURN, TAG_HIDDEN\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpregel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwrite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelWrite, ChannelWriteEntry\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetryPolicy\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/pregel/write.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONF, CONFIG_KEY_SEND, TASKS, Send\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InvalidUpdateError\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableCallable\n\u001b[32m     21\u001b[39m TYPE_SEND = Callable[[Sequence[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]], \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m     22\u001b[39m R = TypeVar(\u001b[33m\"\u001b[39m\u001b[33mR\u001b[39m\u001b[33m\"\u001b[39m, bound=Runnable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/utils/runnable.py:50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseStore\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StreamWriter\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     51\u001b[39m     ensure_config,\n\u001b[32m     52\u001b[39m     get_async_callback_manager_for_config,\n\u001b[32m     53\u001b[39m     get_callback_manager_for_config,\n\u001b[32m     54\u001b[39m     patch_config,\n\u001b[32m     55\u001b[39m )\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_config_context\u001b[39m(\n\u001b[32m     59\u001b[39m     config: RunnableConfig,\n\u001b[32m     60\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Token[Optional[RunnableConfig]], Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[32m     61\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Set the child Runnable config + tracing context.\u001b[39;00m\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m        config (RunnableConfig): The config to set.\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langgraph/utils/config.py:12\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     AsyncCallbackManager,\n\u001b[32m      7\u001b[39m     BaseCallbackManager,\n\u001b[32m      8\u001b[39m     CallbackManager,\n\u001b[32m      9\u001b[39m     Callbacks,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableConfig\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     CONFIG_KEYS,\n\u001b[32m     14\u001b[39m     COPIABLE_KEYS,\n\u001b[32m     15\u001b[39m     var_child_runnable_config,\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckpointMetadata\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config, get_store, get_stream_writer  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'CONFIG_KEYS' from 'langchain_core.runnables.config' (/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/langchain_core/runnables/config.py)"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StateGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrewrite_query\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Loop back to the start\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# --- Build the Graph ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m workflow = \u001b[43mStateGraph\u001b[49m(AgentState)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Add nodes\u001b[39;00m\n\u001b[32m     63\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mrewrite_query\u001b[39m\u001b[33m\"\u001b[39m, rewrite_query_node)\n",
      "\u001b[31mNameError\u001b[39m: name 'StateGraph' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Conditional Edges ---\n",
    "\n",
    "def route_query(state: AgentState):\n",
    "    \"\"\"Routes to the appropriate agent node based on classification.\"\"\"\n",
    "    print(f\"--- Conditional Edge: Routing Query ---\")\n",
    "    if state.get(\"error\"):\n",
    "        print(\"Error detected before routing.\")\n",
    "        return \"error_handler\" # Or END directly\n",
    "\n",
    "    classification = state['classified_agent']\n",
    "    print(f\"Routing based on classification: {classification}\")\n",
    "    if classification == \"Company\":\n",
    "        return \"execute_company\"\n",
    "    elif classification == \"Customer\":\n",
    "        # Add check for customer ID existence\n",
    "        if state.get(\"customer_id\"):\n",
    "             return \"execute_customer\"\n",
    "        else:\n",
    "             print(\"Customer classification but no ID found.\")\n",
    "             # Update state to reflect missing ID issue and go to reflection/end\n",
    "             state[\"error\"] = \"Query classified as Customer, but no Customer ID was provided or found.\"\n",
    "             state[\"agent_response\"] = \"I need a customer ID to answer that question.\"\n",
    "             return \"reflect\" # Go to reflection to potentially end gracefully\n",
    "    elif classification == \"Naive\":\n",
    "        return \"execute_naive\"\n",
    "    else: # Unknown or error during classification\n",
    "        print(\"Classification is Unknown or invalid.\")\n",
    "        state[\"error\"] = f\"Could not determine the right agent for the query (classification: {classification}).\"\n",
    "        state[\"agent_response\"] = \"I'm not sure how to handle that query. Could you please rephrase?\"\n",
    "        return \"reflect\" # Go to reflection\n",
    "\n",
    "def decide_after_reflection(state: AgentState):\n",
    "    \"\"\"Decides whether to end the process or retry based on reflection.\"\"\"\n",
    "    print(f\"--- Conditional Edge: After Reflection ---\")\n",
    "    is_final = state['is_final']\n",
    "    retry_count = state['retry_count']\n",
    "    max_retries = 2 # Set a limit for retries\n",
    "\n",
    "    if state.get(\"error\") and \"Reflection failed\" not in state[\"error\"]: # Handle agent errors first\n",
    "        print(f\"Ending due to execution error: {state['error']}\")\n",
    "        return END # End directly on execution error\n",
    "\n",
    "    print(f\"Reflection result: is_final={is_final}, Retry count={retry_count}\")\n",
    "\n",
    "    if is_final:\n",
    "        print(\"Reflection approved. Ending.\")\n",
    "        return END\n",
    "    elif retry_count >= max_retries:\n",
    "        print(f\"Max retries ({max_retries}) reached. Ending.\")\n",
    "        # Optionally, provide the last reflection feedback\n",
    "        state[\"agent_response\"] += f\"\\n\\n[System Note: Max retries reached. Last feedback: {state.get('reflection', 'N/A')}]\"\n",
    "        return END\n",
    "    else:\n",
    "        print(\"Reflection suggests retry. Looping back to Rewriter.\")\n",
    "        # Add reflection feedback to the original query for context in the next loop? Optional.\n",
    "        # state['original_query'] = f\"{state['original_query']}\\n\\n[Retry Context: Previous attempt failed. Feedback: {state.get('reflection', 'N/A')}]\"\n",
    "        return \"rewrite_query\" # Loop back to the start\n",
    "\n",
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query_node)\n",
    "workflow.add_node(\"execute_company\", company_agent_node)\n",
    "workflow.add_node(\"execute_customer\", customer_agent_node)\n",
    "workflow.add_node(\"execute_naive\", naive_agent_node)\n",
    "workflow.add_node(\"reflect\", reflection_node)\n",
    "# Optional: Add a specific error handling node if needed\n",
    "# workflow.add_node(\"error_handler\", ...)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"rewrite_query\")\n",
    "\n",
    "# Routing from Rewriter\n",
    "workflow.add_conditional_edges(\n",
    "    \"rewrite_query\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"execute_company\": \"execute_company\",\n",
    "        \"execute_customer\": \"execute_customer\",\n",
    "        \"execute_naive\": \"execute_naive\",\n",
    "        \"reflect\": \"reflect\", # Handle Unknown/Error cases by going directly to reflection\n",
    "        # \"error_handler\": \"error_handler\" # Route explicit errors\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edges from execution nodes to reflection\n",
    "workflow.add_edge(\"execute_company\", \"reflect\")\n",
    "workflow.add_edge(\"execute_customer\", \"reflect\")\n",
    "workflow.add_edge(\"execute_naive\", \"reflect\")\n",
    "\n",
    "# Conditional edge from Reflection (Loop or End)\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    decide_after_reflection,\n",
    "    {\n",
    "        \"rewrite_query\": \"rewrite_query\", # Loop back\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\") # Optional: In-memory checkpointing\n",
    "# app = workflow.compile(checkpointer=memory)\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "print(\"LangGraph Compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Agentic System for Query: 'Where is the main Google office located?'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'Tell me about my purchase history for cust123'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'What was my last support ticket about?' (Customer ID: cust123)\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'can you tell me about the plan for customer cust456 please?'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'Who invented the telephone?'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'hwat is googles mission sttement?'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'Tell me about cust999'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n",
      "\n",
      "🚀 Starting Agentic System for Query: 'This query is confusing and makes no sense.'\n",
      "\n",
      "💥 An unhandled error occurred during graph execution: name 'app' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_975034/2447842949.py\", line 20, in run_agentic_system\n",
      "    final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
      "                  ^^^\n",
      "NameError: name 'app' is not defined\n"
     ]
    }
   ],
   "source": [
    "def run_agentic_system(query: str, cust_id: Optional[str] = None):\n",
    "    \"\"\"Runs the agentic system with a user query.\"\"\"\n",
    "    initial_state = AgentState(\n",
    "        original_query=query,\n",
    "        customer_id=cust_id,\n",
    "        rewritten_query=\"\",\n",
    "        classified_agent=\"Unknown\", # Start as Unknown\n",
    "        agent_response=\"\",\n",
    "        reflection=\"\",\n",
    "        is_final=False,\n",
    "        error=None,\n",
    "        retry_count=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🚀 Starting Agentic System for Query: '{query}'\" + (f\" (Customer ID: {cust_id})\" if cust_id else \"\"))\n",
    "    # config = {\"configurable\": {\"thread_id\": f\"thread_{query[:10]}\"}} # Example thread ID if using checkpointer\n",
    "\n",
    "    try:\n",
    "        # final_state = app.invoke(initial_state, config=config)\n",
    "        final_state = app.invoke(initial_state, {\"recursion_limit\": 10})\n",
    "\n",
    "\n",
    "        print(\"\\n🏁 Agentic System Finished!\")\n",
    "        print(\"------ Final State ------\")\n",
    "        # Pretty print relevant parts of the final state\n",
    "        print(f\"Original Query: {final_state['original_query']}\")\n",
    "        if final_state.get('rewritten_query'): print(f\"Rewritten Query: {final_state['rewritten_query']}\")\n",
    "        if final_state.get('classified_agent'): print(f\"Agent Used: {final_state['classified_agent']}\")\n",
    "        if final_state.get('error'):\n",
    "            print(f\"Error Occurred: {final_state['error']}\")\n",
    "        print(f\"\\nFinal Response:\\n{final_state['agent_response']}\")\n",
    "        if final_state.get('reflection') and not final_state.get('is_final') and final_state.get('retry_count') > 0:\n",
    "             print(f\"\\nLast Reflection Feedback (Process Ended): {final_state['reflection']}\")\n",
    "\n",
    "        return final_state\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 An unhandled error occurred during graph execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# --- Example Queries ---\n",
    "run_agentic_system(\"Where is the main Google office located?\")\n",
    "run_agentic_system(\"Tell me about my purchase history for cust123\")\n",
    "run_agentic_system(\"What was my last support ticket about?\", cust_id=\"cust123\") # Providing ID separately\n",
    "run_agentic_system(\"can you tell me about the plan for customer cust456 please?\")\n",
    "run_agentic_system(\"Who invented the telephone?\")\n",
    "run_agentic_system(\"hwat is googles mission sttement?\") # Test spelling correction\n",
    "run_agentic_system(\"Tell me about cust999\") # Test non-existent customer ID (if retriever handles it)\n",
    "run_agentic_system(\"This query is confusing and makes no sense.\") # Test unknown/retry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
