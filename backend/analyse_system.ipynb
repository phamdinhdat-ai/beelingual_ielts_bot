{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import Iterator, List\n",
    "# CrewAI imports\n",
    "\n",
    "# LLM\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "# from agent.utils.load_documents import covert_document\n",
    "from agent.tools.retrieve_tool import RetrieveTool, IngestTool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "# from docling.document_converter import DocumentConverter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = [\"data/2501.07329v2.pdf\",\n",
    "             \"data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DocumentPDFLoader(BaseLoader):\n",
    "    \n",
    "#     def __init__(self, filepath: List[str]) -> None: \n",
    "#         self._filepath = filepath if isinstance(filepath, list) else [filepath]\n",
    "#         self._coverter = DocumentConverter()\n",
    "    \n",
    "#     def lazy_load (self)->Iterator[Document]:\n",
    "#         for file in self._filepath:\n",
    "#             dl = self._coverter.convert(file).document\n",
    "#             text = dl.export_to_markdown()\n",
    "#             yield Document(page_content=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import  SerperDevTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"SERPER_API_KEY\"] = \"a6ce4b5fc4ef3f9754144d519ecf9e418bc1c7bc\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results  = tool.invoke(\"What is the capital of France?\") # returns a list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from loguru import logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgentTool:\n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.tool = None\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_tool(self):\n",
    "        logger.info(\"Setting up the tool\")\n",
    "        self.tool = {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description\n",
    "        }\n",
    "        logger.info(f\"Tool set up: {self.tool}\")\n",
    "    @abstractmethod\n",
    "    def run(self, query, **kwargs):\n",
    "        logger.info(\"Running the tool\")\n",
    "        # Placeholder for tool-specific logic\n",
    "        pass\n",
    "\n",
    "class SearchTool(BaseAgentTool):\n",
    "    def __init__(self, name: str, description: str):\n",
    "        super().__init__(name, description)\n",
    "        \n",
    "\n",
    "    \n",
    "    def setup_tool(self):\n",
    "        logger.info(\"Setting up the search tool\")\n",
    "        from langchain_tavily import TavilySearch\n",
    "        self.tool = {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description\n",
    "        }\n",
    "        self.search = TavilySearch(max_results=5, topic=\"general\")\n",
    "\n",
    "        logger.info(f\"Search tool set up: {self.tool}\")\n",
    "\n",
    "    def run(self, query):\n",
    "        logger.info(f\"Running search tool with query: {query}\")\n",
    "        # Placeholder for search logic\n",
    "        resutls = self.search.invoke(query)\n",
    "        logger.info(f\"Search results: {resutls}\")\n",
    "        content_results = []\n",
    "        for result in resutls[\"results\"]:\n",
    "            if result[\"score\"] > 0.5:\n",
    "                text = f\"\"\"Document Title: {result[\"title\"]}\\n\n",
    "                Document URL: {result[\"url\"]}\\n\n",
    "                Document Content: {result[\"content\"]}\\n\n",
    "                Document Score: {result[\"score\"]}\\n\n",
    "                \"\"\"\n",
    "                content_results.append(text)\n",
    "        logger.info(f\"Search results: {content_results}\")\n",
    "        return content_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 03:11:03.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_tool\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mSetting up the search tool\u001b[0m\n",
      "\u001b[32m2025-05-12 03:11:03.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_tool\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mSearch tool set up: {'name': 'SearchTool', 'description': 'A tool for searching documents.'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "search_tool = SearchTool(name=\"SearchTool\", description=\"A tool for searching documents.\")\n",
    "search_tool.setup_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 03:11:10.962\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mRunning search tool with query: What is the capital of France?\u001b[0m\n",
      "\u001b[32m2025-05-12 03:11:13.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mSearch results: {'query': 'What is the capital of France?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'What is the Capital of France? - WorldAtlas', 'url': 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html', 'content': 'Learn about Paris, the largest and most populous city in France, and its history, geography, economy, tourism, and administration. Find out why Paris is called the City of Light and the City of Love.', 'score': 0.8170061, 'raw_content': None}, {'title': 'Paris | Definition, Map, Population, Facts, & History | Britannica', 'url': 'https://www.britannica.com/place/Paris', 'content': 'Paris is located in the north-central part of France along the Seine River. Paris,  city and capital of France, situated in the north-central part of the country. For centuries Paris has been one of the world’s most important and attractive cities. Paris is positioned at the centre of the Île-de-France region, which is crossed by the Seine, Oise, and Marne rivers. The garden effect of the Seine’s open waters and its tree-lined banks foster in part the appearance of Paris as a city well-endowed with green spaces. The Promenade Plantée is a partially elevated parkway built along an abandoned rail line and viaduct in the 12th arrondissement (municipal district) of Paris, on the right bank of the Seine River.', 'score': 0.7014069, 'raw_content': None}, {'title': 'Paris: Facts & Related Content - Encyclopedia Britannica', 'url': 'https://www.britannica.com/facts/Paris', 'content': \"Paris: Facts & Related Content News • Location | Ville-de-Paris, Île-de-France, France Time Zone | Central European Summer Time (CEST) • Central European Time (CET) Headquarters Of | International Astronomical Union•Pechiney•International New York Times•Charbonnages de France•Le Figaro•Agence France-Presse•Organisation Internationale de la Francophonie•European Aeronautic Defence and Space Company•International Council of Women•UNESCO•Total SA•Le Parisien•Air France•BNP Paribas•Section d'Or•Le Monde•Trilateral Commission•Carrefour SA•Société Générale•Compagnie de Saint-Gobain-Pont-à-Mousson•Le Journal des Débats•Elf Aquitaine•International Federation of Human Rights•France-Soir•Le Crédit Lyonnais•Banque de France•Reporters Without Borders•France Telecom SA•European Space Agency•Rhône-Poulenc SA•Larousse•Orchestre de Paris Related Events | Commune of Paris (1871)•Paris attacks of 2015•September Massacres (1792)•Paris 1924 Olympic Games•Massacre of St. Bartholomew's Day (1572)•Paris 1900 Olympic Games•Siege of Paris (1870–1871)•Paris Peace Conference (1919–1920)•Siege of Paris (885–886)•XYZ Affair Related Topics and References Related Quizzes and Features\", 'score': 0.52962893, 'raw_content': None}, {'title': 'Paris - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Paris', 'content': 'Paris is a global centre of finance, diplomacy, culture, and gastronomy, with an estimated population of 2 million residents in 2025. It has many famous landmarks, museums, and historical districts, and hosts major international organizations and events.', 'score': 0.49128565, 'raw_content': None}, {'title': 'France | History, Maps, Flag, Population, Cities, Capital, & Facts ...', 'url': 'https://www.britannica.com/place/France', 'content': '(more) France, country of northwestern Europe. Historically and culturally among the most important nations in the Western world, France has also played a highly significant role in international affairs, with former colonies in every corner of the globe. Bounded by the Atlantic Ocean and the Mediterranean Sea, the Alps and the Pyrenees, France has long provided a geographic, economic, and linguistic bridge joining northern and southern Europe. It is Europe’s most important agricultural producer and one of the world’s leading industrial powers. France(more) France is among the globe’s oldest nations, the product of an alliance of duchies and principalities under a single ruler in the Middle Ages.', 'score': 0.25589228, 'raw_content': None}], 'response_time': 0.99}\u001b[0m\n",
      "\u001b[32m2025-05-12 03:11:13.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mSearch results: ['Document Title: What is the Capital of France? - WorldAtlas\\n\\n                Document URL: https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\\n\\n                Document Content: Learn about Paris, the largest and most populous city in France, and its history, geography, economy, tourism, and administration. Find out why Paris is called the City of Light and the City of Love.\\n\\n                Document Score: 0.8170061\\n\\n                ', 'Document Title: Paris | Definition, Map, Population, Facts, & History | Britannica\\n\\n                Document URL: https://www.britannica.com/place/Paris\\n\\n                Document Content: Paris is located in the north-central part of France along the Seine River. Paris,  city and capital of France, situated in the north-central part of the country. For centuries Paris has been one of the world’s most important and attractive cities. Paris is positioned at the centre of the Île-de-France region, which is crossed by the Seine, Oise, and Marne rivers. The garden effect of the Seine’s open waters and its tree-lined banks foster in part the appearance of Paris as a city well-endowed with green spaces. The Promenade Plantée is a partially elevated parkway built along an abandoned rail line and viaduct in the 12th arrondissement (municipal district) of Paris, on the right bank of the Seine River.\\n\\n                Document Score: 0.7014069\\n\\n                ', \"Document Title: Paris: Facts & Related Content - Encyclopedia Britannica\\n\\n                Document URL: https://www.britannica.com/facts/Paris\\n\\n                Document Content: Paris: Facts & Related Content News • Location | Ville-de-Paris, Île-de-France, France Time Zone | Central European Summer Time (CEST) • Central European Time (CET) Headquarters Of | International Astronomical Union•Pechiney•International New York Times•Charbonnages de France•Le Figaro•Agence France-Presse•Organisation Internationale de la Francophonie•European Aeronautic Defence and Space Company•International Council of Women•UNESCO•Total SA•Le Parisien•Air France•BNP Paribas•Section d'Or•Le Monde•Trilateral Commission•Carrefour SA•Société Générale•Compagnie de Saint-Gobain-Pont-à-Mousson•Le Journal des Débats•Elf Aquitaine•International Federation of Human Rights•France-Soir•Le Crédit Lyonnais•Banque de France•Reporters Without Borders•France Telecom SA•European Space Agency•Rhône-Poulenc SA•Larousse•Orchestre de Paris Related Events | Commune of Paris (1871)•Paris attacks of 2015•September Massacres (1792)•Paris 1924 Olympic Games•Massacre of St. Bartholomew's Day (1572)•Paris 1900 Olympic Games•Siege of Paris (1870–1871)•Paris Peace Conference (1919–1920)•Siege of Paris (885–886)•XYZ Affair Related Topics and References Related Quizzes and Features\\n\\n                Document Score: 0.52962893\\n\\n                \"]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Document Title: What is the Capital of France? - WorldAtlas\\n\\n                Document URL: https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\\n\\n                Document Content: Learn about Paris, the largest and most populous city in France, and its history, geography, economy, tourism, and administration. Find out why Paris is called the City of Light and the City of Love.\\n\\n                Document Score: 0.8170061\\n\\n                ',\n",
       " 'Document Title: Paris | Definition, Map, Population, Facts, & History | Britannica\\n\\n                Document URL: https://www.britannica.com/place/Paris\\n\\n                Document Content: Paris is located in the north-central part of France along the Seine River. Paris,  city and capital of France, situated in the north-central part of the country. For centuries Paris has been one of the world’s most important and attractive cities. Paris is positioned at the centre of the Île-de-France region, which is crossed by the Seine, Oise, and Marne rivers. The garden effect of the Seine’s open waters and its tree-lined banks foster in part the appearance of Paris as a city well-endowed with green spaces. The Promenade Plantée is a partially elevated parkway built along an abandoned rail line and viaduct in the 12th arrondissement (municipal district) of Paris, on the right bank of the Seine River.\\n\\n                Document Score: 0.7014069\\n\\n                ',\n",
       " \"Document Title: Paris: Facts & Related Content - Encyclopedia Britannica\\n\\n                Document URL: https://www.britannica.com/facts/Paris\\n\\n                Document Content: Paris: Facts & Related Content News • Location | Ville-de-Paris, Île-de-France, France Time Zone | Central European Summer Time (CEST) • Central European Time (CET) Headquarters Of | International Astronomical Union•Pechiney•International New York Times•Charbonnages de France•Le Figaro•Agence France-Presse•Organisation Internationale de la Francophonie•European Aeronautic Defence and Space Company•International Council of Women•UNESCO•Total SA•Le Parisien•Air France•BNP Paribas•Section d'Or•Le Monde•Trilateral Commission•Carrefour SA•Société Générale•Compagnie de Saint-Gobain-Pont-à-Mousson•Le Journal des Débats•Elf Aquitaine•International Federation of Human Rights•France-Soir•Le Crédit Lyonnais•Banque de France•Reporters Without Borders•France Telecom SA•European Space Agency•Rhône-Poulenc SA•Larousse•Orchestre de Paris Related Events | Commune of Paris (1871)•Paris attacks of 2015•September Massacres (1792)•Paris 1924 Olympic Games•Massacre of St. Bartholomew's Day (1572)•Paris 1900 Olympic Games•Siege of Paris (1870–1871)•Paris Peace Conference (1919–1920)•Siege of Paris (885–886)•XYZ Affair Related Topics and References Related Quizzes and Features\\n\\n                Document Score: 0.52962893\\n\\n                \"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool.run(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is the Capital of France? - WorldAtlas',\n",
       "  'url': 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html',\n",
       "  'content': 'Learn about Paris, the largest and most populous city in France, and its history, geography, economy, tourism, and administration. Find out why Paris is called the City of Light and the City of Love.',\n",
       "  'score': 0.8170061,\n",
       "  'raw_content': None},\n",
       " {'title': 'Paris | Definition, Map, Population, Facts, & History | Britannica',\n",
       "  'url': 'https://www.britannica.com/place/Paris',\n",
       "  'content': 'Paris is located in the north-central part of France along the Seine River. Paris,  city and capital of France, situated in the north-central part of the country. For centuries Paris has been one of the world’s most important and attractive cities. Paris is positioned at the centre of the Île-de-France region, which is crossed by the Seine, Oise, and Marne rivers. The garden effect of the Seine’s open waters and its tree-lined banks foster in part the appearance of Paris as a city well-endowed with green spaces. The Promenade Plantée is a partially elevated parkway built along an abandoned rail line and viaduct in the 12th arrondissement (municipal district) of Paris, on the right bank of the Seine River.',\n",
       "  'score': 0.7014069,\n",
       "  'raw_content': None},\n",
       " {'title': 'Paris: Facts & Related Content - Encyclopedia Britannica',\n",
       "  'url': 'https://www.britannica.com/facts/Paris',\n",
       "  'content': \"Paris: Facts & Related Content News • Location | Ville-de-Paris, Île-de-France, France Time Zone | Central European Summer Time (CEST) • Central European Time (CET) Headquarters Of | International Astronomical Union•Pechiney•International New York Times•Charbonnages de France•Le Figaro•Agence France-Presse•Organisation Internationale de la Francophonie•European Aeronautic Defence and Space Company•International Council of Women•UNESCO•Total SA•Le Parisien•Air France•BNP Paribas•Section d'Or•Le Monde•Trilateral Commission•Carrefour SA•Société Générale•Compagnie de Saint-Gobain-Pont-à-Mousson•Le Journal des Débats•Elf Aquitaine•International Federation of Human Rights•France-Soir•Le Crédit Lyonnais•Banque de France•Reporters Without Borders•France Telecom SA•European Space Agency•Rhône-Poulenc SA•Larousse•Orchestre de Paris Related Events | Commune of Paris (1871)•Paris attacks of 2015•September Massacres (1792)•Paris 1924 Olympic Games•Massacre of St. Bartholomew's Day (1572)•Paris 1900 Olympic Games•Siege of Paris (1870–1871)•Paris Peace Conference (1919–1920)•Siege of Paris (885–886)•XYZ Affair Related Topics and References Related Quizzes and Features\",\n",
       "  'score': 0.52962893,\n",
       "  'raw_content': None},\n",
       " {'title': 'Paris - Wikipedia',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Paris',\n",
       "  'content': 'Paris is a global centre of finance, diplomacy, culture, and gastronomy, with an estimated population of 2 million residents in 2025. It has many famous landmarks, museums, and historical districts, and hosts major international organizations and events.',\n",
       "  'score': 0.49128565,\n",
       "  'raw_content': None},\n",
       " {'title': 'France | History, Maps, Flag, Population, Cities, Capital, & Facts ...',\n",
       "  'url': 'https://www.britannica.com/place/France',\n",
       "  'content': '(more) France, country of northwestern Europe. Historically and culturally among the most important nations in the Western world, France has also played a highly significant role in international affairs, with former colonies in every corner of the globe. Bounded by the Atlantic Ocean and the Mediterranean Sea, the Alps and the Pyrenees, France has long provided a geographic, economic, and linguistic bridge joining northern and southern Europe. It is Europe’s most important agricultural producer and one of the world’s leading industrial powers. France(more) France is among the globe’s oldest nations, the product of an alliance of duchies and principalities under a single ruler in the Middle Ages.',\n",
       "  'score': 0.25589228,\n",
       "  'raw_content': None}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_loader = DocumentPDFLoader(file_path)\n",
    "\n",
    "class DocumentPDFLoader(BaseLoader):\n",
    "    def __init__(self, filepath: List[str]) -> None: \n",
    "        self._filepath = filepath if isinstance(filepath, list) else [filepath]\n",
    "        self._loaders = [PyPDFLoader(file) for file in self._filepath]\n",
    "    \n",
    "    def lazy_load (self)->Iterator[Document]:\n",
    "        for loader in self._loaders:\n",
    "            for doc in loader.load():\n",
    "                yield doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = DocumentPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Joint Automatic Speech Recognition And Structure\\nLearning For Better Speech Understanding\\nJiliang Hu1, Zuchao Li 2,*, Mengjia Shen 3, Haojun Ai 1, Sheng Li 4, Jun Zhang 3\\n1Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education,\\nSchool of Cyber Science and Engineering, Wuhan University, Wuhan, China,\\n2School of Computer Science, Wuhan University, Wuhan, China,\\n3Wuhan Second Ship Design and Research Institute, Wuhan, China,\\n4National Institute of Information and Communications Technology, Japan.\\nAbstract—Spoken language understanding (SLU) is a structure\\nprediction task in the field of speech. Recently, many works\\non SLU that treat it as a sequence-to-sequence task have\\nachieved great success. However, This method is not suitable\\nfor simultaneous speech recognition and understanding. In this\\npaper, we propose a joint speech recognition and structure\\nlearning framework (JSRSL), an end-to-end SLU model based\\non span, which can accurately transcribe speech and extract\\nstructured content simultaneously. We conduct experiments on\\nname entity recognition and intent classification using the Chinese\\ndataset AISHELL-NER and the English dataset SLURP. The\\nresults show that our proposed method not only outperforms the\\ntraditional sequence-to-sequence method in both transcription\\nand extraction capabilities but also achieves state-of-the-art\\nperformance on the two datasets.\\nIndex Terms—Speech Recognition, Spoken Language Under-\\nstanding, Information Extraction.\\nI. I NTRODUCTION\\nAutomatic speech recognition (ASR) aims to convert human\\nspeech into text in the corresponding language [1]. On the\\nother hand, SLU seeks to enhance machines’ ability to com-\\nprehend and react to human language by extracting specific\\nstructured information from speech. SLU tasks encompass\\nname entity recognition (NER), intent classification (IC),\\nsentiment analysis (SA), among others [2]. SLU models can\\nbe categorized into two types: the pipeline model, which\\nuses an ASR module first, followed by a natural language\\nunderstanding (NLU) module in a sequential manner, and the\\nend-to-end (E2E) model, which directly converts speech rep-\\nresentations into structured information. The pipeline model\\nencounters challenges related to error propagation due to the\\nweak linkage between the two modules. Currently, there is a\\ngreater emphasis on the E2E model [3].\\nThere has been significant progress in the field of SLU.\\nHowever, few works consider how to improve or maintain\\nthe accuracy of transcription during spoken language un-\\nderstanding. For NER, most papers utilize the sequence-to-\\n*Corresponding author.\\nThis work was supported by the National Natural Science Foundation of\\nChina (No. 62306216), the Natural Science Foundation of Hubei Province of\\nChina (No. 2023AFB816), the Fundamental Research Funds for the Central\\nUniversities (No. 2042023kf0133).\\nCodes are available at https://github.com/193746/JSRSL\\nsequence method to achieve entity recognition. For instance,\\n[4] use ” — ] ”, ” $ ] ”, and ” { ]” to annotate Person, Place,\\nand Organization in the transcribed text. Subsequently, the\\nannotated text was used to train ASR model. However, [5]\\nhave shown that this method of labeling in the transcribed\\ntext can affect the recognition performance of the ASR model\\nand increase transcription errors. For classification tasks such\\nas SA and IC, some studies [6], [7] also achieve success by\\nannotating classification types in the transcribed text, while\\nothers [8], [9] directly connect the output of the ASR model\\nto a classification layer. The former will also reduce the\\nperformance of transcription, while the latter cannot transcribe\\nthe speech simultaneously. It is a significant challenge at the\\nSLU to ensure accurate transcription by the ASR module and\\ncorrect extraction by the SLU module simultaneously.\\nIn this paper, we propose a joint speech recognition and\\nstructure learning framework (JSRSL), an E2E SLU model\\nbased on span, which can accurately recognize speech while\\nextracting structured information. This model utilizes a re-\\nfinement module to enhance text representations for struc-\\nture prediction. Specifically, we adopt a parallel transformer\\nfor non-autoregressive end-to-end speech recognition as our\\nframework and introduce the hidden layer output of the ASR\\ndecoder into a span for structure prediction. We additionally\\nintroduce a refinement module between the ASR model and\\nthe span to strengthen extraction performance. We conduct\\nexperiments on NER and IC using the AISHELL-NER and\\nSLURP. The results indicate that the proposed idea has supe-\\nrior capabilities in speech transcription and speech understand-\\ning compared to the traditional sequence-to-sequence method.\\nIt also outperforms the current state-of-the-art (SOTA) on the\\ntwo datasets.\\nII. R ELATED WORK\\nSo far, NLU has been well developed, and many works\\nhave involved key tasks of natural language processing (NLP),\\nsuch as natural language inference (NLI) [10], semantic role\\nlabeling (SRL) [11], [12], and SA [13], [14]. In recent years, a\\nnumber of excellent SLU systems have emerged. [15] propose\\nan acoustic model called TDT and they attempt to use the\\nproposed model to do SLU tasks by jointly optimizing token\\narXiv:2501.07329v2  [cs.SD]  17 Jan 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='prediction and temporal prediction. [8] introduce a novel\\nmethod known as context-aware fine-tuning. They incorporate\\na context module into the pre-trained model to extract the\\ncontext embedding vector, which is subsequently used as\\nextra features for the ultimate prediction. [16] combine pre-\\ntrained self-supervised learning (SSL), ASR, language model\\n(LM) and SLU models to explore the model combination\\nthat can achieve the best SLU performance and shows that\\npre-training approaches rooted in self-supervised learning are\\nmore potent than those grounded in supervised learning. [17]\\ndevelop compositional end-to-end SLU systems that initially\\ntransform spoken utterances into a series of token represen-\\ntations, followed by token-level classification using a NLU\\nsystem. [6] suggest a comprehensive approach that combines\\na multilingual pretrained speech model, a text model, and\\nan adapter to enhance speech understanding. [18] incorporate\\nsemantics into speech encoders and present a task-agnostic\\nunsupervised technique for integrating semantic information\\nfrom large language models (LLMs) [19], [20] into self-\\nsupervised speech encoders without the need for labeled audio\\ntranscriptions.\\nIII. M ETHOD\\nEncoderDecoderPredictorRefiner0011000001003000Start:End:PersonTimeASR ModuleSpan Module\\nplease wake joe up at seven thirty am\\nIntent:Emotion:65Alarm-SetNeutral𝑋1:𝑇 𝐻1:𝑇𝐸 𝐻1:𝑇𝐸 𝑁′,𝐸𝑎 𝑌′ 𝐻1:𝑁′𝐷 𝐻1:𝑁′𝑅 𝑆′ 𝑍′ Bridge𝐻1:𝑁′𝐼𝑇ROPESelf-Attention Poolng\\nFig. 1. The structure of our proposed framework, JSRSL.\\nFigure 1 shows the full overview of our proposed frame-\\nwork, JSRSL. Follow [21], we adopt this parallel Transformer\\nfor non-autoregressive end-to-end speech recognition as our\\nASR module framework. In addition, we use a Refiner module\\nto connect the ASR module and span [22], and a Bridge\\nmodule to connect the ASR module and Refiner module. The\\nBridge module can convert acoustic representation into text\\nrepresentation. The Refiner module can strengthen the text\\nrepresentation of upstream inputs and extract richer features\\nfor structured learning. Before doing structure learning, we\\napply rotary embedded encoding (ROPE) [23] into span.\\nROPE can leverage the boundary information of span by\\ninjecting relative position information.\\nA. SLU Representation Learning\\nLet X be a speech sequence with T frames, X =\\n{x1, x2, x3, . . . , xT }. Y is a sequence of tokens, and its\\nlength is N. Each token is in the vocabulary V , Y =\\n{y1, y2, y3, . . . , yN | yi ∈ V }. The ASR module gets X as\\ninput and outputs the decoding result Y\\n′\\nand the Decoder’s\\nhidden representation HD\\n1:N′ .\\nThe Refiner module is essentially a NLU model, and it\\nrequires a text vector as input. It will not function correctly\\nif the acoustic hidden representation HD\\n1:N′ is fed directly. If\\nwe use the decoded token sequence as input, the computation\\ngraph will be truncated, preventing the joint training of the\\nASR module and Refiner module. Therefore, a Bridge module\\nis needed to implement the conversion from acoustic hid-\\nden representation HD\\n1:N′ to initial text hidden representation\\nHIT\\n1:N′ . Our Bridge module first calculates the probability\\ndistribution of the token sequence Y p based on HD\\n1:N′ , then\\nit maps the distribution to the embedding layer of the Refiner\\nmodule to convert Y p into the initial text representation HIT\\n1:N′ .\\nThe specific operation of the map is to multiply Y p and the\\nweights of embedding layer We\\n1:V in a matrix.\\nY p = Softmax (Wp\\nV HD\\n1:N′ + bp\\nV )\\nHIT\\n1:N′ = Matmul(Y p, We\\n1:V )\\nSubsequently, we feed HIT\\n1:N′ into the Refiner module for\\nstrengthening text representation, resulting in a refined text\\nfeatures HR\\n1:N′ . It is used as the input features for SLU\\ndownstream tasks. The Refiner is implemented of multiple\\nstacked Transformer layers.\\nB. Span-based Structure Learning\\nWe utilize span to extract the token sequence of each\\nspecific information in a given speech sequence X along with\\ntheir corresponding types. Let S be the pointer sequence, the\\nset of pointer labels be Ω, and S = {s1, s2, s3, . . . , sN |\\nsi ∈ Ω}. The pointer sequence corresponds one-to-one to\\nthe token sequence obtained from the speech transcription.\\nWe define two pointer sequences, namely the starting pointer\\nsequence Sstart and the ending pointer sequence Send. The\\nformer is used to locate the head position of the specific\\ninformation, Ωstart = {0, 1}, and the latter is used to locate\\nthe tail position of the specific information and identify its\\ntype, Ωend = {0, 1, 2. . . Ntype}.\\nFor ordinary structure extraction task, we firstly use a linear\\nlayer called Dense to double the dimension of HR\\n1:N′ , which\\nis aimed at make two pointers use different feature vectors.\\nThe output of Dense is evenly divided into two parts, d1\\nand d2. Then, ROPE is employed to embed relative position\\ninformation for the pointer sequence.\\nd1, d2 = Dense(HR\\n1:N′ )'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='S\\n′\\nstart, S\\n′\\nend = ROP E(d1), ROP E(d2)\\nFor classification tasks, we treat them as sentence-level\\nspan. Let Z be the classification result, and the set of classi-\\nfication result be Ωcls, Ωcls = {1, 2, . . . , Ntype}. Firstly, we\\nuse ROPE to embed relative position information and get the\\ntoken-level classification features. Z1:N′ .\\nZ1:N′ = ROPE (HR\\n1:N′ )\\nDrawing inspiration from [24], a modified self-attention\\npooling mechanism is utilized to comprehensively extract\\nsentence-level classification result Z\\n′\\nfrom token-level fea-\\ntures. The process of the self-attention pooling mechanism is\\nas follows, AZ\\n1:N′ is attention value of Z1:N′ calculated by a\\nlinear layer.\\nAZ\\n1:N′ = Softmax (W1Z1:N′ + b1)\\nZ\\n′\\n= arg max\\nNtype\\n(WNtype (\\nX\\nN′\\nAZ\\n1:N′ × Z1:N′ ) + bNtype )\\nC. Joint Optimization\\nThe adopted ASR module calculates three loss functions\\nwhen training: the cross-entropy (CE), the mean absolute error\\n(MAE), and the minimum word error rate (MWER) loss. CE\\nand MWER are used to optimize the model’s transcription\\nability, while MAE guides the predictor to convergence. Ac-\\ncording to [21], the loss function of the ASR part is:\\nLasr = γLCE + LMAE + LN\\nwerr(x, y∗)\\nLN\\nwerr(x, y∗) =\\nX\\nyi∈Sample\\nbP(yi | x)[W(yi, y∗) − cW]\\nWe also use CE to optimize the model’s ability for structure\\nprediction. The total loss function can be formulated as\\nfollows, where α is used to control the proportion of ASR\\nloss and structured loss, α ∈ (0, 1).\\nLtotal = (1 − α)Lasr + αLsp\\nIV. E XPERIMENT\\nA. Configuration\\nWe employ AISHELL-NER [5] and SLURP’s NER (slot\\nfilling) subset for NER, and SLURP’s IC subset for IC [25].\\nFollowing the previous work [26] that used SLURP, the\\nprovided synthetic data is utilized for training. For assess-\\nment metrics, SLURP-NER utilizes WER and SLURP-F1,\\nSLURP-IC employs WER and micro-F1, while AISHELL-\\nNER uses CER and micro-F1. Our baseline, Seq2Seq, ap-\\nplies the sequence-to-sequence method to the JSRSL’s ASR\\nmodule and annotates like [5]. Another baseline, Pipe, is\\nthe concatenation of the JSRSL’s ASR module and a NLU\\nmodule, predicting structure by span. The models chosen as\\nbenchmarks all adopt sequence generation method by adding\\nspecial symbols in transcribed text or generating structured\\nsequence directly.\\nWe build the experiment environment based on Funasr\\n[27] and ModelScope, utilizing 220M Chinese and English\\nParaformer for experiments. The pre-training parameters of\\nBERT-base [28] is adopted to initialize the Refiner of JSRSL\\nand the NLU module of Pipe. The α of the loss is set to 0.5.\\nThe models are trained until converges. We consistently use\\nthe Adam optimizer with a learning rate of 5e-5.\\nB. Main Result\\nTABLE I\\nCOMPARISON RESULTS WITH BENCHMARKS IN AISHELL-NER AND\\nSLURP. I N SLURP, ALL MODELS CONDUCT JOINT TRAINING AND\\nEVALUATION FOR NER AND IC TASKS . THE BERT IN [5]’ S PIPELINE\\nMODEL ARE PRE -TRAINED . THE POST AED USES 1K EXTERNAL DATA .\\nModel Paradigm\\nAISHELL-NER\\nCER (↓) F1 ( ↑)\\nTransformer [5] E2E 9.25 64.34\\nConformer [5] E2E 4.79 73.37\\nTransformer+BERT* [5] Pipeline 9.25 65.95\\nConformer+BERT* [5] Pipeline 4.79 74.90\\nSeq2Seq (Ours) E2E 3.48 76.17\\nPipe (Ours) Pipeline 1.76 77.98\\nJSRSL (Ours) E2E 1.71 80.85\\nModel Paradigm\\nSLURP\\nWER (↓) SLU F1 ( ↑) IC F1 ( ↑)\\nRNNT→BertNLU [29] Pipeline 15.20 72.35 82.45\\nEspnet SLU [26] E2E - 71.90 86.30\\nPostDec AED* [6] E2E - 80.08 89.33\\nConformer-RNNT [7] E2E 14.20 77.22 90.10\\nConformer-CTC [7] E2E 14.50 69.30 85.50\\nTDT 0-6 [15] E2E - 80.61 89.28\\nTDT 0-8 [15] E2E - 79.90 90.07\\nSeq2Seq (Ours) E2E 18.83 68.84 86.63\\nPipe (Ours) Pipeline 13.61 76.62 86.67\\nJSRSL (Ours) E2E 13.14 80.17 91.03\\nTable I presents the comparison results with baselines\\nand benchmarks. Before the joint training of Paraformer and\\nBERT, the Paraformer in JSRSL is pre-trained by audio-\\ntext pairs from AISHELL-NER or SLURP and the BERT\\nis pre-trained by the vocabulary of Paraformer and the text-\\nspan pairs of corresponding dataset. The results show that\\nthe proposed model, based on span, outperforms the E2E\\nand pipeline baselines and benchmarks using the sequence-\\nto-sequence method in both structure extraction and ASR.\\nBy using span, we can distinguish between ASR and SLU\\ntasks, avoid the impact of extra structured annotations on\\ntranscription accuracy and allow for more precise extraction\\nof structured information. In addition, Pipe is worse than\\nJSRSL due to issues with error propagation of pipeline model,\\nindicating that the E2E model is more promising. In SLURP,\\nTDT models and PostDEC AED have the best SLU perfor-\\nmance in the benchmarks. TDT models adopt the token and\\nduration transformer architecture, which enhances the ability\\nof sequence generation and accelerates the inference speed of\\nthe original RNN-Transducer [30] through joint learning of\\ntoken and duration prediction. ”0-X” refers to the maximum\\nduration of duration configurations as ”X”. The performance of\\nTDT is comparable to that of our JSRSL, but it is influenced by\\nspecific configurations. TDT 0-6 performs better on SLURP-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='TABLE II\\nABLATION RESULTS IN AISHELL-NER AND SLURP. S AMPLER IS A COMPONENT IN PARAFORMER USED TO ENHANCE TRAINING EFFECTIVENESS .\\nModel AISHELL-NER SLURP-NER SLURP-IC\\nRefiner CER (↓) Precision (↑) Recall (↑) F1 (↑) WER (↓) Precision (↑) Recall (↑) SLU F1 (↑) WER (↓) F1 (↑)\\nJSRSL ✕ 1.76 83.21 78.15 80.46 10.79 80.25 73.37 76.66 13.84 88.19\\nw/o ROPE ✕ 1.78 82.50 78.00 80.06 10.81 79.33 73.44 76.27 13.87 87.93\\nw/o Sampler✕ 1.75 82.98 78.00 80.32 10.72 80.14 73.20 76.51 14.05 88.19\\nJSRSL ✓ 1.77 81.89 79.27 80.45 11.17 83.26 77.46 80.26 13.30 89.90\\nw/o pretrain✓ 1.71 82.36 79.54 80.85 12.17 82.41 76.36 79.27 13.53 89.67\\nNER, while TDT 0-8 performs better on SLURP-IC. PostDec\\nAED uses an adaptor to concatenate the speech encoder\\nand NLU model. The adaptor tries to minimize the distance\\nbetween the speech representation and the text representation.\\nBefore finetune the SLU system, it must undergo adapter\\npretraining, which optimizes the loss between predicted text\\nand real text. Although PostDec AED’s performance is just\\nslightly inferior to our JSRSL, its training process is more\\ncomplex and requires a large amount of external data.\\nC. Ablation Study\\nTABLE III\\nASR RESULTS (CER ON AISHELL-NER AND WER ON SLURP) AMONG\\nPARAFORMER , WHISPER , XLSR-53 AND OUR JSRSL WITH SIMILAR\\nSIZE .\\nModel Params AISHELL-NER SLURP\\nXLSR-53 [31] 315M 4.12 22.10\\nWhisper [32] 244M 5.54 16.89\\nParaformer 220M 1.76 13.61\\nJSRSL (Ours) 314M 1.71 13.14\\nTable II shows the ablation results. The proposed model\\nshows a certain improvement after introducing ROPE. In\\naddition, in SLURP-NER and AISHELL-NER, we find that\\nSampler does not significantly improve model’s ASR perfor-\\nmance, but rather increases its spoken understanding ability.\\nIn AISHELL-NER and SLURP-IC, the introduction of refine-\\nment module leads to a further decrease in the model’s CER\\nor WER and an increase in F1 score. However, in SLURP-\\nNER, we find that with refinement module, while JSRSL\\nenhances SLU F1, WER increases. But overall, the introduc-\\ntion of refinement modules is beneficial for improving model\\nperformance. In AISHELL-NER, the JSRSL with pre-trained\\nParaformer does not achieve better performance, possibly\\ndue to overfitting. Nevertheless, in SLURP, pre-trained ASR\\nmodule improves the overall ability of JSRSL. Additionally,\\nto assess the effectiveness of the adopted ASR module, we\\ncarry out an ASR experiment detailed in Table III. The results\\nindicate that Paraformer’s ASR performance on AISHELL-\\nNER and SLURP surpasses that of Whisper and XLSR-53.\\nThis demonstrates the robustness of our model’s ASR module.\\nFurthermore, JSRSL achieves better ASR capability after joint\\ntraining.\\nPCCS ST-CMDSCommonV oice\\n4\\n6\\n8\\n10\\nCER\\nParaformer-ZhSeq2SeqJSRSL W/O RefinerJSRSL W/ Refiner\\nTED-LIUMLibriSpeechCommonV oice\\n5\\n10\\n15\\n20\\n25\\nWER\\nParaformer-EnSeq2SeqJSRSL W/O RefinerJSRSL W/ Refiner\\nFig. 2. Out-of-Distribution experiment result. The results on the left are in\\nthe Chinese datasets and the results on the right are in the English datasets.\\nD. OOD Analysis\\nThis section conducts Out-of-Distribution (OOD) experi-\\nment with unseen ASR datasets. The Chinese ASR datasets\\nare Primewords Chinese Corpus Set 1 (PCCS) [33], Free ST\\nChinese Mandarin Corpus (ST-CMDS), and Common V oice\\n(Zh) [34]. The English ASR datasets include Librispeech\\n[35], Ted-LIUM [36], and Common V oice (En). Each dataset\\nuniformly uses the first 8000 training samples for the ex-\\nperiment. When evaluating the Seq2Seq, the special symbols\\nthat indicate name entities are ignored. Figure 2 presents the\\nOOD results. The original Chinese or English Paraformer\\ndemonstrates better generalization ability and achieves the\\nlowest CER or WER among all datasets. Although all trained\\nmodels’ transcription performance has decreased, compared to\\nSeq2Seq, which uses a sequence generation method, the span-\\nbased models exhibit lower recognition error rates. What’s\\nmore, JSRSL with refinement module performs better than that\\nwithout refinement module. This indicates that the proposed\\nmethod results in less performance loss in speech recognition,\\nmaking it more suitable for joint ASR and structure prediction.\\nV. C ONCLUSION\\nThis paper proposes a method called JSRSL for jointly ASR\\nand structure prediction based on span. This approach aims\\nto ensure accurate transcribing and understanding of speech\\nsimultaneously. Experiments have shown that the proposed\\nscheme is superior to traditional sequence-to-sequence method\\nin both transcription and extraction capabilities, and achieves\\nSOTA performance on the datasets used for the experiments.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='REFERENCES\\n[1] J. Li et al. , “Recent advances in end-to-end automatic speech recog-\\nnition,” APSIPA Transactions on Signal and Information Processing ,\\nvol. 11, no. 1, 2022.\\n[2] S. Arora, H. Futami, J.-w. Jung, Y . Peng, R. Sharma, Y . Kashiwagi,\\nE. Tsunoo, K. Livescu, and S. Watanabe, “Universlu: Universal spo-\\nken language understanding for diverse tasks with natural language\\ninstructions,” in Proceedings of the 2024 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Papers) , 2024, pp.\\n2754–2774.\\n[3] A. Pasad, F. Wu, S. Shon, K. Livescu, and K. Han, “On the use of\\nexternal data for spoken named entity recognition,” in Proceedings of\\nthe 2022 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies , 2022,\\nConference Proceedings, pp. 724–737.\\n[4] H. Yadav, S. Ghosh, Y . Yu, and R. R. Shah, “End-to-end named entity\\nrecognition from english speech,” Organization, vol. 2299, no. 1473, p.\\n3772, 2020.\\n[5] B. Chen, G. Xu, X. Wang, P. Xie, M. Zhang, and F. Huang, “Aishell-\\nner: Named entity recognition from chinese speech,” in ICASSP 2022-\\n2022 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP). IEEE, 2022, Conference Proceedings, pp. 8352–\\n8356.\\n[6] P. Denisov and N. T. Vu, “Leveraging multilingual self-supervised\\npretrained models for sequence-to-sequence end-to-end spoken language\\nunderstanding,” arXiv preprint arXiv:2310.06103 , 2023.\\n[7] K. Singla, S. Jalalvand, Y .-J. Kim, A. Moreno Daniel, S. Bangalore,\\nA. Ljolje, and B. Stern, “1spu: 1-step speech processing unit,” arXiv\\ne-prints, p. arXiv: 2311.04753, 2023.\\n[8] S. Shon, F. Wu, K. Kim, P. Sridhar, K. Livescu, and S. Watan-\\nabe, “Context-aware fine-tuning of self-supervised speech models,” in\\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP). IEEE, 2023, Conference Proceedings,\\npp. 1–5.\\n[9] S. Shon, K. Kim, P. Sridhar, Y .-T. Hsu, S. Watanabe, and K. Livescu,\\n“Generative context-aware fine-tuning of self-supervised speech mod-\\nels,” arXiv preprint arXiv:2312.09895 , 2023.\\n[10] Z. Zhang, Y . Wu, H. Zhao, Z. Li, S. Zhang, X. Zhou, and X. Zhou,\\n“Semantics-aware bert for language understanding,” in Proceedings of\\nthe AAAI conference on artificial intelligence , vol. 34, no. 05, 2020, pp.\\n9628–9635.\\n[11] S. He, Z. Li, H. Zhao, and H. Bai, “Syntax for semantic role labeling,\\nto be, or not to be,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\n2018, pp. 2061–2071.\\n[12] Z. Li, S. He, H. Zhao, Y . Zhang, Z. Zhang, X. Zhou, and X. Zhou,\\n“Dependency or span, end-to-end uniform semantic role labeling,” in\\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 33,\\nno. 01, 2019, pp. 6730–6737.\\n[13] H. Jing, Z. Li, H. Zhao, and S. Jiang, “Seeking common but distinguish-\\ning difference, a joint aspect-based sentiment analysis model,” arXiv\\npreprint arXiv:2111.09634, 2021.\\n[14] L. Peng, Z. Li, and H. Zhao, “Sparse fuzzy attention for structured\\nsentiment analysis,” arXiv preprint arXiv:2109.06719 , 2021.\\n[15] H. Xu, F. Jia, S. Majumdar, H. Huang, S. Watanabe, and B. Ginsburg,\\n“Efficient sequence transduction by jointly predicting tokens and dura-\\ntions,” ArXiv, vol. abs/2304.06795, 2023.\\n[16] Y . Peng, S. Arora, Y . Higuchi, Y . Ueda, S. Kumar, K. Ganesan,\\nS. Dalmia, X. Chang, and S. Watanabe, “A study on the integration\\nof pre-trained ssl, asr, lm and slu models for spoken language under-\\nstanding,” in 2022 IEEE Spoken Language Technology Workshop (SLT).\\nIEEE, 2023, Conference Proceedings, pp. 406–413.\\n[17] S. Arora, S. Dalmia, B. Yan, F. Metze, A. W. Black, and S. Watanabe,\\n“Token-level sequence labeling for spoken language understanding using\\ncompositional end-to-end models,” in Findings of the Association for\\nComputational Linguistics: EMNLP 2022 , 2022, Conference Proceed-\\nings, pp. 5419–5429.\\n[18] D. Xu, S. Dong, C. Wang, S. Kim, Z. Lin, A. Shrivastava, S.-W. Li,\\nL.-H. Tseng, A. Baevski, G.-T. Lin, H.-y. Lee, Y . Sun, and W. Wang,\\n“Introducing semantics into speech encoders,” in Annual Meeting of the\\nAssociation for Computational Linguistics , 2022, Conference Proceed-\\nings.\\n[19] T. B. Brown, “Language models are few-shot learners,” arXiv preprint\\narXiv:2005.14165, 2020.\\n[20] Z. Li, S. Zhang, H. Zhao, Y . Yang, and D. Yang, “Batgpt: A bidirectional\\nautoregessive talker from generative pre-trained transformer,” arXiv\\npreprint arXiv:2307.00360, 2023.\\n[21] Z. Gao, S. Zhang, I. McLoughlin, and Z. Yan, “Paraformer: Fast and\\naccurate parallel transformer for non-autoregressive end-to-end speech\\nrecognition,” arXiv preprint arXiv:2206.08317 , 2022.\\n[22] J. Fu, X.-J. Huang, and P. Liu, “Spanner: Named entity re-/recognition\\nas span prediction,” in Proceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long\\nPapers), 2021, Conference Proceedings, pp. 7183–7195.\\n[23] J. Su, A. Murtadha, S. Pan, J. Hou, J. Sun, W. Huang, B. Wen, and\\nY . Liu, “Global pointer: Novel efficient span-based approach for named\\nentity recognition,” arXiv preprint arXiv:2208.03054 , 2022.\\n[24] S. Shon, A. Pasad, F. Wu, P. Brusco, Y . Artzi, K. Livescu, and K. J.\\nHan, “Slue: New benchmark tasks for spoken language understanding\\nevaluation on natural speech,” in ICASSP 2022-2022 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\\nIEEE, 2022, Conference Proceedings, pp. 7927–7931.\\n[25] E. Bastianelli, A. Vanzo, P. Swietojanski, and V . Rieser, “Slurp: A\\nspoken language understanding resource package,” in Proceedings of the\\n2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP), 2020, Conference Proceedings, pp. 7252–7262.\\n[26] S. Arora, S. Dalmia, P. Denisov, X. Chang, Y . Ueda, Y . Peng, Y . Zhang,\\nS. Kumar, K. Ganesan, and B. Yan, “Espnet-slu: Advancing spoken\\nlanguage understanding through espnet,” in ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP). IEEE, 2022, Conference Proceedings, pp. 7167–7171.\\n[27] Z. Gao, Z. Li, J. Wang, H. Luo, X. Shi, M. Chen, Y . Li, L. Zuo, Z. Du,\\nand Z. Xiao, “Funasr: A fundamental end-to-end speech recognition\\ntoolkit,” arXiv preprint arXiv:2305.11013 , 2023.\\n[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805, 2018.\\n[29] A. Raju, M. Rao, G. Tiwari, P. Dheram, B. Anderson, Z. Zhang, C. Lee,\\nB. Bui, and A. Rastrow, “On joint training with interfaces for spoken\\nlanguage understanding,” arXiv preprint arXiv:2106.15919 , 2021.\\n[30] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “Rnn-\\ntransducer with stateless prediction network,” in ICASSP 2020-2020\\nIEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP). IEEE, 2020, pp. 7049–7053.\\n[31] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,\\nK. Singh, P. V on Platen, Y . Saraf, J. Pinoet al., “Xls-r: Self-supervised\\ncross-lingual speech representation learning at scale,” arXiv preprint\\narXiv:2111.09296, 2021.\\n[32] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\\nI. Sutskever, “Robust speech recognition via large-scale weak super-\\nvision,” in International Conference on Machine Learning . PMLR,\\n2023, Conference Proceedings, pp. 28 492–28 518.\\n[33] L. Primewords Information Technology Co., “Primewords chinese cor-\\npus set 1,” 2018, https://www.primewords.cn.\\n[34] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer,\\nR. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice:\\nA massively-multilingual speech corpus,” in Proceedings of the 12th\\nConference on Language Resources and Evaluation (LREC 2020), 2020,\\npp. 4211–4215.\\n[35] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr\\ncorpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Section 1\\nQuestion 1-10Complete the notes below.WriteONE WORD AND/OR A NUMBERfor each answer.\\nBuckworth Conservation GroupRegular activities\\nBeach●   making sure the beach does not have 1 …………………… on it●   no 2 ……………………\\nNature reserve\\n●   maintaining paths●   nesting boxes for birds installed●   next task is taking action to attract 3 …………………… to the place●   identifying types of 4 ……………………●   building a new 5 ……………………\\nForthcoming events\\nSaturday\\n●   meet at Dunsmore Beach car park●   walk across the sands and reach the 6 ……………………●   take a picnic●   wear appropriate 7 ……………………\\nWoodwork session\\n●   suitable for 8 …………………… to participate in●   making 9 …………………… out of wood●   17th, from 10 a.m. to 3 p.m.●   cost of session (no camping): 10 £ ……………………'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Questions 17 and 18ChooseTWOletters,A-E.\\nWhichTWOtypes of creature might come close to theboat?\\nA sea eagleB fur sealsC dolphinsD whalesE penguins\\nQuestions 19 and 20ChooseTWOletters,A-E.\\nWhichTWOpoints does Lou make about the caves?\\nA   Only large tourist boats can visit them.B   The entrances to them are often blocked.C   It is too dangerous for individuals to go near them.D   Someone will explain what is inside them.E   They cannot be reached on foot.\\nSection 3\\nQuestion 21-26Choose the correct letter,A,BorC.\\nWork experience for veterinary science students21   What problem did both Diana and Tim have when arranging their workexperience?A make initial contact with suitable farmsB organising transport to and from the farmC finding a placement for the required length oftime\\n22   Tim was pleased to be able to helpA a lamb that had a broken leg.B a sheep that was having difficulty giving birth.C a newly born lamb that was having trouble feeding.\\n23   Diana says the sheep on her farm'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='A were of various different varieties.B was mainly reared for their meat.C had better quality wool than sheep on the hills.\\n24   What did the students learn about adding supplements to chicken feed?A These should only be given if especially needed.B It is worth paying extra for the most effectiveones.C The amount given at one time should be limited.\\n25   What happened when Diana was working with dairy cows?A She identified some cows incorrectly.B She accidentally threw some milk away.C She made a mistake when storing milk.\\n26   What did both farmers mention about vets and farming?A Vets are failing to cope with some aspects ofanimal health.B There needs to be a fundamental change in thetraining of vets.C Some jobs could be done by the farmer rather thanby a vet.\\nQuestions 27-30What opinion do the students give about each of the following modules on their veterinaryscience course?\\nChooseFOURanswers from the box and write the correctletter,A-F, next to questions27-30.\\nOpinions\\nA.Tim found this easier than expected.B.Tim found this easier than expected.C.Diana may do some further study on this.D.They both found the reading required for this difficult.E. Tim was shocked at something he learned in this module.F. They were both surprised at how little is known about some aspects of this.\\nModules on Veterinary Science course27 Medical terminology ………………….28 Diet and nutrition ………………….29 Animal disease ………………….30 Wildlife medication ………………….'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Section 4\\nQuestions 31-40Complete the notes below.WriteONE WORD ONLYfor each answer.\\nLabyrinths\\nDefinition●   a winding spiral path leading to a central area\\nLabyrinths compared with mazes●   Mazes are a type of 31 ……………………–  32 …………………… is needed to navigate through a maze–  the word ‘maze’ is derived from a word meaning a feeling of 33 …………●   Labyrinths represent a journey through life–  they have frequently been used in 34 …………………… and prayer\\nEarly examples of the labyrinth spiral●   Ancient carvings on35…………………… have been foundacross many cultures●   The Pima, a Native American tribe, wove the symbol on baskets●   Ancient Greeks used the symbol on36……………………\\nWalking labyrinths●   The largest surviving example of a turf labyrinth once had a big37…………………… at its centre\\nLabyrinths nowadays●   Believed to have a beneficial impact on mental and physical health, e.g., walkinga maze can reduce a person’s38…………………… rate●   Used in medical and health and fitness settings and also prisons●   Popular with patients, visitors and staff in hospitals–  patients who can’t walk can use ‘finger labyrinths’ made from39…………–  research has shown that Alzheimer’s sufferers experience less40…………'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Answers\\nSection 11   LITTER2   DOGS3   INSECTS4   BUTTERFLIES5   WALLS6   ISLANDS7   BOOTS8   BEGINNERS9   SPOONS10   35 / THIRTY-FIVE\\nSection 211   A12   C13   B14   B15   A/D (in any order)16   A/D (in any order)17   B/C (in any order)18   B/C (in any order)19   B/E (in any order)20   B/E (in any order)\\nSection 321   A22   B23   B24   A25   C26   C27   A28   E29   F30   C\\nSection 431   PUZZLE32   LOGIC33   CONFUSION'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='34   MEDITATION35   STONE36   COINS37   TREES38   BREATHING39   PAPER40   ANXIETY')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Joint Automatic Speech Recognition And Structure\\nLearning For Better Speech Understanding\\nJiliang Hu1, Zuchao Li 2,*, Mengjia Shen 3, Haojun Ai 1, Sheng Li 4, Jun Zhang 3\\n1Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education,\\nSchool of Cyber Science and Engineering, Wuhan University, Wuhan, China,\\n2School of Computer Science, Wuhan University, Wuhan, China,\\n3Wuhan Second Ship Design and Research Institute, Wuhan, China,\\n4National Institute of Information and Communications Technology, Japan.\\nAbstract—Spoken language understanding (SLU) is a structure\\nprediction task in the field of speech. Recently, many works\\non SLU that treat it as a sequence-to-sequence task have\\nachieved great success. However, This method is not suitable\\nfor simultaneous speech recognition and understanding. In this\\npaper, we propose a joint speech recognition and structure\\nlearning framework (JSRSL), an end-to-end SLU model based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='for simultaneous speech recognition and understanding. In this\\npaper, we propose a joint speech recognition and structure\\nlearning framework (JSRSL), an end-to-end SLU model based\\non span, which can accurately transcribe speech and extract\\nstructured content simultaneously. We conduct experiments on\\nname entity recognition and intent classification using the Chinese\\ndataset AISHELL-NER and the English dataset SLURP. The\\nresults show that our proposed method not only outperforms the\\ntraditional sequence-to-sequence method in both transcription\\nand extraction capabilities but also achieves state-of-the-art\\nperformance on the two datasets.\\nIndex Terms—Speech Recognition, Spoken Language Under-\\nstanding, Information Extraction.\\nI. I NTRODUCTION\\nAutomatic speech recognition (ASR) aims to convert human\\nspeech into text in the corresponding language [1]. On the\\nother hand, SLU seeks to enhance machines’ ability to com-\\nprehend and react to human language by extracting specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='speech into text in the corresponding language [1]. On the\\nother hand, SLU seeks to enhance machines’ ability to com-\\nprehend and react to human language by extracting specific\\nstructured information from speech. SLU tasks encompass\\nname entity recognition (NER), intent classification (IC),\\nsentiment analysis (SA), among others [2]. SLU models can\\nbe categorized into two types: the pipeline model, which\\nuses an ASR module first, followed by a natural language\\nunderstanding (NLU) module in a sequential manner, and the\\nend-to-end (E2E) model, which directly converts speech rep-\\nresentations into structured information. The pipeline model\\nencounters challenges related to error propagation due to the\\nweak linkage between the two modules. Currently, there is a\\ngreater emphasis on the E2E model [3].\\nThere has been significant progress in the field of SLU.\\nHowever, few works consider how to improve or maintain\\nthe accuracy of transcription during spoken language un-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='There has been significant progress in the field of SLU.\\nHowever, few works consider how to improve or maintain\\nthe accuracy of transcription during spoken language un-\\nderstanding. For NER, most papers utilize the sequence-to-\\n*Corresponding author.\\nThis work was supported by the National Natural Science Foundation of\\nChina (No. 62306216), the Natural Science Foundation of Hubei Province of\\nChina (No. 2023AFB816), the Fundamental Research Funds for the Central\\nUniversities (No. 2042023kf0133).\\nCodes are available at https://github.com/193746/JSRSL\\nsequence method to achieve entity recognition. For instance,\\n[4] use ” — ] ”, ” $ ] ”, and ” { ]” to annotate Person, Place,\\nand Organization in the transcribed text. Subsequently, the\\nannotated text was used to train ASR model. However, [5]\\nhave shown that this method of labeling in the transcribed\\ntext can affect the recognition performance of the ASR model\\nand increase transcription errors. For classification tasks such'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='have shown that this method of labeling in the transcribed\\ntext can affect the recognition performance of the ASR model\\nand increase transcription errors. For classification tasks such\\nas SA and IC, some studies [6], [7] also achieve success by\\nannotating classification types in the transcribed text, while\\nothers [8], [9] directly connect the output of the ASR model\\nto a classification layer. The former will also reduce the\\nperformance of transcription, while the latter cannot transcribe\\nthe speech simultaneously. It is a significant challenge at the\\nSLU to ensure accurate transcription by the ASR module and\\ncorrect extraction by the SLU module simultaneously.\\nIn this paper, we propose a joint speech recognition and\\nstructure learning framework (JSRSL), an E2E SLU model\\nbased on span, which can accurately recognize speech while\\nextracting structured information. This model utilizes a re-\\nfinement module to enhance text representations for struc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='based on span, which can accurately recognize speech while\\nextracting structured information. This model utilizes a re-\\nfinement module to enhance text representations for struc-\\nture prediction. Specifically, we adopt a parallel transformer\\nfor non-autoregressive end-to-end speech recognition as our\\nframework and introduce the hidden layer output of the ASR\\ndecoder into a span for structure prediction. We additionally\\nintroduce a refinement module between the ASR model and\\nthe span to strengthen extraction performance. We conduct\\nexperiments on NER and IC using the AISHELL-NER and\\nSLURP. The results indicate that the proposed idea has supe-\\nrior capabilities in speech transcription and speech understand-\\ning compared to the traditional sequence-to-sequence method.\\nIt also outperforms the current state-of-the-art (SOTA) on the\\ntwo datasets.\\nII. R ELATED WORK\\nSo far, NLU has been well developed, and many works\\nhave involved key tasks of natural language processing (NLP),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='two datasets.\\nII. R ELATED WORK\\nSo far, NLU has been well developed, and many works\\nhave involved key tasks of natural language processing (NLP),\\nsuch as natural language inference (NLI) [10], semantic role\\nlabeling (SRL) [11], [12], and SA [13], [14]. In recent years, a\\nnumber of excellent SLU systems have emerged. [15] propose\\nan acoustic model called TDT and they attempt to use the\\nproposed model to do SLU tasks by jointly optimizing token\\narXiv:2501.07329v2  [cs.SD]  17 Jan 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='prediction and temporal prediction. [8] introduce a novel\\nmethod known as context-aware fine-tuning. They incorporate\\na context module into the pre-trained model to extract the\\ncontext embedding vector, which is subsequently used as\\nextra features for the ultimate prediction. [16] combine pre-\\ntrained self-supervised learning (SSL), ASR, language model\\n(LM) and SLU models to explore the model combination\\nthat can achieve the best SLU performance and shows that\\npre-training approaches rooted in self-supervised learning are\\nmore potent than those grounded in supervised learning. [17]\\ndevelop compositional end-to-end SLU systems that initially\\ntransform spoken utterances into a series of token represen-\\ntations, followed by token-level classification using a NLU\\nsystem. [6] suggest a comprehensive approach that combines\\na multilingual pretrained speech model, a text model, and\\nan adapter to enhance speech understanding. [18] incorporate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='system. [6] suggest a comprehensive approach that combines\\na multilingual pretrained speech model, a text model, and\\nan adapter to enhance speech understanding. [18] incorporate\\nsemantics into speech encoders and present a task-agnostic\\nunsupervised technique for integrating semantic information\\nfrom large language models (LLMs) [19], [20] into self-\\nsupervised speech encoders without the need for labeled audio\\ntranscriptions.\\nIII. M ETHOD\\nEncoderDecoderPredictorRefiner0011000001003000Start:End:PersonTimeASR ModuleSpan Module\\nplease wake joe up at seven thirty am\\nIntent:Emotion:65Alarm-SetNeutral𝑋1:𝑇 𝐻1:𝑇𝐸 𝐻1:𝑇𝐸 𝑁′,𝐸𝑎 𝑌′ 𝐻1:𝑁′𝐷 𝐻1:𝑁′𝑅 𝑆′ 𝑍′ Bridge𝐻1:𝑁′𝐼𝑇ROPESelf-Attention Poolng\\nFig. 1. The structure of our proposed framework, JSRSL.\\nFigure 1 shows the full overview of our proposed frame-\\nwork, JSRSL. Follow [21], we adopt this parallel Transformer\\nfor non-autoregressive end-to-end speech recognition as our\\nASR module framework. In addition, we use a Refiner module'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='work, JSRSL. Follow [21], we adopt this parallel Transformer\\nfor non-autoregressive end-to-end speech recognition as our\\nASR module framework. In addition, we use a Refiner module\\nto connect the ASR module and span [22], and a Bridge\\nmodule to connect the ASR module and Refiner module. The\\nBridge module can convert acoustic representation into text\\nrepresentation. The Refiner module can strengthen the text\\nrepresentation of upstream inputs and extract richer features\\nfor structured learning. Before doing structure learning, we\\napply rotary embedded encoding (ROPE) [23] into span.\\nROPE can leverage the boundary information of span by\\ninjecting relative position information.\\nA. SLU Representation Learning\\nLet X be a speech sequence with T frames, X =\\n{x1, x2, x3, . . . , xT }. Y is a sequence of tokens, and its\\nlength is N. Each token is in the vocabulary V , Y =\\n{y1, y2, y3, . . . , yN | yi ∈ V }. The ASR module gets X as\\ninput and outputs the decoding result Y\\n′\\nand the Decoder’s'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='length is N. Each token is in the vocabulary V , Y =\\n{y1, y2, y3, . . . , yN | yi ∈ V }. The ASR module gets X as\\ninput and outputs the decoding result Y\\n′\\nand the Decoder’s\\nhidden representation HD\\n1:N′ .\\nThe Refiner module is essentially a NLU model, and it\\nrequires a text vector as input. It will not function correctly\\nif the acoustic hidden representation HD\\n1:N′ is fed directly. If\\nwe use the decoded token sequence as input, the computation\\ngraph will be truncated, preventing the joint training of the\\nASR module and Refiner module. Therefore, a Bridge module\\nis needed to implement the conversion from acoustic hid-\\nden representation HD\\n1:N′ to initial text hidden representation\\nHIT\\n1:N′ . Our Bridge module first calculates the probability\\ndistribution of the token sequence Y p based on HD\\n1:N′ , then\\nit maps the distribution to the embedding layer of the Refiner\\nmodule to convert Y p into the initial text representation HIT\\n1:N′ .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='distribution of the token sequence Y p based on HD\\n1:N′ , then\\nit maps the distribution to the embedding layer of the Refiner\\nmodule to convert Y p into the initial text representation HIT\\n1:N′ .\\nThe specific operation of the map is to multiply Y p and the\\nweights of embedding layer We\\n1:V in a matrix.\\nY p = Softmax (Wp\\nV HD\\n1:N′ + bp\\nV )\\nHIT\\n1:N′ = Matmul(Y p, We\\n1:V )\\nSubsequently, we feed HIT\\n1:N′ into the Refiner module for\\nstrengthening text representation, resulting in a refined text\\nfeatures HR\\n1:N′ . It is used as the input features for SLU\\ndownstream tasks. The Refiner is implemented of multiple\\nstacked Transformer layers.\\nB. Span-based Structure Learning\\nWe utilize span to extract the token sequence of each\\nspecific information in a given speech sequence X along with\\ntheir corresponding types. Let S be the pointer sequence, the\\nset of pointer labels be Ω, and S = {s1, s2, s3, . . . , sN |\\nsi ∈ Ω}. The pointer sequence corresponds one-to-one to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='their corresponding types. Let S be the pointer sequence, the\\nset of pointer labels be Ω, and S = {s1, s2, s3, . . . , sN |\\nsi ∈ Ω}. The pointer sequence corresponds one-to-one to\\nthe token sequence obtained from the speech transcription.\\nWe define two pointer sequences, namely the starting pointer\\nsequence Sstart and the ending pointer sequence Send. The\\nformer is used to locate the head position of the specific\\ninformation, Ωstart = {0, 1}, and the latter is used to locate\\nthe tail position of the specific information and identify its\\ntype, Ωend = {0, 1, 2. . . Ntype}.\\nFor ordinary structure extraction task, we firstly use a linear\\nlayer called Dense to double the dimension of HR\\n1:N′ , which\\nis aimed at make two pointers use different feature vectors.\\nThe output of Dense is evenly divided into two parts, d1\\nand d2. Then, ROPE is employed to embed relative position\\ninformation for the pointer sequence.\\nd1, d2 = Dense(HR\\n1:N′ )'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='S\\n′\\nstart, S\\n′\\nend = ROP E(d1), ROP E(d2)\\nFor classification tasks, we treat them as sentence-level\\nspan. Let Z be the classification result, and the set of classi-\\nfication result be Ωcls, Ωcls = {1, 2, . . . , Ntype}. Firstly, we\\nuse ROPE to embed relative position information and get the\\ntoken-level classification features. Z1:N′ .\\nZ1:N′ = ROPE (HR\\n1:N′ )\\nDrawing inspiration from [24], a modified self-attention\\npooling mechanism is utilized to comprehensively extract\\nsentence-level classification result Z\\n′\\nfrom token-level fea-\\ntures. The process of the self-attention pooling mechanism is\\nas follows, AZ\\n1:N′ is attention value of Z1:N′ calculated by a\\nlinear layer.\\nAZ\\n1:N′ = Softmax (W1Z1:N′ + b1)\\nZ\\n′\\n= arg max\\nNtype\\n(WNtype (\\nX\\nN′\\nAZ\\n1:N′ × Z1:N′ ) + bNtype )\\nC. Joint Optimization\\nThe adopted ASR module calculates three loss functions\\nwhen training: the cross-entropy (CE), the mean absolute error\\n(MAE), and the minimum word error rate (MWER) loss. CE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='C. Joint Optimization\\nThe adopted ASR module calculates three loss functions\\nwhen training: the cross-entropy (CE), the mean absolute error\\n(MAE), and the minimum word error rate (MWER) loss. CE\\nand MWER are used to optimize the model’s transcription\\nability, while MAE guides the predictor to convergence. Ac-\\ncording to [21], the loss function of the ASR part is:\\nLasr = γLCE + LMAE + LN\\nwerr(x, y∗)\\nLN\\nwerr(x, y∗) =\\nX\\nyi∈Sample\\nbP(yi | x)[W(yi, y∗) − cW]\\nWe also use CE to optimize the model’s ability for structure\\nprediction. The total loss function can be formulated as\\nfollows, where α is used to control the proportion of ASR\\nloss and structured loss, α ∈ (0, 1).\\nLtotal = (1 − α)Lasr + αLsp\\nIV. E XPERIMENT\\nA. Configuration\\nWe employ AISHELL-NER [5] and SLURP’s NER (slot\\nfilling) subset for NER, and SLURP’s IC subset for IC [25].\\nFollowing the previous work [26] that used SLURP, the\\nprovided synthetic data is utilized for training. For assess-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='filling) subset for NER, and SLURP’s IC subset for IC [25].\\nFollowing the previous work [26] that used SLURP, the\\nprovided synthetic data is utilized for training. For assess-\\nment metrics, SLURP-NER utilizes WER and SLURP-F1,\\nSLURP-IC employs WER and micro-F1, while AISHELL-\\nNER uses CER and micro-F1. Our baseline, Seq2Seq, ap-\\nplies the sequence-to-sequence method to the JSRSL’s ASR\\nmodule and annotates like [5]. Another baseline, Pipe, is\\nthe concatenation of the JSRSL’s ASR module and a NLU\\nmodule, predicting structure by span. The models chosen as\\nbenchmarks all adopt sequence generation method by adding\\nspecial symbols in transcribed text or generating structured\\nsequence directly.\\nWe build the experiment environment based on Funasr\\n[27] and ModelScope, utilizing 220M Chinese and English\\nParaformer for experiments. The pre-training parameters of\\nBERT-base [28] is adopted to initialize the Refiner of JSRSL\\nand the NLU module of Pipe. The α of the loss is set to 0.5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Paraformer for experiments. The pre-training parameters of\\nBERT-base [28] is adopted to initialize the Refiner of JSRSL\\nand the NLU module of Pipe. The α of the loss is set to 0.5.\\nThe models are trained until converges. We consistently use\\nthe Adam optimizer with a learning rate of 5e-5.\\nB. Main Result\\nTABLE I\\nCOMPARISON RESULTS WITH BENCHMARKS IN AISHELL-NER AND\\nSLURP. I N SLURP, ALL MODELS CONDUCT JOINT TRAINING AND\\nEVALUATION FOR NER AND IC TASKS . THE BERT IN [5]’ S PIPELINE\\nMODEL ARE PRE -TRAINED . THE POST AED USES 1K EXTERNAL DATA .\\nModel Paradigm\\nAISHELL-NER\\nCER (↓) F1 ( ↑)\\nTransformer [5] E2E 9.25 64.34\\nConformer [5] E2E 4.79 73.37\\nTransformer+BERT* [5] Pipeline 9.25 65.95\\nConformer+BERT* [5] Pipeline 4.79 74.90\\nSeq2Seq (Ours) E2E 3.48 76.17\\nPipe (Ours) Pipeline 1.76 77.98\\nJSRSL (Ours) E2E 1.71 80.85\\nModel Paradigm\\nSLURP\\nWER (↓) SLU F1 ( ↑) IC F1 ( ↑)\\nRNNT→BertNLU [29] Pipeline 15.20 72.35 82.45\\nEspnet SLU [26] E2E - 71.90 86.30\\nPostDec AED* [6] E2E - 80.08 89.33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='JSRSL (Ours) E2E 1.71 80.85\\nModel Paradigm\\nSLURP\\nWER (↓) SLU F1 ( ↑) IC F1 ( ↑)\\nRNNT→BertNLU [29] Pipeline 15.20 72.35 82.45\\nEspnet SLU [26] E2E - 71.90 86.30\\nPostDec AED* [6] E2E - 80.08 89.33\\nConformer-RNNT [7] E2E 14.20 77.22 90.10\\nConformer-CTC [7] E2E 14.50 69.30 85.50\\nTDT 0-6 [15] E2E - 80.61 89.28\\nTDT 0-8 [15] E2E - 79.90 90.07\\nSeq2Seq (Ours) E2E 18.83 68.84 86.63\\nPipe (Ours) Pipeline 13.61 76.62 86.67\\nJSRSL (Ours) E2E 13.14 80.17 91.03\\nTable I presents the comparison results with baselines\\nand benchmarks. Before the joint training of Paraformer and\\nBERT, the Paraformer in JSRSL is pre-trained by audio-\\ntext pairs from AISHELL-NER or SLURP and the BERT\\nis pre-trained by the vocabulary of Paraformer and the text-\\nspan pairs of corresponding dataset. The results show that\\nthe proposed model, based on span, outperforms the E2E\\nand pipeline baselines and benchmarks using the sequence-\\nto-sequence method in both structure extraction and ASR.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='the proposed model, based on span, outperforms the E2E\\nand pipeline baselines and benchmarks using the sequence-\\nto-sequence method in both structure extraction and ASR.\\nBy using span, we can distinguish between ASR and SLU\\ntasks, avoid the impact of extra structured annotations on\\ntranscription accuracy and allow for more precise extraction\\nof structured information. In addition, Pipe is worse than\\nJSRSL due to issues with error propagation of pipeline model,\\nindicating that the E2E model is more promising. In SLURP,\\nTDT models and PostDEC AED have the best SLU perfor-\\nmance in the benchmarks. TDT models adopt the token and\\nduration transformer architecture, which enhances the ability\\nof sequence generation and accelerates the inference speed of\\nthe original RNN-Transducer [30] through joint learning of\\ntoken and duration prediction. ”0-X” refers to the maximum\\nduration of duration configurations as ”X”. The performance of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='the original RNN-Transducer [30] through joint learning of\\ntoken and duration prediction. ”0-X” refers to the maximum\\nduration of duration configurations as ”X”. The performance of\\nTDT is comparable to that of our JSRSL, but it is influenced by\\nspecific configurations. TDT 0-6 performs better on SLURP-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='TABLE II\\nABLATION RESULTS IN AISHELL-NER AND SLURP. S AMPLER IS A COMPONENT IN PARAFORMER USED TO ENHANCE TRAINING EFFECTIVENESS .\\nModel AISHELL-NER SLURP-NER SLURP-IC\\nRefiner CER (↓) Precision (↑) Recall (↑) F1 (↑) WER (↓) Precision (↑) Recall (↑) SLU F1 (↑) WER (↓) F1 (↑)\\nJSRSL ✕ 1.76 83.21 78.15 80.46 10.79 80.25 73.37 76.66 13.84 88.19\\nw/o ROPE ✕ 1.78 82.50 78.00 80.06 10.81 79.33 73.44 76.27 13.87 87.93\\nw/o Sampler✕ 1.75 82.98 78.00 80.32 10.72 80.14 73.20 76.51 14.05 88.19\\nJSRSL ✓ 1.77 81.89 79.27 80.45 11.17 83.26 77.46 80.26 13.30 89.90\\nw/o pretrain✓ 1.71 82.36 79.54 80.85 12.17 82.41 76.36 79.27 13.53 89.67\\nNER, while TDT 0-8 performs better on SLURP-IC. PostDec\\nAED uses an adaptor to concatenate the speech encoder\\nand NLU model. The adaptor tries to minimize the distance\\nbetween the speech representation and the text representation.\\nBefore finetune the SLU system, it must undergo adapter\\npretraining, which optimizes the loss between predicted text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='between the speech representation and the text representation.\\nBefore finetune the SLU system, it must undergo adapter\\npretraining, which optimizes the loss between predicted text\\nand real text. Although PostDec AED’s performance is just\\nslightly inferior to our JSRSL, its training process is more\\ncomplex and requires a large amount of external data.\\nC. Ablation Study\\nTABLE III\\nASR RESULTS (CER ON AISHELL-NER AND WER ON SLURP) AMONG\\nPARAFORMER , WHISPER , XLSR-53 AND OUR JSRSL WITH SIMILAR\\nSIZE .\\nModel Params AISHELL-NER SLURP\\nXLSR-53 [31] 315M 4.12 22.10\\nWhisper [32] 244M 5.54 16.89\\nParaformer 220M 1.76 13.61\\nJSRSL (Ours) 314M 1.71 13.14\\nTable II shows the ablation results. The proposed model\\nshows a certain improvement after introducing ROPE. In\\naddition, in SLURP-NER and AISHELL-NER, we find that\\nSampler does not significantly improve model’s ASR perfor-\\nmance, but rather increases its spoken understanding ability.\\nIn AISHELL-NER and SLURP-IC, the introduction of refine-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='Sampler does not significantly improve model’s ASR perfor-\\nmance, but rather increases its spoken understanding ability.\\nIn AISHELL-NER and SLURP-IC, the introduction of refine-\\nment module leads to a further decrease in the model’s CER\\nor WER and an increase in F1 score. However, in SLURP-\\nNER, we find that with refinement module, while JSRSL\\nenhances SLU F1, WER increases. But overall, the introduc-\\ntion of refinement modules is beneficial for improving model\\nperformance. In AISHELL-NER, the JSRSL with pre-trained\\nParaformer does not achieve better performance, possibly\\ndue to overfitting. Nevertheless, in SLURP, pre-trained ASR\\nmodule improves the overall ability of JSRSL. Additionally,\\nto assess the effectiveness of the adopted ASR module, we\\ncarry out an ASR experiment detailed in Table III. The results\\nindicate that Paraformer’s ASR performance on AISHELL-\\nNER and SLURP surpasses that of Whisper and XLSR-53.\\nThis demonstrates the robustness of our model’s ASR module.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='indicate that Paraformer’s ASR performance on AISHELL-\\nNER and SLURP surpasses that of Whisper and XLSR-53.\\nThis demonstrates the robustness of our model’s ASR module.\\nFurthermore, JSRSL achieves better ASR capability after joint\\ntraining.\\nPCCS ST-CMDSCommonV oice\\n4\\n6\\n8\\n10\\nCER\\nParaformer-ZhSeq2SeqJSRSL W/O RefinerJSRSL W/ Refiner\\nTED-LIUMLibriSpeechCommonV oice\\n5\\n10\\n15\\n20\\n25\\nWER\\nParaformer-EnSeq2SeqJSRSL W/O RefinerJSRSL W/ Refiner\\nFig. 2. Out-of-Distribution experiment result. The results on the left are in\\nthe Chinese datasets and the results on the right are in the English datasets.\\nD. OOD Analysis\\nThis section conducts Out-of-Distribution (OOD) experi-\\nment with unseen ASR datasets. The Chinese ASR datasets\\nare Primewords Chinese Corpus Set 1 (PCCS) [33], Free ST\\nChinese Mandarin Corpus (ST-CMDS), and Common V oice\\n(Zh) [34]. The English ASR datasets include Librispeech\\n[35], Ted-LIUM [36], and Common V oice (En). Each dataset'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='Chinese Mandarin Corpus (ST-CMDS), and Common V oice\\n(Zh) [34]. The English ASR datasets include Librispeech\\n[35], Ted-LIUM [36], and Common V oice (En). Each dataset\\nuniformly uses the first 8000 training samples for the ex-\\nperiment. When evaluating the Seq2Seq, the special symbols\\nthat indicate name entities are ignored. Figure 2 presents the\\nOOD results. The original Chinese or English Paraformer\\ndemonstrates better generalization ability and achieves the\\nlowest CER or WER among all datasets. Although all trained\\nmodels’ transcription performance has decreased, compared to\\nSeq2Seq, which uses a sequence generation method, the span-\\nbased models exhibit lower recognition error rates. What’s\\nmore, JSRSL with refinement module performs better than that\\nwithout refinement module. This indicates that the proposed\\nmethod results in less performance loss in speech recognition,\\nmaking it more suitable for joint ASR and structure prediction.\\nV. C ONCLUSION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='method results in less performance loss in speech recognition,\\nmaking it more suitable for joint ASR and structure prediction.\\nV. C ONCLUSION\\nThis paper proposes a method called JSRSL for jointly ASR\\nand structure prediction based on span. This approach aims\\nto ensure accurate transcribing and understanding of speech\\nsimultaneously. Experiments have shown that the proposed\\nscheme is superior to traditional sequence-to-sequence method\\nin both transcription and extraction capabilities, and achieves\\nSOTA performance on the datasets used for the experiments.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='REFERENCES\\n[1] J. Li et al. , “Recent advances in end-to-end automatic speech recog-\\nnition,” APSIPA Transactions on Signal and Information Processing ,\\nvol. 11, no. 1, 2022.\\n[2] S. Arora, H. Futami, J.-w. Jung, Y . Peng, R. Sharma, Y . Kashiwagi,\\nE. Tsunoo, K. Livescu, and S. Watanabe, “Universlu: Universal spo-\\nken language understanding for diverse tasks with natural language\\ninstructions,” in Proceedings of the 2024 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Papers) , 2024, pp.\\n2754–2774.\\n[3] A. Pasad, F. Wu, S. Shon, K. Livescu, and K. Han, “On the use of\\nexternal data for spoken named entity recognition,” in Proceedings of\\nthe 2022 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies , 2022,\\nConference Proceedings, pp. 724–737.\\n[4] H. Yadav, S. Ghosh, Y . Yu, and R. R. Shah, “End-to-end named entity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='for Computational Linguistics: Human Language Technologies , 2022,\\nConference Proceedings, pp. 724–737.\\n[4] H. Yadav, S. Ghosh, Y . Yu, and R. R. Shah, “End-to-end named entity\\nrecognition from english speech,” Organization, vol. 2299, no. 1473, p.\\n3772, 2020.\\n[5] B. Chen, G. Xu, X. Wang, P. Xie, M. Zhang, and F. Huang, “Aishell-\\nner: Named entity recognition from chinese speech,” in ICASSP 2022-\\n2022 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP). IEEE, 2022, Conference Proceedings, pp. 8352–\\n8356.\\n[6] P. Denisov and N. T. Vu, “Leveraging multilingual self-supervised\\npretrained models for sequence-to-sequence end-to-end spoken language\\nunderstanding,” arXiv preprint arXiv:2310.06103 , 2023.\\n[7] K. Singla, S. Jalalvand, Y .-J. Kim, A. Moreno Daniel, S. Bangalore,\\nA. Ljolje, and B. Stern, “1spu: 1-step speech processing unit,” arXiv\\ne-prints, p. arXiv: 2311.04753, 2023.\\n[8] S. Shon, F. Wu, K. Kim, P. Sridhar, K. Livescu, and S. Watan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='A. Ljolje, and B. Stern, “1spu: 1-step speech processing unit,” arXiv\\ne-prints, p. arXiv: 2311.04753, 2023.\\n[8] S. Shon, F. Wu, K. Kim, P. Sridhar, K. Livescu, and S. Watan-\\nabe, “Context-aware fine-tuning of self-supervised speech models,” in\\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP). IEEE, 2023, Conference Proceedings,\\npp. 1–5.\\n[9] S. Shon, K. Kim, P. Sridhar, Y .-T. Hsu, S. Watanabe, and K. Livescu,\\n“Generative context-aware fine-tuning of self-supervised speech mod-\\nels,” arXiv preprint arXiv:2312.09895 , 2023.\\n[10] Z. Zhang, Y . Wu, H. Zhao, Z. Li, S. Zhang, X. Zhou, and X. Zhou,\\n“Semantics-aware bert for language understanding,” in Proceedings of\\nthe AAAI conference on artificial intelligence , vol. 34, no. 05, 2020, pp.\\n9628–9635.\\n[11] S. He, Z. Li, H. Zhao, and H. Bai, “Syntax for semantic role labeling,\\nto be, or not to be,” in Proceedings of the 56th Annual Meeting of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='9628–9635.\\n[11] S. He, Z. Li, H. Zhao, and H. Bai, “Syntax for semantic role labeling,\\nto be, or not to be,” in Proceedings of the 56th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\n2018, pp. 2061–2071.\\n[12] Z. Li, S. He, H. Zhao, Y . Zhang, Z. Zhang, X. Zhou, and X. Zhou,\\n“Dependency or span, end-to-end uniform semantic role labeling,” in\\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 33,\\nno. 01, 2019, pp. 6730–6737.\\n[13] H. Jing, Z. Li, H. Zhao, and S. Jiang, “Seeking common but distinguish-\\ning difference, a joint aspect-based sentiment analysis model,” arXiv\\npreprint arXiv:2111.09634, 2021.\\n[14] L. Peng, Z. Li, and H. Zhao, “Sparse fuzzy attention for structured\\nsentiment analysis,” arXiv preprint arXiv:2109.06719 , 2021.\\n[15] H. Xu, F. Jia, S. Majumdar, H. Huang, S. Watanabe, and B. Ginsburg,\\n“Efficient sequence transduction by jointly predicting tokens and dura-\\ntions,” ArXiv, vol. abs/2304.06795, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='[15] H. Xu, F. Jia, S. Majumdar, H. Huang, S. Watanabe, and B. Ginsburg,\\n“Efficient sequence transduction by jointly predicting tokens and dura-\\ntions,” ArXiv, vol. abs/2304.06795, 2023.\\n[16] Y . Peng, S. Arora, Y . Higuchi, Y . Ueda, S. Kumar, K. Ganesan,\\nS. Dalmia, X. Chang, and S. Watanabe, “A study on the integration\\nof pre-trained ssl, asr, lm and slu models for spoken language under-\\nstanding,” in 2022 IEEE Spoken Language Technology Workshop (SLT).\\nIEEE, 2023, Conference Proceedings, pp. 406–413.\\n[17] S. Arora, S. Dalmia, B. Yan, F. Metze, A. W. Black, and S. Watanabe,\\n“Token-level sequence labeling for spoken language understanding using\\ncompositional end-to-end models,” in Findings of the Association for\\nComputational Linguistics: EMNLP 2022 , 2022, Conference Proceed-\\nings, pp. 5419–5429.\\n[18] D. Xu, S. Dong, C. Wang, S. Kim, Z. Lin, A. Shrivastava, S.-W. Li,\\nL.-H. Tseng, A. Baevski, G.-T. Lin, H.-y. Lee, Y . Sun, and W. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='ings, pp. 5419–5429.\\n[18] D. Xu, S. Dong, C. Wang, S. Kim, Z. Lin, A. Shrivastava, S.-W. Li,\\nL.-H. Tseng, A. Baevski, G.-T. Lin, H.-y. Lee, Y . Sun, and W. Wang,\\n“Introducing semantics into speech encoders,” in Annual Meeting of the\\nAssociation for Computational Linguistics , 2022, Conference Proceed-\\nings.\\n[19] T. B. Brown, “Language models are few-shot learners,” arXiv preprint\\narXiv:2005.14165, 2020.\\n[20] Z. Li, S. Zhang, H. Zhao, Y . Yang, and D. Yang, “Batgpt: A bidirectional\\nautoregessive talker from generative pre-trained transformer,” arXiv\\npreprint arXiv:2307.00360, 2023.\\n[21] Z. Gao, S. Zhang, I. McLoughlin, and Z. Yan, “Paraformer: Fast and\\naccurate parallel transformer for non-autoregressive end-to-end speech\\nrecognition,” arXiv preprint arXiv:2206.08317 , 2022.\\n[22] J. Fu, X.-J. Huang, and P. Liu, “Spanner: Named entity re-/recognition\\nas span prediction,” in Proceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the 11th International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='as span prediction,” in Proceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long\\nPapers), 2021, Conference Proceedings, pp. 7183–7195.\\n[23] J. Su, A. Murtadha, S. Pan, J. Hou, J. Sun, W. Huang, B. Wen, and\\nY . Liu, “Global pointer: Novel efficient span-based approach for named\\nentity recognition,” arXiv preprint arXiv:2208.03054 , 2022.\\n[24] S. Shon, A. Pasad, F. Wu, P. Brusco, Y . Artzi, K. Livescu, and K. J.\\nHan, “Slue: New benchmark tasks for spoken language understanding\\nevaluation on natural speech,” in ICASSP 2022-2022 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\\nIEEE, 2022, Conference Proceedings, pp. 7927–7931.\\n[25] E. Bastianelli, A. Vanzo, P. Swietojanski, and V . Rieser, “Slurp: A\\nspoken language understanding resource package,” in Proceedings of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='IEEE, 2022, Conference Proceedings, pp. 7927–7931.\\n[25] E. Bastianelli, A. Vanzo, P. Swietojanski, and V . Rieser, “Slurp: A\\nspoken language understanding resource package,” in Proceedings of the\\n2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP), 2020, Conference Proceedings, pp. 7252–7262.\\n[26] S. Arora, S. Dalmia, P. Denisov, X. Chang, Y . Ueda, Y . Peng, Y . Zhang,\\nS. Kumar, K. Ganesan, and B. Yan, “Espnet-slu: Advancing spoken\\nlanguage understanding through espnet,” in ICASSP 2022-2022 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP). IEEE, 2022, Conference Proceedings, pp. 7167–7171.\\n[27] Z. Gao, Z. Li, J. Wang, H. Luo, X. Shi, M. Chen, Y . Li, L. Zuo, Z. Du,\\nand Z. Xiao, “Funasr: A fundamental end-to-end speech recognition\\ntoolkit,” arXiv preprint arXiv:2305.11013 , 2023.\\n[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='toolkit,” arXiv preprint arXiv:2305.11013 , 2023.\\n[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805, 2018.\\n[29] A. Raju, M. Rao, G. Tiwari, P. Dheram, B. Anderson, Z. Zhang, C. Lee,\\nB. Bui, and A. Rastrow, “On joint training with interfaces for spoken\\nlanguage understanding,” arXiv preprint arXiv:2106.15919 , 2021.\\n[30] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “Rnn-\\ntransducer with stateless prediction network,” in ICASSP 2020-2020\\nIEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP). IEEE, 2020, pp. 7049–7053.\\n[31] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,\\nK. Singh, P. V on Platen, Y . Saraf, J. Pinoet al., “Xls-r: Self-supervised\\ncross-lingual speech representation learning at scale,” arXiv preprint\\narXiv:2111.09296, 2021.\\n[32] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='cross-lingual speech representation learning at scale,” arXiv preprint\\narXiv:2111.09296, 2021.\\n[32] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\\nI. Sutskever, “Robust speech recognition via large-scale weak super-\\nvision,” in International Conference on Machine Learning . PMLR,\\n2023, Conference Proceedings, pp. 28 492–28 518.\\n[33] L. Primewords Information Technology Co., “Primewords chinese cor-\\npus set 1,” 2018, https://www.primewords.cn.\\n[34] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer,\\nR. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice:\\nA massively-multilingual speech corpus,” in Proceedings of the 12th\\nConference on Language Resources and Evaluation (LREC 2020), 2020,\\npp. 4211–4215.\\n[35] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr\\ncorpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-20T01:51:49+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/2501.07329v2.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='corpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Section 1\\nQuestion 1-10Complete the notes below.WriteONE WORD AND/OR A NUMBERfor each answer.\\nBuckworth Conservation GroupRegular activities\\nBeach●   making sure the beach does not have 1 …………………… on it●   no 2 ……………………\\nNature reserve\\n●   maintaining paths●   nesting boxes for birds installed●   next task is taking action to attract 3 …………………… to the place●   identifying types of 4 ……………………●   building a new 5 ……………………\\nForthcoming events\\nSaturday\\n●   meet at Dunsmore Beach car park●   walk across the sands and reach the 6 ……………………●   take a picnic●   wear appropriate 7 ……………………\\nWoodwork session\\n●   suitable for 8 …………………… to participate in●   making 9 …………………… out of wood●   17th, from 10 a.m. to 3 p.m.●   cost of session (no camping): 10 £ ……………………'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Questions 17 and 18ChooseTWOletters,A-E.\\nWhichTWOtypes of creature might come close to theboat?\\nA sea eagleB fur sealsC dolphinsD whalesE penguins\\nQuestions 19 and 20ChooseTWOletters,A-E.\\nWhichTWOpoints does Lou make about the caves?\\nA   Only large tourist boats can visit them.B   The entrances to them are often blocked.C   It is too dangerous for individuals to go near them.D   Someone will explain what is inside them.E   They cannot be reached on foot.\\nSection 3\\nQuestion 21-26Choose the correct letter,A,BorC.\\nWork experience for veterinary science students21   What problem did both Diana and Tim have when arranging their workexperience?A make initial contact with suitable farmsB organising transport to and from the farmC finding a placement for the required length oftime\\n22   Tim was pleased to be able to helpA a lamb that had a broken leg.B a sheep that was having difficulty giving birth.C a newly born lamb that was having trouble feeding.\\n23   Diana says the sheep on her farm'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='A were of various different varieties.B was mainly reared for their meat.C had better quality wool than sheep on the hills.\\n24   What did the students learn about adding supplements to chicken feed?A These should only be given if especially needed.B It is worth paying extra for the most effectiveones.C The amount given at one time should be limited.\\n25   What happened when Diana was working with dairy cows?A She identified some cows incorrectly.B She accidentally threw some milk away.C She made a mistake when storing milk.\\n26   What did both farmers mention about vets and farming?A Vets are failing to cope with some aspects ofanimal health.B There needs to be a fundamental change in thetraining of vets.C Some jobs could be done by the farmer rather thanby a vet.\\nQuestions 27-30What opinion do the students give about each of the following modules on their veterinaryscience course?\\nChooseFOURanswers from the box and write the correctletter,A-F, next to questions27-30.\\nOpinions'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='ChooseFOURanswers from the box and write the correctletter,A-F, next to questions27-30.\\nOpinions\\nA.Tim found this easier than expected.B.Tim found this easier than expected.C.Diana may do some further study on this.D.They both found the reading required for this difficult.E. Tim was shocked at something he learned in this module.F. They were both surprised at how little is known about some aspects of this.\\nModules on Veterinary Science course27 Medical terminology ………………….28 Diet and nutrition ………………….29 Animal disease ………………….30 Wildlife medication ………………….'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Section 4\\nQuestions 31-40Complete the notes below.WriteONE WORD ONLYfor each answer.\\nLabyrinths\\nDefinition●   a winding spiral path leading to a central area\\nLabyrinths compared with mazes●   Mazes are a type of 31 ……………………–  32 …………………… is needed to navigate through a maze–  the word ‘maze’ is derived from a word meaning a feeling of 33 …………●   Labyrinths represent a journey through life–  they have frequently been used in 34 …………………… and prayer\\nEarly examples of the labyrinth spiral●   Ancient carvings on35…………………… have been foundacross many cultures●   The Pima, a Native American tribe, wove the symbol on baskets●   Ancient Greeks used the symbol on36……………………\\nWalking labyrinths●   The largest surviving example of a turf labyrinth once had a big37…………………… at its centre'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Walking labyrinths●   The largest surviving example of a turf labyrinth once had a big37…………………… at its centre\\nLabyrinths nowadays●   Believed to have a beneficial impact on mental and physical health, e.g., walkinga maze can reduce a person’s38…………………… rate●   Used in medical and health and fitness settings and also prisons●   Popular with patients, visitors and staff in hospitals–  patients who can’t walk can use ‘finger labyrinths’ made from39…………–  research has shown that Alzheimer’s sufferers experience less40…………'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Answers\\nSection 11   LITTER2   DOGS3   INSECTS4   BUTTERFLIES5   WALLS6   ISLANDS7   BOOTS8   BEGINNERS9   SPOONS10   35 / THIRTY-FIVE\\nSection 211   A12   C13   B14   B15   A/D (in any order)16   A/D (in any order)17   B/C (in any order)18   B/C (in any order)19   B/E (in any order)20   B/E (in any order)\\nSection 321   A22   B23   B24   A25   C26   C27   A28   E29   F30   C\\nSection 431   PUZZLE32   LOGIC33   CONFUSION'),\n",
       " Document(metadata={'producer': 'Skia/PDF m109 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'ielts-listening-practice-test-pdf-1', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='34   MEDITATION35   STONE36   COINS37   TREES38   BREATHING39   PAPER40   ANXIETY')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = chunker.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = chunker.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '',\n",
       " 'title': 'ielts-listening-practice-test-pdf-1',\n",
       " 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       " 'total_pages': 7,\n",
       " 'page': 3,\n",
       " 'page_label': '4'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[40].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc.page_content for doc in text_chunks]\n",
    "metadata = [doc.metadata for doc in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load environment variables if needed (e.g., API keys for other tools)\n",
    "TEST_COLLECTION_NAME = \"agent_test_docs\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\" # Ensure this model is available locally\n",
    "PERSIST_DIR = \"./_agent_test_chroma_db\"\n",
    "LLM_MODEL = \"deepseek-r1:1.5b\" # Or your preferred Ollama model\n",
    "# llm = ChatOllama(model='deepseek-r1:1.5b', temperature=0.2, max_tokens=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joint Automatic Speech Recognition And Structure\\nLearning For Better Speech Understanding\\nJiliang Hu1, Zuchao Li 2,*, Mengjia Shen 3, Haojun Ai 1, Sheng Li 4, Jun Zhang 3\\n1Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education,\\nSchool of Cyber Science and Engineering, Wuhan University, Wuhan, China,\\n2School of Computer Science, Wuhan University, Wuhan, China,\\n3Wuhan Second Ship Design and Research Institute, Wuhan, China,\\n4National Institute of Information and Communications Technology, Japan.\\nAbstract—Spoken language understanding (SLU) is a structure\\nprediction task in the field of speech. Recently, many works\\non SLU that treat it as a sequence-to-sequence task have\\nachieved great success. However, This method is not suitable\\nfor simultaneous speech recognition and understanding. In this\\npaper, we propose a joint speech recognition and structure\\nlearning framework (JSRSL), an end-to-end SLU model based'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Initializes LLM, Tools, and cleans up old DB.\"\"\"\n",
    "    logger.info(\"--- Setting up test environment ---\")\n",
    "\n",
    "    try:\n",
    "        # Initialize LLM\n",
    "        logger.info(f\"Loading LLM: {LLM_MODEL}\")\n",
    "        llm = ChatOllama(model=LLM_MODEL, temperature=0.1)\n",
    "        # Simple check if LLM is accessible (optional, Ollama might not have a direct check)\n",
    "        # llm.invoke(\"Hi\")\n",
    "        logger.info(\"LLM loaded successfully.\")\n",
    "\n",
    "        # Initialize Tools\n",
    "        logger.info(\"Initializing RAG Tools...\")\n",
    "        retriever_tool = RetrieveTool(\n",
    "            embedding_model_name=EMBEDDING_MODEL,\n",
    "            persist_dir=PERSIST_DIR\n",
    "        )\n",
    "        ingest_tool = IngestTool(retriever_tool=retriever_tool)\n",
    "        logger.info(\"RAG Tools initialized successfully.\")\n",
    "\n",
    "        return llm, retriever_tool, ingest_tool\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set up environment: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 03:17:56.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_environment\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m--- Setting up test environment ---\u001b[0m\n",
      "\u001b[32m2025-05-12 03:17:56.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_environment\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading LLM: deepseek-r1:1.5b\u001b[0m\n",
      "/tmp/ipykernel_204065/4034160067.py:8: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=LLM_MODEL, temperature=0.1)\n",
      "\u001b[32m2025-05-12 03:17:56.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_environment\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mLLM loaded successfully.\u001b[0m\n",
      "\u001b[32m2025-05-12 03:17:56.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_environment\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mInitializing RAG Tools...\u001b[0m\n",
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/db/impl/sqlite.py:111: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\n",
      "2025-05-12 03:18:15,495 - 128449607476288 - sqlite.py-sqlite:111 - WARNING: ⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n",
      "\u001b[32m2025-05-12 03:18:31.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_environment\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mRAG Tools initialized successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm, retriever_tool, ingest_tool = setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, ingest_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 03:18:31,861 - 128449607476288 - retrieve_tool.py-retrieve_tool:315 - WARNING: Collection 'dat1' not found or error retrieving it: Collection dat1 does not exist.\n"
     ]
    }
   ],
   "source": [
    "out = retriever_tool._get_collection(collection_name='dat1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': \"Successfully ingested 46 documents into collection 'dat'.\",\n",
       " 'count': 46}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.ingest_documents(documents=docs, collection_name='dat', metadatas=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n"
     ]
    }
   ],
   "source": [
    "results = retriever_tool._run(\n",
    "    query=\"What is the capital of France?\",\n",
    "    collection_name='dat',\n",
    "    top_k=5,\n",
    "    use_mmr=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from typing import Optional, Dict, Any\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 04:03:25.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mChromaDB client initialized. Persistence directory: ./_agent_test_chroma_db\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persistent_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "logger.info(f\"ChromaDB client initialized. Persistence directory: {PERSIST_DIR}\")\n",
    "\n",
    "# Text splitter configuration\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveTool(BaseAgentTool):\n",
    "    def __init__(self, embedding_model_name: str, persist_dir: str, collection_name: Optional[str] = None):\n",
    "        super().__init__(name=\"Retrieve Tool\", description=\"Retrieves documents from the vector store\")\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.persist_dir = persist_dir\n",
    "        self.vector_store = None\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_model = self.load_embedding_model(embedding_model_name)\n",
    "\n",
    "\n",
    "    def load_embedding_model(self, model_name: str):\n",
    "        logger.info(f\"Loading embedding model: {model_name}\")\n",
    "        try:\n",
    "            embedding_model = OllamaEmbeddings(model=model_name)\n",
    "            logger.info(f\"Embedding model loaded: {embedding_model}\")\n",
    "            return embedding_model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading embedding model '{model_name}': {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    \n",
    "    def setup_tool(self):\n",
    "        logger.info(\"Setting up the retrieve tool\")\n",
    "        self.vector_store = Chroma(\n",
    "            client=persistent_client,\n",
    "            embedding_function=self.embedding_model,\n",
    "            collection_name=self.collection_name,\n",
    "        )\n",
    "        logger.info(f\"Retrieve tool set up with vector store: {self.vector_store}\")\n",
    "\n",
    "    def run(self, query, **kwargs):\n",
    "        logger.info(f\"Running retrieve tool with query: {query}\")\n",
    "        \n",
    "        # Placeholder for retrieval logic\n",
    "        retriever = self.get_retriever()\n",
    "        if not retriever:\n",
    "            logger.error(\"Retriever is not available.\")\n",
    "            return []\n",
    "        results = retriever.get_relevant_documents(query)\n",
    "        if not results:\n",
    "            logger.warning(\"No results found.\")\n",
    "            return []\n",
    "\n",
    "        logger.info(f\"Retrieved results: {results}\")\n",
    "        return self.format_docs(results)   \n",
    "    \n",
    "    def format_docs(self, docs: List[Document]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Formats documents for output.\"\"\"\n",
    "        formatted_docs = []\n",
    "        for doc in docs:\n",
    "            formatted_doc = {\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata\n",
    "            }\n",
    "            formatted_docs.append(formatted_doc)\n",
    "        logger.info(f\"Formatted {len(docs)} documents.\")\n",
    "        return formatted_docs\n",
    "\n",
    "    def get_retriever(self):\n",
    "        \"\"\"Returns the retriever object for the vector store.\"\"\"\n",
    "        try:\n",
    "            retriever = self.vector_store.as_retriever()\n",
    "            logger.info(f\"Retriever object created: {retriever}\")\n",
    "            return retriever\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating retriever: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def process_file_content(self, content: bytes, filename: str) -> List[Document]:\n",
    "        \"\"\"Processes file content into LangChain Documents using text splitting.\"\"\"\n",
    "        try:\n",
    "            text = content.decode('utf-8', errors='ignore')\n",
    "            texts = text_splitter.split_text(text)\n",
    "            docs = [Document(page_content=t, metadata={\"source\": filename}) for t in texts]\n",
    "            logger.info(f\"Processed file '{filename}' into {len(docs)} chunks.\")\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file content for {filename}: {e}\", exc_info=True)\n",
    "            return [] \n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Adds documents to the vector store.\"\"\"\n",
    "        try:\n",
    "            self.vector_store.add_documents(documents, collection_name=self.collection_name)\n",
    "            logger.info(f\"Added {len(documents)} documents to collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding documents to collection '{self.collection_name}': {e}\", exc_info=True)\n",
    "\n",
    "    def delete_collection(self, collection_name: str):\n",
    "        \"\"\"Deletes a collection from the vector store.\"\"\"\n",
    "        try:\n",
    "            self.vector_store.delete_collection(collection_name)\n",
    "            logger.info(f\"Deleted collection '{collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting collection '{collection_name}': {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 04:24:19.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_embedding_model\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading embedding model: nomic-embed-text:latest\u001b[0m\n",
      "\u001b[32m2025-05-12 04:24:19.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_embedding_model\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mEmbedding model loaded: base_url='http://localhost:11434' model='nomic-embed-text:latest' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever_data = RetrieveTool(embedding_model_name=\"nomic-embed-text:latest\", persist_dir=PERSIST_DIR, collection_name=\"dat226\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 04:24:24.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_tool\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mSetting up the retrieve tool\u001b[0m\n",
      "\u001b[32m2025-05-12 04:24:24.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup_tool\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mRetrieve tool set up with vector store: <langchain_community.vectorstores.chroma.Chroma object at 0x74d1c83a54d0>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever_data.setup_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "\u001b[32m2025-05-12 04:25:20.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_documents\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1mAdded 46 documents to collection 'dat226'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever_data.add_documents(documents=text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-12 04:25:20.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mRunning retrieve tool with query: What is the capital of France?\u001b[0m\n",
      "\u001b[32m2025-05-12 04:25:20.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_retriever\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mRetriever object created: tags=['Chroma', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x74d1c83a54d0> search_kwargs={}\u001b[0m\n",
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "\u001b[32m2025-05-12 04:25:20.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mRetrieved results: [Document(metadata={'author': '', 'creationdate': '2025-01-20T01:51:49+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'data/2501.07329v2.pdf', 'subject': '', 'title': '', 'total_pages': 5, 'trapped': '/False'}, page_content='corpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.'), Document(metadata={'author': '', 'creationdate': '2025-01-20T01:51:49+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'data/2501.07329v2.pdf', 'subject': '', 'title': '', 'total_pages': 5, 'trapped': '/False'}, page_content='corpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.'), Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 1, 'page_label': '2', 'producer': 'Skia/PDF m109 Google Docs Renderer', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'title': 'ielts-listening-practice-test-pdf-1', 'total_pages': 7}, page_content='Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with'), Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 1, 'page_label': '2', 'producer': 'Skia/PDF m109 Google Docs Renderer', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'title': 'ielts-listening-practice-test-pdf-1', 'total_pages': 7}, page_content='Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with')]\u001b[0m\n",
      "\u001b[32m2025-05-12 04:25:20.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mformat_docs\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mFormatted 4 documents.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'corpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.',\n",
       "  'metadata': {'author': '',\n",
       "   'creationdate': '2025-01-20T01:51:49+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-01-20T01:51:49+00:00',\n",
       "   'page': 4,\n",
       "   'page_label': '5',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': 'data/2501.07329v2.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 5,\n",
       "   'trapped': '/False'}},\n",
       " {'content': 'corpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.',\n",
       "  'metadata': {'author': '',\n",
       "   'creationdate': '2025-01-20T01:51:49+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-01-20T01:51:49+00:00',\n",
       "   'page': 4,\n",
       "   'page_label': '5',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': 'data/2501.07329v2.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 5,\n",
       "   'trapped': '/False'}},\n",
       " {'content': 'Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 1,\n",
       "   'page_label': '2',\n",
       "   'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       "   'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       "   'title': 'ielts-listening-practice-test-pdf-1',\n",
       "   'total_pages': 7}},\n",
       " {'content': 'Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 1,\n",
       "   'page_label': '2',\n",
       "   'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       "   'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       "   'title': 'ielts-listening-practice-test-pdf-1',\n",
       "   'total_pages': 7}}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever = retriever_data.vector_store.as_retriever()\n",
    "results = retriever_data.run(\"What is the capital of France?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'metadata'"
     ]
    }
   ],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationdate': '2025-01-20T01:51:49+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'data/2501.07329v2.pdf', 'subject': '', 'title': '', 'total_pages': 5, 'trapped': '/False'}, page_content='corpus based on public domain audio books,” in2015 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP). IEEE,\\n2015, pp. 5206–5210.\\n[36] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–129.'),\n",
       " Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 1, 'page_label': '2', 'producer': 'Skia/PDF m109 Google Docs Renderer', 'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf', 'title': 'ielts-listening-practice-test-pdf-1', 'total_pages': 7}, page_content='Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with'),\n",
       " Document(metadata={'author': '', 'creationdate': '2025-01-20T01:51:49+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'data/2501.07329v2.pdf', 'subject': '', 'title': '', 'total_pages': 5, 'trapped': '/False'}, page_content='for simultaneous speech recognition and understanding. In this\\npaper, we propose a joint speech recognition and structure\\nlearning framework (JSRSL), an end-to-end SLU model based\\non span, which can accurately transcribe speech and extract\\nstructured content simultaneously. We conduct experiments on\\nname entity recognition and intent classification using the Chinese\\ndataset AISHELL-NER and the English dataset SLURP. The\\nresults show that our proposed method not only outperforms the\\ntraditional sequence-to-sequence method in both transcription\\nand extraction capabilities but also achieves state-of-the-art\\nperformance on the two datasets.\\nIndex Terms—Speech Recognition, Spoken Language Under-\\nstanding, Information Extraction.\\nI. I NTRODUCTION\\nAutomatic speech recognition (ASR) aims to convert human\\nspeech into text in the corresponding language [1]. On the\\nother hand, SLU seeks to enhance machines’ ability to com-\\nprehend and react to human language by extracting specific'),\n",
       " Document(metadata={'author': '', 'creationdate': '2025-01-20T01:51:49+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-20T01:51:49+00:00', 'page': 0, 'page_label': '1', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': 'data/2501.07329v2.pdf', 'subject': '', 'title': '', 'total_pages': 5, 'trapped': '/False'}, page_content='based on span, which can accurately recognize speech while\\nextracting structured information. This model utilizes a re-\\nfinement module to enhance text representations for struc-\\nture prediction. Specifically, we adopt a parallel transformer\\nfor non-autoregressive end-to-end speech recognition as our\\nframework and introduce the hidden layer output of the ASR\\ndecoder into a span for structure prediction. We additionally\\nintroduce a refinement module between the ASR model and\\nthe span to strengthen extraction performance. We conduct\\nexperiments on NER and IC using the AISHELL-NER and\\nSLURP. The results indicate that the proposed idea has supe-\\nrior capabilities in speech transcription and speech understand-\\ning compared to the traditional sequence-to-sequence method.\\nIt also outperforms the current state-of-the-art (SOTA) on the\\ntwo datasets.\\nII. R ELATED WORK\\nSo far, NLU has been well developed, and many works\\nhave involved key tasks of natural language processing (NLP),')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'document': 'S\\n′\\nstart, S\\n′\\nend = ROP E(d1), ROP E(d2)\\nFor classification tasks, we treat them as sentence-level\\nspan. Let Z be the classification result, and the set of classi-\\nfication result be Ωcls, Ωcls = {1, 2, . . . , Ntype}. Firstly, we\\nuse ROPE to embed relative position information and get the\\ntoken-level classification features. Z1:N′ .\\nZ1:N′ = ROPE (HR\\n1:N′ )\\nDrawing inspiration from [24], a modified self-attention\\npooling mechanism is utilized to comprehensively extract\\nsentence-level classification result Z\\n′\\nfrom token-level fea-\\ntures. The process of the self-attention pooling mechanism is\\nas follows, AZ\\n1:N′ is attention value of Z1:N′ calculated by a\\nlinear layer.\\nAZ\\n1:N′ = Softmax (W1Z1:N′ + b1)\\nZ\\n′\\n= arg max\\nNtype\\n(WNtype (\\nX\\nN′\\nAZ\\n1:N′ × Z1:N′ ) + bNtype )\\nC. Joint Optimization\\nThe adopted ASR module calculates three loss functions\\nwhen training: the cross-entropy (CE), the mean absolute error\\n(MAE), and the minimum word error rate (MWER) loss. CE',\n",
       "  'metadata': {'author': '',\n",
       "   'creationdate': '2025-01-20T01:51:49+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-01-20T01:51:49+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': 'data/2501.07329v2.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 5,\n",
       "   'trapped': '/False'}},\n",
       " {'document': 'Paraformer for experiments. The pre-training parameters of\\nBERT-base [28] is adopted to initialize the Refiner of JSRSL\\nand the NLU module of Pipe. The α of the loss is set to 0.5.\\nThe models are trained until converges. We consistently use\\nthe Adam optimizer with a learning rate of 5e-5.\\nB. Main Result\\nTABLE I\\nCOMPARISON RESULTS WITH BENCHMARKS IN AISHELL-NER AND\\nSLURP. I N SLURP, ALL MODELS CONDUCT JOINT TRAINING AND\\nEVALUATION FOR NER AND IC TASKS . THE BERT IN [5]’ S PIPELINE\\nMODEL ARE PRE -TRAINED . THE POST AED USES 1K EXTERNAL DATA .\\nModel Paradigm\\nAISHELL-NER\\nCER (↓) F1 ( ↑)\\nTransformer [5] E2E 9.25 64.34\\nConformer [5] E2E 4.79 73.37\\nTransformer+BERT* [5] Pipeline 9.25 65.95\\nConformer+BERT* [5] Pipeline 4.79 74.90\\nSeq2Seq (Ours) E2E 3.48 76.17\\nPipe (Ours) Pipeline 1.76 77.98\\nJSRSL (Ours) E2E 1.71 80.85\\nModel Paradigm\\nSLURP\\nWER (↓) SLU F1 ( ↑) IC F1 ( ↑)\\nRNNT→BertNLU [29] Pipeline 15.20 72.35 82.45\\nEspnet SLU [26] E2E - 71.90 86.30\\nPostDec AED* [6] E2E - 80.08 89.33',\n",
       "  'metadata': {'author': '',\n",
       "   'creationdate': '2025-01-20T01:51:49+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-01-20T01:51:49+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': 'data/2501.07329v2.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 5,\n",
       "   'trapped': '/False'}},\n",
       " {'document': '34   MEDITATION35   STONE36   COINS37   TREES38   BREATHING39   PAPER40   ANXIETY',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 6,\n",
       "   'page_label': '7',\n",
       "   'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       "   'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       "   'title': 'ielts-listening-practice-test-pdf-1',\n",
       "   'total_pages': 7}},\n",
       " {'document': 'Section 1\\nQuestion 1-10Complete the notes below.WriteONE WORD AND/OR A NUMBERfor each answer.\\nBuckworth Conservation GroupRegular activities\\nBeach●   making sure the beach does not have 1 …………………… on it●   no 2 ……………………\\nNature reserve\\n●   maintaining paths●   nesting boxes for birds installed●   next task is taking action to attract 3 …………………… to the place●   identifying types of 4 ……………………●   building a new 5 ……………………\\nForthcoming events\\nSaturday\\n●   meet at Dunsmore Beach car park●   walk across the sands and reach the 6 ……………………●   take a picnic●   wear appropriate 7 ……………………\\nWoodwork session\\n●   suitable for 8 …………………… to participate in●   making 9 …………………… out of wood●   17th, from 10 a.m. to 3 p.m.●   cost of session (no camping): 10 £ ……………………',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 0,\n",
       "   'page_label': '1',\n",
       "   'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       "   'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       "   'title': 'ielts-listening-practice-test-pdf-1',\n",
       "   'total_pages': 7}},\n",
       " {'document': 'Section 2\\nQuestion 11-14Choose the correct letter,A,BorC.\\nBoat trip round Tasmania\\n11   What is the maximum number of people who can stand on each side of theboat?\\nA 9B 15C 18\\n12   What colour are the tour boats?\\nA dark redB jet blackC light green\\n13   Which lunchbox is suitable for someone who doesn’t eat meat or fish?\\nA Lunchbox 1B Lunch box 2C Lunch box 3\\n14   What should people do with their litter?\\nA take it homeB hand it to a member of staffC put it in the bins provided on the boat\\nQuestions 15 and 16ChooseTWOletters,A-E.\\nWhichTWOfeatures of the lighthouse does Lou mention?\\nA why it was builtB who built itC how long it took to buildD who staffed itE what it was built with',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 1,\n",
       "   'page_label': '2',\n",
       "   'producer': 'Skia/PDF m109 Google Docs Renderer',\n",
       "   'source': 'data/ielts_listening_practice_test_pdf_1_1_1ae068b05d.pdf',\n",
       "   'title': 'ielts-listening-practice-test-pdf-1',\n",
       "   'total_pages': 7}}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.retrieve(\n",
    "    query=\"What is the capital of France?\",\n",
    "    collection_name='dat',\n",
    "    top_k=5,\n",
    "    use_mmr=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 21:06:27,810 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Defining Knowledge Base Manager Agent ---\")\n",
    "kb_manager_agent = Agent(\n",
    "    role='Knowledge Base Manager',\n",
    "    goal=f\"Efficiently manage and retrieve information from the company's knowledge base stored in ChromaDB. Use the provided tools to ingest new documents into specific collections and retrieve relevant information based on queries.\",\n",
    "    backstory=(\n",
    "        \"You are an expert AI assistant responsible for maintaining the accuracy and accessibility \"\n",
    "        \"of the company's document knowledge base. You meticulously ingest new information using the \"\n",
    "        \"'ChromaDB Document Ingest Tool' and expertly query the database using the \"\n",
    "        \"'ChromaDB Retriever Tool' to answer questions. Always specify the correct collection name.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    verbose=True, # Set to True to see LLM reasoning and tool calls\n",
    "    allow_delegation=False,\n",
    "    \n",
    "    # memory=True # Optional: Enable memory for conversation context if needed\n",
    ")\n",
    "logger.info(f\"Agent '{kb_manager_agent.role}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_repr = repr(docs)\n",
    "metas_repr = repr(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ingest = Task(\n",
    "        description=(\n",
    "            f\"Ingest the following set of documents into the '{TEST_COLLECTION_NAME}' collection \"\n",
    "            f\"using the 'ChromaDB Document Ingest Tool'. Ensure you pass both the document texts \"\n",
    "            f\"and their corresponding metadata.\\n\\n\"\n",
    "            f\"Documents to ingest: {docs_repr}\\n\"\n",
    "            f\"Associated Metadatas: {metas_repr}\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            f\"Confirmation that {len(docs)} documents were successfully ingested \"\n",
    "            f\"into the '{TEST_COLLECTION_NAME}' collection.\"\n",
    "        ),\n",
    "        agent=kb_manager_agent,\n",
    "        tools=[ingest_tool] # Optional: Limit tools for this specific task\n",
    "    )\n",
    "logger.info(\"Ingestion task defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Joint Automatic Speech Recognition and Machine Translation (JASR-MT) task?\"\n",
    "task_retrieve = Task(\n",
    "    description=(\n",
    "        f\"Search the '{TEST_COLLECTION_NAME}' collection using the 'ChromaDB Retriever Tool' to find information relevant to the following query: '{query}'. \"\n",
    "        f\"Retrieve the top 3 most relevant documents using MMR for diversity. \" # Explicitly guide MMR usage\n",
    "        f\"Present the content of the retrieved documents clearly.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A summary or list of the content from the top 3 relevant documents found in the \"\n",
    "        f\"'{TEST_COLLECTION_NAME}' collection related to '{query}', retrieved using MMR.\"\n",
    "    ),\n",
    "    agent=kb_manager_agent,\n",
    "    context=[task_ingest], # Make this task depend on the ingestion task\n",
    "    tools=[retriever_tool] # Optional: Limit tools for this specific task\n",
    ")\n",
    "logger.info(\"Retrieval task defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 21:06:27,884 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reflection_agent = Agent(\n",
    "    role='Reflection Agent',\n",
    "    goal=\"Reflect on the tasks and provide insights.\",\n",
    "    backstory=(\n",
    "        \"You are an AI assistant designed to reflect on the tasks performed by the Knowledge Base Manager. \"\n",
    "        \"Your role is to analyze the ingestion and retrieval processes, ensuring they align with the company's goals.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[],\n",
    "    verbose=True, # Set to True to see LLM reasoning and tool calls\n",
    "    allow_delegation=False,\n",
    ")\n",
    "logger.info(\"Reflection agent created.\")\n",
    "reflection_task = Task(\n",
    "    description=(\n",
    "        \"Reflect on the tasks performed by the Knowledge Base Manager. \"\n",
    "        \"Analyze the ingestion and retrieval processes, ensuring they align with the company's goals.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Insights and analysis of the ingestion and retrieval processes, \"\n",
    "        \"ensuring they align with the company's goals.\"\n",
    "    ),\n",
    "    agent=reflection_agent,\n",
    "    context=[task_ingest, task_retrieve], # Make this task depend on both ingestion and retrieval tasks\n",
    "    tools=[] # No tools needed for reflection\n",
    ")\n",
    "logger.info(\"Reflection task defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 21:06:27,908 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Create and Run Crew\n",
    "logger.info(\"--- Creating and Running the Crew ---\")\n",
    "company_knowledge_crew = Crew(\n",
    "    agents=[kb_manager_agent],\n",
    "    tasks=[task_ingest, task_retrieve],\n",
    "    process=Process.sequential, # Ensure tasks run in order: ingest then retrieve\n",
    "    verbose=True # Use verbose=2 to see detailed LLM thoughts and tool calls\n",
    ")\n",
    "\n",
    "# result = company_knowledge_crew.kickoff()\n",
    "\n",
    "# logger.info(\"--- Crew Execution Finished ---\")\n",
    "# print(\"\\n\\n===== Final Crew Result =====\")\n",
    "# print(result)\n",
    "# print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Result: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from crewai import Agent, Task, Crew, Process\n",
    "# from crewai_tools import BaseTool, SerperDevTool\n",
    "# from langchain_community.chat_models.ollama import ChatOllama\n",
    "# from agent.config.load_config import agents_config\n",
    "# from agent.tools.nl2sql_tool import NL2SQLTool, ValidateSQLQueryTool\n",
    "# from dotenv import load_dotenv\n",
    "# from typing import List, Optional, Dict, Any\n",
    "# import chromadb\n",
    "# from os.path import dirname, join, abspath\n",
    "\n",
    "# # Import the retrieval tools\n",
    "# from agent.tools.retrieve_tool import RetrieveTool, IngestTool\n",
    "\n",
    "# # Load environment variables\n",
    "# env_path = join(dirname(dirname(abspath(__file__))), '.env')\n",
    "# load_dotenv(env_path)\n",
    "\n",
    "# class MultiAgentSystem:\n",
    "#     \"\"\"\n",
    "#     A multi-agent system using CrewAI framework.\n",
    "#     Integrates company, customer service, HR, and recommender agents.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, model_name: str = \"deepseek-r1:1.5b\"):\n",
    "#         \"\"\"\n",
    "#         Initialize the multi-agent system with specified LLM model.\n",
    "        \n",
    "#         Args:\n",
    "#             model_name: Name of the LLM model to use across agents\n",
    "#         \"\"\"\n",
    "#         self.model_name = model_name\n",
    "#         self.llm = self._load_llm(model_name)\n",
    "        \n",
    "#         # Initialize tools\n",
    "#         self.tools = self._setup_tools()\n",
    "        \n",
    "#         # Initialize agents\n",
    "#         self.agents = self._setup_agents()\n",
    "        \n",
    "#         # Create crew\n",
    "#         self.crew = self._setup_crew()\n",
    "    \n",
    "#     def _load_llm(self, model_name: str):\n",
    "#         \"\"\"Load the language model.\"\"\"\n",
    "#         return ChatOllama(model=model_name, temperature=0.2, max_tokens=2000)\n",
    "    \n",
    "#     def _setup_tools(self) -> Dict[str, List[BaseTool]]:\n",
    "#         \"\"\"Set up tools for each agent.\"\"\"\n",
    "#         # Common tools\n",
    "#         serper_api_key = os.environ.get('SERPER_API_KEY')\n",
    "#         search_tool = SerperDevTool(api_key=serper_api_key) if serper_api_key else None\n",
    "#         nl2sql_tool = NL2SQLTool()\n",
    "#         validate_sql_tool = ValidateSQLQueryTool()\n",
    "        \n",
    "#         # Retrieval tools\n",
    "#         retriever_tool = RetrieveTool(embedding_model_name=\"all-MiniLM-L6-v2\")\n",
    "#         ingest_tool = IngestTool(retriever_tool=retriever_tool)\n",
    "        \n",
    "#         # Define tool sets for each agent\n",
    "#         return {\n",
    "#             \"company_agent\": [tool for tool in [search_tool, nl2sql_tool, validate_sql_tool, retriever_tool, ingest_tool] if tool],\n",
    "#             \"customer_service_agent\": [tool for tool in [search_tool, nl2sql_tool, validate_sql_tool, retriever_tool] if tool],\n",
    "#             \"hr_agent\": [tool for tool in [search_tool, nl2sql_tool, retriever_tool] if tool],\n",
    "#             \"naive_agent\": [tool for tool in [search_tool, retriever_tool] if tool],\n",
    "#             \"recommender_agent\": [tool for tool in [search_tool, retriever_tool] if tool]\n",
    "#         }\n",
    "    \n",
    "#     def _setup_agents(self) -> Dict[str, Agent]:\n",
    "#         \"\"\"Set up all agents with their tools and configurations.\"\"\"\n",
    "#         agents = {}\n",
    "        \n",
    "#         # Create each agent using their config\n",
    "#         for agent_type in [\"company_agent\", \"customer_service_agent\", \"hr_agent\", \"naive_agent\", \"recommender_agent\"]:\n",
    "#             config = agents_config.get(agent_type, {})\n",
    "#             agents[agent_type] = Agent(\n",
    "#                 role=config.get('role', f\"{agent_type.replace('_', ' ').title()}\"),\n",
    "#                 goal=config.get('goal', \"Help the company achieve its objectives\"),\n",
    "#                 backstory=config.get('backstory', \"An experienced professional in the field\"),\n",
    "#                 verbose=config.get('verbose', True),\n",
    "#                 allow_delegation=config.get('allow_delegation', False),\n",
    "#                 tools=self.tools.get(agent_type, []),\n",
    "#                 llm=self.llm\n",
    "#             )\n",
    "        \n",
    "#         return agents\n",
    "    \n",
    "#     def _setup_crew(self) -> Crew:\n",
    "#         \"\"\"Set up the crew with all agents and their tasks.\"\"\"\n",
    "#         # Define tasks for each agent\n",
    "#         company_task = Task(\n",
    "#             description=\"Analyze company data and make strategic decisions\",\n",
    "#             agent=self.agents[\"company_agent\"],\n",
    "#             expected_output=\"Strategic analysis and recommendations for the company\"\n",
    "#         )\n",
    "        \n",
    "#         customer_task = Task(\n",
    "#             description=\"Address customer inquiries and improve satisfaction\",\n",
    "#             agent=self.agents[\"customer_service_agent\"],\n",
    "#             expected_output=\"Customer service report and satisfaction improvement plan\"\n",
    "#         )\n",
    "        \n",
    "#         hr_task = Task(\n",
    "#             description=\"Manage employee relations and recruitment\",\n",
    "#             agent=self.agents[\"hr_agent\"],\n",
    "#             expected_output=\"HR management report and recruitment strategy\"\n",
    "#         )\n",
    "        \n",
    "#         recommender_task = Task(\n",
    "#             description=\"Provide personalized recommendations to customers\",\n",
    "#             agent=self.agents[\"recommender_agent\"],\n",
    "#             expected_output=\"Customer recommendation system plan and implementation strategy\"\n",
    "#         )\n",
    "        \n",
    "#         # Create crew with all agents and tasks\n",
    "#         return Crew(\n",
    "#             agents=list(self.agents.values()),\n",
    "#             tasks=[company_task, customer_task, hr_task, recommender_task],\n",
    "#             verbose=2,\n",
    "#             process=Process.sequential  # Can be changed to Process.hierarchical if needed\n",
    "#         )\n",
    "    \n",
    "#     def run(self, query: str) -> str:\n",
    "#         \"\"\"\n",
    "#         Run the multi-agent system with a specific query.\n",
    "        \n",
    "#         Args:\n",
    "#             query: User query to process across agents\n",
    "            \n",
    "#         Returns:\n",
    "#             str: Results from the crew's execution\n",
    "#         \"\"\"\n",
    "#         # You could customize the tasks or crew configuration based on the query\n",
    "#         result = self.crew.kickoff(inputs={\"query\": query})\n",
    "#         return result\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize the multi-agent system\n",
    "#     system = MultiAgentSystem(model_name=\"deepseek-r1:1.5b\")\n",
    "    \n",
    "#     # Example document ingestion for testing the retrieval tool\n",
    "#     documents = [\n",
    "#         \"Our company specializes in AI-powered solutions for businesses.\",\n",
    "#         \"Customer satisfaction is our top priority, with 24/7 support available.\",\n",
    "#         \"The HR department handles recruitment, onboarding, and employee relations.\",\n",
    "#         \"Our recommendation engine uses machine learning to suggest products.\",\n",
    "#         \"Company policies include flexible working hours and remote options.\"\n",
    "#     ]\n",
    "    \n",
    "#     metadatas = [\n",
    "#         {\"category\": \"company_info\", \"department\": \"general\"},\n",
    "#         {\"category\": \"customer_service\", \"department\": \"support\"},\n",
    "#         {\"category\": \"hr\", \"department\": \"human_resources\"},\n",
    "#         {\"category\": \"technology\", \"department\": \"engineering\"},\n",
    "#         {\"category\": \"policy\", \"department\": \"human_resources\"}\n",
    "#     ]\n",
    "    \n",
    "#     # Access company agent and ingest sample documents\n",
    "#     retriever_tool = next((t for t in system.tools[\"company_agent\"] if isinstance(t, RetrieveTool)), None)\n",
    "#     if retriever_tool:\n",
    "#         retriever_tool.ingest_documents(\n",
    "#             documents=documents,\n",
    "#             collection_name=\"company_docs\",\n",
    "#             metadatas=metadatas\n",
    "#         )\n",
    "    \n",
    "#     # Run system with a test query\n",
    "#     result = system.run(\"What are our company's HR policies and how can we improve employee satisfaction?\")\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPER_API_KEY\"] = \"a6ce4b5fc4ef3f9754144d519ecf9e418bc1c7bc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrieveTool(name='ChromaDB Retriever Tool', description=\"Tool Name: ChromaDB Retriever Tool\\nTool Arguments: {'query': {'description': None, 'type': 'str'}, 'collection_name': {'description': None, 'type': 'str'}, 'top_k': {'description': None, 'type': 'int'}, 'use_mmr': {'description': None, 'type': 'bool'}, 'mmr_diversity': {'description': None, 'type': 'float'}}\\nTool Description: Retrieves relevant information from a specified ChromaDB collection. Can use standard similarity search or Maximum Marginal Relevance (MMR) for diversity.\", args_schema=<class 'abc.RetrieveToolSchema'>, description_updated=False, cache_function=<function BaseTool.<lambda> at 0x73d524542980>, result_as_answer=False, input_schema=<class 'agent.tools.retrieve_tool.RetrieveInput'>, persist_dir='./_agent_test_chroma_db')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ctai-datpd-l/anaconda3/envs/ai_agents/lib/python3.11/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "import operator\n",
    "from agent.utils.helper_function import extract_clean_json\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.tools import Tool\n",
    "# from crewai_tools import DuckDuckGoSearchRun # Re-use crewai tool directly\n",
    "from crewai_tools import SerperDevTool\n",
    "# For Customer Retriever (Example using FAISS)\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.tools.vectorstore.tool import VectorStoreQATool\n",
    "\n",
    "# from langgraph.graph import StateGraph, END\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver # For state persistence/debugging if needed\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- Initialize LLM ---\n",
    "# Using a more capable model might be beneficial for classification and reflection\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.2, num_predict=2000)\n",
    "\n",
    "# --- Initialize Tools ---\n",
    "search_tool = SerperDevTool(n_results=2)\n",
    "\n",
    "# --- Mock Customer Database & Retriever Tool ---\n",
    "# In a real scenario, connect to your actual database/vector store\n",
    "customer_data = {\n",
    "    \"cust123\": [\n",
    "        Document(page_content=\"Customer John Doe. Premium plan member since 2022. Last interaction: support ticket #5678 resolved.\", metadata={\"customer_id\": \"cust123\", \"source\": \"crm\"}),\n",
    "        Document(page_content=\"John Doe purchased Product X in Jan 2023 and Product Y in Nov 2023.\", metadata={\"customer_id\": \"cust123\", \"source\": \"orders\"}),\n",
    "    ],\n",
    "    \"cust456\": [\n",
    "        Document(page_content=\"Customer Jane Smith. Basic plan member since 2023. No open support tickets.\", metadata={\"customer_id\": \"cust456\", \"source\": \"crm\"}),\n",
    "    ]\n",
    "}\n",
    "# Flatten documents for indexing\n",
    "all_docs = [doc for docs in customer_data.values() for doc in docs]\n",
    "\n",
    "if all_docs:\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = OllamaEmbeddings(model=\"hf.co/CompendiumLabs/bge-base-en-v1.5-gguf:latest\")\n",
    "    vector_store = Chroma.from_documents(documents=all_docs, embedding=embeddings, persist_directory=PERSIST_DIR, collection_name=TEST_COLLECTION_NAME)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 docs\n",
    "\n",
    "    customer_retriever_tool = VectorStoreQATool(\n",
    "        name=\"customer_info_retriever\",\n",
    "        description=\"Searches and returns information about a specific customer from the customer database based on their ID.\",\n",
    "        vectorstore=vector_store,\n",
    "        llm=llm # The tool itself can use an LLM for QA over retrieved docs\n",
    "    )\n",
    "    # Note: This tool needs the query to implicitly or explicitly contain the customer ID for filtering,\n",
    "    # or the retriever needs more sophisticated filtering setup.\n",
    "    # For simplicity here, we'll assume the query passed to the tool includes context like \"for customer cust123\".\n",
    "else:\n",
    "    # Create a dummy tool if no customer data exists\n",
    "    def dummy_retriever_func(query: str):\n",
    "        return \"No customer data available for retrieval.\"\n",
    "    customer_retriever_tool = Tool(\n",
    "        name=\"customer_info_retriever\",\n",
    "        description=\"Searches and returns information about a specific customer. Currently, no data is loaded.\",\n",
    "        func=dummy_retriever_func\n",
    "    )\n",
    "\n",
    "print(\"Tools Initialized.\")\n",
    "# print(f\"Customer Retriever Tool Ready: {customer_retriever_tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.vectorstore.tool import VectorStoreQATool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 21:08:16,817 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2025-05-05 21:08:16,821 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2025-05-05 21:08:16,824 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "CrewAI Agent Definitions Ready (for conceptual reference).\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent as CrewAgent # Alias to avoid confusion with node names\n",
    "\n",
    "# Agent 1: Company Agent (Definition)\n",
    "company_agent_def = CrewAgent(\n",
    "    role='Company Information Specialist',\n",
    "    goal='Provide accurate information about the company (e.g., Google) including location, products, services, history, and mission, based on search results and internal knowledge.',\n",
    "    backstory='You are an AI assistant dedicated to representing the company accurately and helpfully to external queries. You use search tools to find the latest public information.',\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Agent 2: Customer Agent (Definition)\n",
    "customer_agent_def = CrewAgent(\n",
    "    role='Customer Support Agent',\n",
    "    goal='Answer customer-specific questions by retrieving their information from the customer database using their customer ID. Handle queries about plans, purchase history, support tickets etc.',\n",
    "    backstory='You are a helpful customer support agent with access to the customer database. You must use the customer ID provided to retrieve relevant information using your specialized tool.',\n",
    "    tools=[search_tool, retriever_tool], # Has retriever and search\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Agent 3: Naive Agent (Definition)\n",
    "naive_agent_def = CrewAgent(\n",
    "    role='General Knowledge Assistant',\n",
    "    goal='Answer general knowledge questions or queries that do not fall under specific company or customer information categories. Use search tools to find relevant information online.',\n",
    "    backstory='You are a general-purpose AI assistant capable of answering a wide range of topics by searching the internet.',\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"CrewAI Agent Definitions Ready (for conceptual reference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Crew, Process\n",
    "from typing import List, Dict, Any, Optional, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph State Defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    original_query: str\n",
    "    customer_id: Optional[str]\n",
    "    rewritten_query: str\n",
    "    classified_agent: Literal[\"Company\", \"Customer\", \"Naive\", \"Unknown\"]\n",
    "    agent_response: str\n",
    "    reflection: str\n",
    "    is_final: bool\n",
    "    error: Optional[str]\n",
    "    retry_count: int\n",
    "    suggested_questions: List[str]  # New field for suggested questions\n",
    "\n",
    "\n",
    "print(\"LangGraph State Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_node(state: AgentState):\n",
    "    \"\"\"Rewrites the user query and classifies which agent should handle it.\"\"\"\n",
    "    logger.info(\"--- Node: Rewriter ---\")\n",
    "    if state.get(\"customer_id\"):\n",
    "        logger.info(f\"Customer ID found: {state['customer_id']}\")\n",
    "        return {\n",
    "                    \"rewritten_query\": state['original_query'], # No rewriting needed\n",
    "                    \"classified_agent\": \"Customer\", # Default to Customer if customer_id is present\n",
    "                    \"customer_id\": state['customer_id'], # Use state customer_id directly\n",
    "                    \"error\": None\n",
    "                }\n",
    "\n",
    "    query = state['original_query']\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert query processor. Your tasks are:\n",
    "    1.  Correct any spelling mistakes in the user query.\n",
    "    2.  Rephrase the query for maximum clarity if necessary.\n",
    "    3.  Classify the query's intent to determine the best agent to handle it:\n",
    "        - 'Company': For questions about a specific company's details (location, products, services, mission etc.). Assume the company is 'Google' if not specified.\n",
    "        - 'Customer': For questions related to a specific customer's account, history, plan, support tickets etc. These queries MUST contain or imply a customer ID.\n",
    "        - 'Naive': For general knowledge questions, queries unrelated to the company or a specific customer.\n",
    "        - 'Unknown': If the query is ambiguous or cannot be clearly classified.\n",
    "    4. Extract any customer ID mentioned (like cust123, user45, id: 7890). Return null if no ID is found.\n",
    "    Provide your output in the specified JSON format.\n",
    "    For example, for the query 'What is the status of my order with customer ID cust123?', your output should be:\n",
    "    <output>\n",
    "    {{\n",
    "        \"rewritten_query\": \"What is the status of my order?\",\n",
    "        \"agent_classification\": \"Customer\",\n",
    "        \"extracted_customer_id\": \"cust123\"\n",
    "    }}\n",
    "    </output>\n",
    "    If no customer ID is found, return null for that field.\n",
    "    If the query is not clear or cannot be classified, return 'Unknown' for the agent classification and null for the customer ID.\n",
    "    Your output should be a JSON object with the following fields:\n",
    "\n",
    "    {{\n",
    "        \"rewritten_query\": \"<corrected and clarified query>\",\n",
    "        \"agent_classification\": \"<Company|Customer|Naive|Unknown>\",\n",
    "        \"extracted_customer_id\": \"<customer ID or null>\"\n",
    "\n",
    "    }}\n",
    "\n",
    "    **CRITICAL:** You MUST format your output ONLY as a JSON object that strictly adheres to the following schema. Do NOT include any other text, explanations, or markdown formatting like ```json ``` around the JSON object itself.\"\"\"\n",
    "\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Process the following user query: {query}\")\n",
    "    ])\n",
    "\n",
    "    # Use OpenAI Functions for structured output\n",
    "    rewriter_chain = prompt | llm\n",
    "\n",
    "    try:\n",
    "        response = rewriter_chain.invoke({\"query\": query})\n",
    "        logger.info(response.content)\n",
    "        response = extract_clean_json(response.content)\n",
    "        if response:\n",
    "                # response = json.loads(json_str)\n",
    "                print(f\"Rewriter Output: {response}\")\n",
    "                return {\n",
    "                    \"rewritten_query\": response['rewritten_query'],\n",
    "                    \"classified_agent\": response['agent_classification'],\n",
    "                    \"customer_id\": response.get('extracted_customer_id') or state.get('customer_id'), # Prioritize extracted, fallback to state\n",
    "                    \"error\": None\n",
    "                }\n",
    "        else:\n",
    "            print(\"Rewriter output was not valid JSON.\")\n",
    "            return {\n",
    "                \"rewritten_query\": query,  # Fallback to original query\n",
    "                \"classified_agent\": \"Unknown\",\n",
    "                \"customer_id\": None,\n",
    "                \"error\": \"Invalid JSON response from rewriter.\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Rewriter node: {e}\")\n",
    "        return {\"error\": f\"Failed to process query: {e}\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"original_query\": \"What is the mission of Google?\",\n",
    "    \"customer_id\": None,\n",
    "    \"rewritten_query\": \"\",\n",
    "    \"classified_agent\": \"\",\n",
    "    \"agent_response\": \"\",\n",
    "    \"reflection\": \"\",\n",
    "    \"is_final\": False,\n",
    "    \"error\": None,\n",
    "    \"retry_count\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewriter Output: {'rewritten_query': 'What is the mission of Google?', 'agent_classification': 'Naive', 'extracted_customer_id': None}\n"
     ]
    }
   ],
   "source": [
    "response = rewrite_query_node(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewritten_query': 'What is the mission of Google?',\n",
       " 'classified_agent': 'Naive',\n",
       " 'customer_id': None,\n",
       " 'error': None}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.rewrite_query_node(state: __main__.AgentState)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_query_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute_agent_node(state: AgentState, agent_def: CrewAgent, agent_name: str, ):\n",
    "    \"\"\"Generic function to execute the logic of a specific agent.\"\"\"\n",
    "    print(f\"--- Node: Execute {agent_name} Agent ---\")\n",
    "    if state.get(\"error\"): return {} # Don't run if prior error\n",
    "\n",
    "    query = state['rewritten_query']\n",
    "    customer_id = state.get('customer_id') # Relevant for Customer agent\n",
    "\n",
    "    # Construct a prompt using the agent's definition\n",
    "    prompt_messages = [\n",
    "        SystemMessage(content=f\"Role: {agent_def.role}\\nGoal: {agent_def.goal}\\nBackstory: {agent_def.backstory}\"),\n",
    "        HumanMessage(content=f\"User Query: {query}\")\n",
    "    ]\n",
    "    if agent_name == \"Customer\" and customer_id:\n",
    "         prompt_messages.append(HumanMessage(content=f\"Context: Apply this query to customer ID: {customer_id}.\"))\n",
    "    elif agent_name == \"Customer\" and not customer_id:\n",
    "         return {\"agent_response\": \"Cannot answer customer question without a Customer ID.\", \"error\": \"Missing Customer ID\"}\n",
    "\n",
    "\n",
    "    # Simplified tool handling for LangGraph node\n",
    "    # A more robust implementation would use LangChain's agent executors or tool calling\n",
    "    available_tools = {tool.name: tool for tool in agent_def.tools}\n",
    "    tool_response = \"\"\n",
    "\n",
    "    # Basic tool check (can be expanded)\n",
    "    # This is a placeholder - real tool use requires more complex agent logic (like ReAct or OpenAI Functions Agent)\n",
    "    if \"duckduckgo_search\" in available_tools and agent_name != \"Customer\": # Example: Use search for non-customer\n",
    "        try:\n",
    "            tool_response = f\"\\nSearch Results: {search_tool.run(query)}\"\n",
    "            prompt_messages.append(AIMessage(content=f\"Tool Used: duckduckgo_search\\nResult: {tool_response[:500]}...\")) # Add tool result snippet\n",
    "        except Exception as e:\n",
    "            print(f\"Error using search tool: {e}\")\n",
    "            tool_response = \"\\nSearch tool failed.\"\n",
    "\n",
    "    if \"ChromaDB Retriever Tool\" in available_tools and agent_name == \"Customer\":\n",
    "        try:\n",
    "            logger.info(f\"Using customer retriever tool for query: {query} with customer ID: {customer_id}\")\n",
    "            # Pass query potentially enriched with customer ID context\n",
    "            retriever_query = f\"Info for customer {customer_id}: {query}\"\n",
    "            tool_response = f\"\\nCustomer DB Info: {retriever_tool._run(query=retriever_query, collection_name=customer_id, use_mmr=False)}\"\n",
    "            logger.info(f\"Tool Response: {tool_response}\")\n",
    "            prompt_messages.append(AIMessage(content=f\"Tool Used: ChromaDB Retriever Tool\\nResult: {tool_response}...\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error using customer retriever tool: {e}\")\n",
    "            tool_response = \"\\nCustomer retrieval tool failed.\"\n",
    "\n",
    "    # Final LLM call to generate response based on persona and potential tool output\n",
    "    final_prompt = ChatPromptTemplate.from_messages(prompt_messages)\n",
    "    chain = final_prompt | llm\n",
    "    try:\n",
    "        response = chain.invoke({}) # Query is already in messages\n",
    "        print(f\"{agent_name} Agent Response Snippet: {response.content[:200]}...\")\n",
    "        return {\"agent_response\": response.content, \"error\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {agent_name} agent LLM call: {e}\")\n",
    "        return {\"error\": f\"{agent_name} agent failed during generation: {e}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Node 5: Reflection ---\n",
    "class ReflectionOutput(BaseModel):\n",
    "    \"\"\"Structured output for the Reflection node.\"\"\"\n",
    "    feedback: str = Field(description=\"Constructive feedback on the response's relevance and correctness relative to the original query.\")\n",
    "    is_final_answer: bool = Field(description=\"True if the answer is satisfactory and directly addresses the original query, False otherwise.\")\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    \"\"\"Reflects on the generated answer, checking relevance and correctness.\"\"\"\n",
    "    print(\"--- Node: Reflection ---\")\n",
    "    if state.get(\"error\"): return {\"is_final\": True} # If error occurred, end the loop\n",
    "\n",
    "    original_query = state['original_query']\n",
    "    agent_response = state['agent_response']\n",
    "    # rewritten_query = state['rewritten_query'] # Could also be used for context\n",
    "\n",
    "    if not agent_response:\n",
    "         print(\"No agent response to reflect on.\")\n",
    "         return {\"reflection\": \"No response generated.\", \"is_final\": True, \"error\": \"Reflection failed: No agent response found.\"}\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\"You are a meticulous quality assurance reviewer. Your task is to evaluate an AI agent's response based on the user's original query.\n",
    "                    Assess the following:\n",
    "                    1.  **Relevance:** Does the response directly address the user's original question?\n",
    "                    2.  **Correctness:** Is the information likely correct (based on general knowledge or provided context)? You don't need to verify external facts exhaustively, but check for obvious flaws or contradictions.\n",
    "                    3.  **Completeness:** Does the response sufficiently answer the question?\n",
    "\n",
    "                    Provide  constructive feedback and determine if the answer is final (good enough) or needs revision/retry. Use the specified JSON format.\n",
    "                    For example, if the original query was 'What is the capital of France?' and the agent response was 'The capital of France is Paris.', your output should be:\n",
    "                    <output>\n",
    "                    {{\n",
    "                        \"feedback\": \"The response is relevant, correct, and complete. It directly answers the question.\",\n",
    "                        \"is_final_answer\": true\n",
    "                    }}\n",
    "                    </output>\n",
    "                    If the original query was 'What is the capital of France?' and the agent response was 'The capital of France is Berlin.', your output should be:\n",
    "                    <output>\n",
    "                    {{\n",
    "                        \"feedback\": \"The response is relevant but incorrect. The capital of France is Paris, not Berlin.\",\n",
    "                        \"is_final_answer\": false\n",
    "                    }}\n",
    "                    </output>\n",
    "                    If the original query was 'What is the capital of France?' and the agent response was 'I don't know.', your output should be:\n",
    "                    <output>\n",
    "                    {{\n",
    "                        \"feedback\": \"The response is not relevant and does not answer the question. It should provide the correct information.\",\n",
    "                        \"is_final_answer\": false\n",
    "                    }}\n",
    "                    </output>\n",
    "                    Your output should be a JSON object with the following fields:\n",
    "                    {{\n",
    "                        \"feedback\": \"<constructive feedback on relevance, correctness, and completeness>\",\n",
    "                        \"is_final_answer\": <true|false>\n",
    "                    }}\n",
    "                    **CRITICAL:** You MUST format your output ONLY as a JSON object that strictly adheres to the above schema. Do NOT include any other text, explanations, or markdown formatting like ```json ``` around the JSON object itself.\n",
    "                    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Original Query: {original_query}\\n\\nAgent Response:\\n{agent_response}\\n\\nPlease evaluate and provide feedback. Your output should be a JSON object as specified above.\")\n",
    "    ])\n",
    "\n",
    "    reflection_chain = prompt | llm\n",
    "    try:\n",
    "        response = reflection_chain.invoke({\n",
    "            \"original_query\": original_query,\n",
    "            \"agent_response\": agent_response\n",
    "        })\n",
    "        response = extract_clean_json(response.content)\n",
    "        logger.info(f\"Reflection Chain Response: {response}\")\n",
    "        logger.info(f\"Reflection Chain JSON: {response}\")\n",
    "        if response:\n",
    "            # response = json.loads(json_str)\n",
    "\n",
    "            logger.info(f\"Reflection Chain Response: {response}\")\n",
    "            return {\n",
    "                \"reflection\": response['feedback'],\n",
    "                \"is_final\": response['is_final_answer'],\n",
    "                \"retry_count\": state.get('retry_count', 0) + 1, # Increment retry count\n",
    "                \"error\": None\n",
    "            }\n",
    "        else:\n",
    "            logger.info(\"Reflection output was not valid JSON.\")\n",
    "            return {\n",
    "                \"reflection\": \"Invalid JSON response from reflection.\",\n",
    "                \"is_final\": False,\n",
    "                \"retry_count\": state.get('retry_count', 0) + 1,\n",
    "                \"error\": \"Invalid JSON response from reflection.\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error in Reflection node: {e}\")\n",
    "        # Decide how to handle reflection error - maybe treat as non-final?\n",
    "        return {\"error\": f\"Reflection failed: {e}\", \"is_final\": False, \"retry_count\": state.get('retry_count', 0) + 1}\n",
    "\n",
    "logger.info(\"LangGraph Nodes Defined.\")\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def route_query(state: AgentState):\n",
    "    \"\"\"Routes to the appropriate agent node based on classification.\"\"\"\n",
    "    logger.info(f\"--- Conditional Edge: Routing Query ---\")\n",
    "    if state.get(\"error\"):\n",
    "        logger.info(\"Error detected before routing.\")\n",
    "        return \"error_handler\" # Or END directly\n",
    "\n",
    "    classification = state['classified_agent']\n",
    "    logger.info(f\"Routing based on classification: {classification}\")\n",
    "    if classification == \"Company\":\n",
    "        return \"execute_company\"\n",
    "    elif classification == \"Customer\":\n",
    "        # Add check for customer ID existence\n",
    "        if state.get(\"customer_id\"):\n",
    "             return \"execute_customer\"\n",
    "        else:\n",
    "             logger.info(\"Customer classification but no ID found.\")\n",
    "             # Update state to reflect missing ID issue and go to reflection/end\n",
    "             state[\"error\"] = \"Query classified as Customer, but no Customer ID was provided or found.\"\n",
    "             state[\"agent_response\"] = \"I need a customer ID to answer that question.\"\n",
    "             return \"reflect\" # Go to reflection to potentially end gracefully\n",
    "    elif classification == \"Naive\":\n",
    "        return \"execute_naive\"\n",
    "    else: # Unknown or error during classification\n",
    "        logger.info(\"Classification is Unknown or invalid.\")\n",
    "        state[\"error\"] = f\"Could not determine the right agent for the query (classification: {classification}).\"\n",
    "        state[\"agent_response\"] = \"I'm not sure how to handle that query. Could you please rephrase?\"\n",
    "        return \"reflect\" # Go to reflection\n",
    "\n",
    "def decide_after_reflection(state: AgentState):\n",
    "    \"\"\"Decides whether to end the process or retry based on reflection.\"\"\"\n",
    "    logger.info(f\"--- Conditional Edge: After Reflection ---\")\n",
    "    is_final = state['is_final']\n",
    "    retry_count = state['retry_count']\n",
    "    max_retries = 2 # Set a limit for retries\n",
    "\n",
    "    if state.get(\"error\") and \"Reflection failed\" not in state[\"error\"]: # Handle agent errors first\n",
    "        logger.info(f\"Ending due to execution error: {state['error']}\")\n",
    "        return END # End directly on execution error\n",
    "\n",
    "    logger.info(f\"Reflection result: is_final={is_final}, Retry count={retry_count}\")\n",
    "\n",
    "    if is_final:\n",
    "        logger.info(\"Reflection approved. Ending.\")\n",
    "        return END\n",
    "    elif retry_count >= max_retries:\n",
    "        logger.info(f\"Max retries ({max_retries}) reached. Ending.\")\n",
    "        # Optionally, provide the last reflection feedback\n",
    "        state[\"agent_response\"] += f\"\\n\\n[System Note: Max retries reached. Last feedback: {state.get('reflection', 'N/A')}]\"\n",
    "        return END\n",
    "    else:\n",
    "        print(\"Reflection suggests retry. Looping back to Rewriter.\")\n",
    "        # Add reflection feedback to the original query for context in the next loop? Optional.\n",
    "        # state['original_query'] = f\"{state['original_query']}\\n\\n[Retry Context: Previous attempt failed. Feedback: {state.get('reflection', 'N/A')}]\"\n",
    "        return \"rewrite_query\" # Loop back to the start\n",
    "\n",
    "def question_generator_node(state: AgentState):\n",
    "    \"\"\"Generates relevant follow-up questions based on previous query and response context.\"\"\"\n",
    "    logger.info(\"--- Node: Question Generator ---\")\n",
    "\n",
    "    if state.get(\"error\"):\n",
    "        return {\"suggested_questions\": [], \"error\": state[\"error\"]}\n",
    "\n",
    "    original_query = state['original_query']\n",
    "    agent_response = state['agent_response']\n",
    "    agent_type = state['classified_agent']\n",
    "    customer_id = state.get('customer_id')\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert question generator. Your task is to generate 3-5 highly relevant follow-up questions\n",
    "    based on the original query and the response provided.\n",
    "\n",
    "    The questions should:\n",
    "    1. Help explore the topic in more depth\n",
    "    2. Be directly related to the response content\n",
    "    3. Include specific data points mentioned in the response\n",
    "    4. Focus on the most interesting or valuable aspects\n",
    "    5. Be tailored to the agent type (Company, Customer, or General Knowledge)\n",
    "\n",
    "    For Company questions: Focus on products, services, history, competition, or market position\n",
    "    For Customer questions: Focus on account details, usage patterns, recommendations, or support issues\n",
    "    For General Knowledge: Focus on broader context, practical applications, or recent developments\n",
    "\n",
    "    Format your response as a JSON array of suggested questions:\n",
    "    {\n",
    "        \"suggested_questions\": [\n",
    "            \"Question 1?\",\n",
    "            \"Question 2?\",\n",
    "            \"Question 3?\"\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", f\"\"\"Original Query: {original_query}\n",
    "\n",
    "Agent Type: {agent_type}\n",
    "{f'Customer ID: {customer_id}' if customer_id else ''}\n",
    "\n",
    "Agent Response:\n",
    "{agent_response}\n",
    "\n",
    "Please generate relevant follow-up questions based on this context.\"\"\")\n",
    "    ])\n",
    "\n",
    "    question_chain = prompt | llm\n",
    "\n",
    "    try:\n",
    "        response = question_chain.invoke({})\n",
    "        response_data = extract_clean_json(response.content)\n",
    "\n",
    "        if response_data:\n",
    "            # response_data = json.loads(json_str)\n",
    "            logger.info(f\"Generated {len(response_data.get('suggested_questions', []))} follow-up questions\")\n",
    "            return {\n",
    "                \"suggested_questions\": response_data.get(\"suggested_questions\", []),\n",
    "                \"error\": None\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(\"Question generator output was not valid JSON\")\n",
    "\n",
    "            # Fallback parsing - try to extract questions directly\n",
    "            questions = []\n",
    "            for line in response_data:\n",
    "                # Try to identify numbered questions or questions with question marks\n",
    "                if (line.strip().startswith(('1.', '2.', '3.', '4.', '5.')) or\n",
    "                    '?' in line) and len(line.strip()) > 5:\n",
    "                    questions.append(line.strip())\n",
    "\n",
    "            if questions:\n",
    "                return {\"suggested_questions\": questions[:5], \"error\": None}\n",
    "            else:\n",
    "                return {\n",
    "                    \"suggested_questions\": [],\n",
    "                    \"error\": \"Failed to parse question generator output\"\n",
    "                }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in question generator node: {e}\")\n",
    "        return {\"suggested_questions\": [], \"error\": f\"Question generation failed: {e}\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 21:51:34,632 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2025-05-05 21:51:34,635 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "2025-05-05 21:51:34,638 - 127360389907520 - llm.py-llm:187 - ERROR: Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    original_query: str\n",
    "    customer_id: Optional[str]\n",
    "    rewritten_query: str\n",
    "    classified_agent: Literal[\"Company\", \"Customer\", \"Naive\", \"Unknown\"]\n",
    "    agent_response: str\n",
    "    reflection: str\n",
    "    is_final: bool\n",
    "    error: Optional[str]\n",
    "    retry_count: int\n",
    "    suggested_questions: List[str]  # New field for suggested questions\n",
    "\n",
    "logger.info(\"LangGraph State Defined.\")\n",
    "\n",
    "\n",
    "\n",
    "logger.info(\"Environment setup starting ......\")\n",
    "llm, retriever_tool, ingest_tool = setup_environment()\n",
    "logger.info(\"Environment setup complete.\")\n",
    "# --- Initialize Tools ---\n",
    "search_tool = SerperDevTool(n_results=2)\n",
    "# Agent 1: Company Agent (Definition)\n",
    "company_agent_def = CrewAgent(\n",
    "    role='Company Information Specialist',\n",
    "    goal='Provide accurate information about the company (e.g., Google) including location, products, services, history, and mission, based on search results and internal knowledge.',\n",
    "    backstory='You are an AI assistant dedicated to representing the company accurately and helpfully to external queries. You use search tools to find the latest public information.',\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agent 2: Customer Agent (Definition)\n",
    "customer_agent_def = CrewAgent(\n",
    "    role='Customer Support Agent',\n",
    "    goal='Answer customer-specific questions by retrieving their information from the customer database using their customer ID. Handle queries about plans, purchase history, support tickets etc.',\n",
    "    backstory='You are a helpful customer support agent with access to the customer database. You must use the customer ID provided to retrieve relevant information using your specialized tool.',\n",
    "    tools=[search_tool, retriever_tool], # Has retriever and search\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Agent 3: Naive Agent (Definition)\n",
    "naive_agent_def = CrewAgent(\n",
    "    role='General Knowledge Assistant',\n",
    "    goal='Answer general knowledge questions or queries that do not fall under specific company or customer information categories. Use search tools to find relevant information online.',\n",
    "    backstory='You are a general-purpose AI assistant capable of answering a wide range of topics by searching the internet.',\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "logger.info(\"CrewAI Agent Definitions Ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Specific node functions calling the generic executor\n",
    "def company_agent_node(state: AgentState):\n",
    "    return execute_agent_node(state, company_agent_def, \"Company\")\n",
    "\n",
    "def customer_agent_node(state: AgentState):\n",
    "    return execute_agent_node(state, customer_agent_def, \"Customer\")\n",
    "\n",
    "def naive_agent_node(state: AgentState):\n",
    "    return execute_agent_node(state, naive_agent_def, \"Naive\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conditional Edges ---\n",
    "\n",
    "def route_query(state: AgentState):\n",
    "    \"\"\"Routes to the appropriate agent node based on classification.\"\"\"\n",
    "    print(f\"--- Conditional Edge: Routing Query ---\")\n",
    "    if state.get(\"error\"):\n",
    "        print(\"Error detected before routing.\")\n",
    "        return \"error_handler\" # Or END directly\n",
    "\n",
    "    classification = state['classified_agent']\n",
    "    print(f\"Routing based on classification: {classification}\")\n",
    "    if classification == \"Company\":\n",
    "        return \"execute_company\"\n",
    "    elif classification == \"Customer\":\n",
    "        # Add check for customer ID existence\n",
    "        if state.get(\"customer_id\"):\n",
    "             return \"execute_customer\"\n",
    "        else:\n",
    "             print(\"Customer classification but no ID found.\")\n",
    "             # Update state to reflect missing ID issue and go to reflection/end\n",
    "             state[\"error\"] = \"Query classified as Customer, but no Customer ID was provided or found.\"\n",
    "             state[\"agent_response\"] = \"I need a customer ID to answer that question.\"\n",
    "             return \"reflect\" # Go to reflection to potentially end gracefully\n",
    "    elif classification == \"Naive\":\n",
    "        return \"execute_naive\"\n",
    "    else: # Unknown or error during classification\n",
    "        print(\"Classification is Unknown or invalid.\")\n",
    "        state[\"error\"] = f\"Could not determine the right agent for the query (classification: {classification}).\"\n",
    "        state[\"agent_response\"] = \"I'm not sure how to handle that query. Could you please rephrase?\"\n",
    "        return \"reflect\" # Go to reflection\n",
    "\n",
    "def decide_after_reflection(state: AgentState):\n",
    "    \"\"\"Decides whether to end the process or retry based on reflection.\"\"\"\n",
    "    print(f\"--- Conditional Edge: After Reflection ---\")\n",
    "    is_final = state['is_final']\n",
    "    retry_count = state['retry_count']\n",
    "    max_retries = 2 # Set a limit for retries\n",
    "\n",
    "    if state.get(\"error\") and \"Reflection failed\" not in state[\"error\"]: # Handle agent errors first\n",
    "        print(f\"Ending due to execution error: {state['error']}\")\n",
    "        return END # End directly on execution error\n",
    "\n",
    "    print(f\"Reflection result: is_final={is_final}, Retry count={retry_count}\")\n",
    "\n",
    "    if is_final:\n",
    "        print(\"Reflection approved. Ending.\")\n",
    "        return END\n",
    "    elif retry_count >= max_retries:\n",
    "        print(f\"Max retries ({max_retries}) reached. Ending.\")\n",
    "        # Optionally, provide the last reflection feedback\n",
    "        state[\"agent_response\"] += f\"\\n\\n[System Note: Max retries reached. Last feedback: {state.get('reflection', 'N/A')}]\"\n",
    "        return END\n",
    "    else:\n",
    "        print(\"Reflection suggests retry. Looping back to Rewriter.\")\n",
    "        # Add reflection feedback to the original query for context in the next loop? Optional.\n",
    "        # state['original_query'] = f\"{state['original_query']}\\n\\n[Retry Context: Previous attempt failed. Feedback: {state.get('reflection', 'N/A')}]\"\n",
    "        return \"rewrite_query\" # Loop back to the start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x73d42db13550>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query_node)\n",
    "workflow.add_node(\"execute_company\", company_agent_node)\n",
    "workflow.add_node(\"execute_customer\", customer_agent_node)\n",
    "workflow.add_node(\"execute_naive\", naive_agent_node)\n",
    "workflow.add_node(\"reflect\", reflection_node)\n",
    "workflow.add_node(\"generate_questions\", question_generator_node)\n",
    "# Optional: Add a specific error handling node if needed\n",
    "# workflow.add_node(\"error_handler\", ...)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"rewrite_query\")\n",
    "\n",
    "# Routing from Rewriter\n",
    "workflow.add_conditional_edges(\n",
    "    \"rewrite_query\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"execute_company\": \"execute_company\",\n",
    "        \"execute_customer\": \"execute_customer\",\n",
    "        \"execute_naive\": \"execute_naive\",\n",
    "        \"reflect\": \"reflect\", # Handle Unknown/Error cases by going directly to reflection\n",
    "        # \"error_handler\": \"error_handler\" # Route explicit errors\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edges from execution nodes to reflection\n",
    "workflow.add_edge(\"execute_company\", \"reflect\")\n",
    "workflow.add_edge(\"execute_customer\", \"reflect\")\n",
    "workflow.add_edge(\"execute_naive\", \"reflect\")\n",
    "\n",
    "# Conditional edge from Reflection (Loop or End)\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    lambda state: \"generate_questions\" if state[\"is_final\"] else decide_after_reflection(state),\n",
    "    {\n",
    "        \"generate_questions\": \"generate_questions\",\n",
    "        \"rewrite_query\": \"rewrite_query\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"generate_questions\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()\n",
    "# Compile the graph\n",
    "app = workflow.compile(checkpointer=checkpointer) # Set recursion limit for safety\n",
    "\n",
    "\n",
    "logger.info(\"LangGraph Compiled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agentic_system(query: str, cust_id: Optional[str] = None):\n",
    "    \"\"\"Runs the agentic system with a user query.\"\"\"\n",
    "    initial_state = AgentState(\n",
    "        original_query=query,\n",
    "        customer_id=cust_id,\n",
    "        rewritten_query=\"\",\n",
    "        classified_agent=\"Unknown\", # Start as Unknown\n",
    "        agent_response=\"\",\n",
    "        reflection=\"\",\n",
    "        is_final=False,\n",
    "        error=None,\n",
    "        retry_count=0\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\n🚀 Starting Agentic System for Query: '{query}'\" + (f\" (Customer ID: {cust_id})\" if cust_id else \"\"))\n",
    "    # config = {\"configurable\": {\"thread_id\": f\"thread_{query[:10]}\"}} # Example thread ID if using checkpointer\n",
    "\n",
    "    try:\n",
    "        # final_state = app.invoke(initial_state, config=config)\n",
    "        config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        final_state = app.invoke(initial_state, config) # Example thread ID if using checkpointer\n",
    "\n",
    "\n",
    "        logger.info(\"\\n🏁 Agentic System Finished!\")\n",
    "        logger.info(\"------ Final State ------\")\n",
    "        # Pretty print relevant parts of the final state\n",
    "        logger.info(f\"Original Query: {final_state['original_query']}\")\n",
    "        if final_state.get('rewritten_query'): print(f\"Rewritten Query: {final_state['rewritten_query']}\")\n",
    "        if final_state.get('classified_agent'): print(f\"Agent Used: {final_state['classified_agent']}\")\n",
    "        if final_state.get('error'):\n",
    "            logger.info(f\"Error Occurred: {final_state['error']}\")\n",
    "        logger.info(f\"\\nFinal Response:\\n{final_state['agent_response']}\")\n",
    "        if final_state.get('reflection') and not final_state.get('is_final') and final_state.get('retry_count') > 0:\n",
    "             logger.info(f\"\\nLast Reflection Feedback (Process Ended): {final_state['reflection']}\")\n",
    "        # Add display of suggested questions if present\n",
    "        if final_state.get('suggested_questions'):\n",
    "            logger.info(\"\\n✨ Suggested follow-up questions:\")\n",
    "            for i, question in enumerate(final_state['suggested_questions'], 1):\n",
    "                logger.info(f\"  {i}. {question}\")\n",
    "        return final_state\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(f\"\\n💥 An unhandled error occurred during graph execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Example Queries ---\n",
    "# run_agentic_system(\"Tell me about my purchase history for cust123\")\n",
    "# run_agentic_system(\"What was my last support ticket about?\", cust_id=\"cust123\") # Providing ID separately\n",
    "# run_agentic_system(\"can you tell me about the plan for customer cust456 please?\")\n",
    "# run_agentic_system(\"Who invented the telephone?\")\n",
    "# run_agentic_system(\"hwat is googles mission sttement?\") # Test spelling correction\n",
    "# run_agentic_system(\"Tell me about cust999\") # Test non-existent customer ID (if retriever handles it)\n",
    "# run_agentic_system(\"This query is confusing and makes no sense.\") # Test unknown/retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewriter Output: {'rewritten_query': 'Where is the main Google office located?', 'agent_classification': 'Company', 'extracted_customer_id': None}\n",
      "--- Conditional Edge: Routing Query ---\n",
      "Routing based on classification: Company\n",
      "--- Node: Execute Company Agent ---\n",
      "Company Agent Response Snippet: <think>\n",
      "Okay, so I need to figure out where the main Google office is located. Hmm, I'm not exactly sure about this off the top of my head. Let me think through it step by step.\n",
      "\n",
      "First, I know that Go...\n",
      "--- Node: Reflection ---\n",
      "--- Conditional Edge: After Reflection ---\n",
      "Reflection result: is_final=False, Retry count=1\n",
      "Reflection suggests retry. Looping back to Rewriter.\n",
      "Rewriter Output: {'rewritten_query': 'Where is the main Google office located?', 'agent_classification': 'Company', 'extracted_customer_id': None}\n",
      "--- Conditional Edge: Routing Query ---\n",
      "Routing based on classification: Company\n",
      "--- Node: Execute Company Agent ---\n",
      "Company Agent Response Snippet: <think>\n",
      "Okay, so I need to figure out where the main Google office is located. Hmm, I'm not exactly sure about this off the top of my head, but I know Google has a lot of offices around the world. Let...\n",
      "--- Node: Reflection ---\n",
      "--- Conditional Edge: After Reflection ---\n",
      "Reflection result: is_final=False, Retry count=2\n",
      "Max retries (2) reached. Ending.\n",
      "Rewritten Query: Where is the main Google office located?\n",
      "Agent Used: Company\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_query': 'Where is the main Google office located?',\n",
       " 'customer_id': None,\n",
       " 'rewritten_query': 'Where is the main Google office located?',\n",
       " 'classified_agent': 'Company',\n",
       " 'agent_response': \"<think>\\nOkay, so I need to figure out where the main Google office is located. Hmm, I'm not exactly sure about this off the top of my head, but I know Google has a lot of offices around the world. Let me think through how I can approach this.\\n\\nFirst, I remember that Google's headquarters are in Mountain View, California. That makes sense because Mountain View is a well-known city with a nice mountain range, which probably reflects their location near Google's offices. But wait, there might be other offices too. Maybe some of them are in different cities or even countries?\\n\\nI think Google has offices in multiple countries, not just the US and Canada. I've heard about offices in places like Tokyo, Hong Kong, and maybe even some European cities. Let me try to recall any specific locations I know.\\n\\nIn Japan, I believe there's a Google office near the Tokyo Skytree. That sounds familiar because I've seen pictures of Google there. So that's one location in Japan. Then there's Hong Kong, which is part of China. I think Google has an office there too, maybe near the Chinese Post Office or something like that.\\n\\nI also remember hearing about a Google office in London. It might be near the British Museum since that's a significant landmark and Google often collaborates with institutions like the British Museum. So London could be another location.\\n\\nAre there any offices in other countries? Maybe in Australia, but I'm not sure where exactly. I think there are some offices in New South Wales or Queensland, but I don't recall their exact locations. Similarly, in Germany, maybe there's an office near a major university or business complex.\\n\\nI should also consider if Google has any offices outside of the US and Canada. Maybe in Europe, Australia, or even parts of Asia. But without specific names, it might be hard to pin down exactly where each office is located.\\n\\nWait, I think I've heard about a Google office in New York City. It's probably near the Times Square area because that's a major business hub and Google has been around for decades. So New York could be another location.\\n\\nPutting this all together, I can list out the locations I know: Mountain View, California; Tokyo Skytree, Japan; Chinese Post Office or nearby in Hong Kong; British Museum in London; possibly New South Wales or Queensland in Australia; and Times Square in New York City.\\n\\nI should make sure these are accurate. Let me check a bit more about each location to confirm:\\n\\n- Mountain View, CA: Definitely the headquarters.\\n- Tokyo Skytree: Yes, that's where Google is based there.\\n- Hong Kong: I think it's near the Chinese Post Office or some other significant building.\\n- British Museum in London: That's correct; Google collaborates with institutions like this.\\n- New South Wales or Queensland: Maybe near a university or business complex. I'm not sure of the exact location, but it's somewhere in Australia.\\n- Times Square, NY: Yes, that's where the headquarters are located.\\n\\nI think these are the main locations I can confidently say about Google's offices based on my current knowledge. There might be more details or specific addresses, but this gives a good overview of where they are located globally.\\n</think>\\n\\nGoogle has its headquarters and multiple offices across various countries and cities around the world. Here is an organized list of the key locations:\\n\\n1. **Mountain View, California**: The main headquarters of Google.\\n2. **Tokyo Skytree**: A Google office in Tokyo, near the Tokyo Tower.\\n3. **Hong Kong**: A Google office near the Chinese Post Office or other significant buildings.\\n4. **British Museum in London**: Google collaborates with institutions like this.\\n5. **New South Wales or Queensland**: Possibly located near a university or business complex.\\n6. **Times Square, New York City**: The headquarters are situated here.\\n\\nThese locations reflect Google's global presence and collaboration across different regions.\",\n",
       " 'reflection': '',\n",
       " 'is_final': False,\n",
       " 'error': \"Reflection failed: 'dict' object has no attribute 'content'\",\n",
       " 'retry_count': 2}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_agent = run_agentic_system(\"Where is the main Google office located?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
